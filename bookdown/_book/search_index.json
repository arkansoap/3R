[["index.html", "Econometrics vizualisation Chapter 1 Introduction 1.1 What is Econometrics 1.2 What about econometric visualization? 1.3 State of the art 1.4 Our project, an R package.", " Econometrics vizualisation Lucas Chaveneau, Thibault Fuchez, Allan Guichard 2021-12-20 Chapter 1 Introduction 1.1 What is Econometrics BLABLABLA Econometrics is a branch of economics that aims to estimate and test economic models (simplified representation of reality). Thus, the econometrician tries to identify the parameters of a model by means of statistical estimation, and thus tries to induce the characteristics of a general group (the population) from those of a particular group (the sample). Three essential words in the language of the econometrician are: correlation, regression, and causality. To better understand the relationships between different variables, visualization is a key tool to help interpret mathematical and statistical results. 1.2 What about econometric visualization? We can start by saying what it is not: it is not strictly speaking data visualization. Data visualization the graphical display of data is often seen as something trivial, to be quickly scrolled through to show stakeholders simple but salient facts of a possibly very complex problem The analyst consciously chooses what to include in a visualization in order to identify intuitively relevant patterns and trends in the data in the most efficient way. sible. The analyst then makes choices in this representation. Modern data sets tend to be very large (in terms of number of observations) and broad (in terms of number of variables) and therefore it is impossible to simply represent all the data. The difficulty is that everything must be made as simple as possible - but not simpler. To explain, notably to teach, econometrics in order to facilitate the understanding of the fundamental concepts without necessarily using a mathematically complex corpus. For the practice of econometrics, i.e. to design (at least some) diagrammatic representations of models, methods and diagrams that can facilitate the understanding and reading of results. 1.3 State of the art Data visualization is an important part of any statistical analysis. Any good statistician will tell you that it is dangerous to undertake an econometric analysis without first examining the data. Scatterplots are used as a tool to diagnose the overall trend, the relationship between variables, the variation around the perceived trend, the exceptions whether they are outliers or distinct groups of observations, the discontinuities. The analysis of residuals largely uses the same tools. A bibliographic search on the theme of visualization in econometrics does not yield much. Nevertheless, we will mention in this document some articles that deal with the subject in a direct or indirect way, and that have been an anchor point in our reflections. Moreover, some packages related to econometric visualization have already been developed in R (or other languages). At the end of this document, we will briefly describe the state of the art of these packages. 1.4 Our project, an R package. We have as a project to propose a library developed under R which proposes different ways to visualize the essential elements of the econometer. Our package will therefore focus on the representation of correlations. We will try to put forward a clear and synthetic representation of the covariance, as well as a representation of the elements that allow to visualize the quality of a regression as well as the part of each variable in the explanation of the variable to predict. This document is intended to accompany the package. It proposes a definition of the econometric tools necessary to understand the package. It also proposes an overview of the different visualization techniques that already exist. "],["covariance.html", "Chapter 2 Covariance 2.1 Reminder of the definition of variance 2.2 Usual literary definition of covariance 2.3 Usual and alternative mathematical definitions", " Chapter 2 Covariance 2.1 Reminder of the definition of variance In statistics and probability theory, the variance is a measure of the dispersion of the values of a sample or a probability distribution. It expresses the average of the squares of the deviations from the mean, also equal to the difference between the average of the squares of the values of the variable and the square of the mean, according to the König-Huygens theorem. Classical formula of variance \\[\\sigma^2_x=\\frac{1}{n}\\sum_{x=1}^{n}(x_i - \\bar{x})^2 = \\frac{1}{n}\\sum_{x=1}^{n}x_i^2 - \\bar{x}^2\\] A new proposition for variance formula(Heffernan 1988): \\[\\sigma^2_x= \\frac{1}{n(n-1)}\\sum_{i=1}^{n-1}\\sum_{j&gt;i}^{n}(x_i-x_j)^2\\] This formulation (extended to covariance) will be discussed again in the paper. Intuitively, we can see here the geometrical aspect of the variance of x perceived as the expression of a square. Variance vs. Covariance: Variance and covariance are mathematical terms frequently used in statistics and probability theory. Variance refers to the spread of a data set around its mean value, while a covariance refers to the measure of the directional relationship between two random variables. 2.2 Usual literary definition of covariance Covariance is an extension of the notion of variance. The covariance between two random variables is a number allowing to quantify their joint deviations from their respective expectations. A covariance refers to the measure of how two random variables will change when they are compared to each other. Intuitively, covariance is a measure of the simultaneous variation of two random variables. That is, the covariance becomes more positive for each pair of values that differ from their mean in the same direction, and more negative for each pair of values that differ from their mean in the opposite direction. The sign of the covariance therefore shows the tendency in the linear relationship between the variables. The magnitude of the covariance is not easy to interpret because it is not normalized and hence depends on the magnitudes of the variables. The covariance of two independent random variables is zero, although the converse is not always true. This concept is naturally generalized to several variables (random vector) by the covariance matrix (or variance-covariance matrix) which, for a set of p real random variables \\(X_1\\), etc., \\(Xp\\) is the square matrix whose element of row i and column j is the covariance of variables \\(X_i\\) and \\(X_j\\). This matrix allows us to quantify the variation of each variable with respect to each of the others. 2.3 Usual and alternative mathematical definitions 2.3.1 Usual definition covariance formula: For two jointly distributed real-valued random variables X and Y with finite, the covariance is defined as the expected value (or mean) of the product of their deviations from their individual expected values \\[Cov(X,Y)=\\mathbb{E}((X-\\mathbb{E}(X))\\times(Y-\\mathbb{E}(Y)))\\] where \\(\\mathbb{E}(X)\\) is the expected value of X, also known as the mean of X. The covariance is also sometimes denoted \\(\\sigma_{X,Y}\\), in analogy to variance. By using the linearity property of expectations, this can be simplified to the expected value of their product minus the product of their expected values: \\[\\begin{align} Cov(X,Y) &amp;=\\mathbb{E}((X-\\mathbb{E}(X))\\times(Y-\\mathbb{E}(Y))) \\\\ &amp;=\\mathbb{E}(XY- X\\mathbb{E}(Y) - \\mathbb{E}(X)Y + \\mathbb{E}(x)\\mathbb{E}(Y)) \\\\ &amp;= \\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y) - \\mathbb{E}(x)\\mathbb{E}(Y) + \\mathbb{E}(x)\\mathbb{E}(Y) \\\\ &amp;= \\mathbb{E}(XY)-\\mathbb{E}(X)\\mathbb{E}(Y) \\end{align}\\] The empirical covariance of a sample is defined by \\[Cov(x,~y)=\\frac{1}{N} \\sum_{i=1}^{N} (x_i-\\overline{x})(y_i -\\overline{y})\\] With \\(\\overline{x}=\\frac{1}{N}\\sum_{i=1}^{N} x_j\\) et \\(\\overline{y}=\\frac{1}{N}\\sum_{i=1}^{N} y_j\\) An unbiased estimator of the population covariance is defined by \\[Cov(x,~y)=\\frac{1}{(N-1)} \\sum_{i=1}^{N} (x_i-\\overline{x})(y_i -\\overline{y})\\] or equivalently : \\[Cov(x,~y)=\\frac{N}{N-1}(\\overline{xy}-\\overline{x}~\\overline{y})\\] 2.3.2 Alternative definition, a story of rectangles  formula from heffernan definition of covariance : \\[cov(X,Y)= \\frac{2}{n(n-1)}\\sum_{i=1}^{n-1}\\sum_{j&gt;i}^{n}\\frac{1}{2}(x_i-x_j)(y_i - y_j)\\] Let be two random variables \\((X,~Y)\\), and a sample of N pairs of independent observations \\[(x_1,~y_1),(x_2,~y_2),\\dotsi (x_N,~y_N)\\]. Let us consider two observations drawn at random of index \\(k\\), \\(l\\) and let us calculate the mathematical expectation of the area of the rectangle formed by the two chosen points: \\[\\mathbb{E}(x_k-x_l)(y_k-y_l)\\] So \\[\\mathbb{E}(x_ky_k-x_ky_l-x_ly_k+x_ly_l)\\] \\[\\mathbb{E}(x_ky_k) -\\mathbb{E}(x_k)\\mathbb{E}(y_l) -\\mathbb{E}(x_l)\\mathbb{E}(y_k)+ \\mathbb{E}(x_ly_l)\\] \\[2\\mathbb{E}(XY)-2\\mathbb{E}(X)\\mathbb{E}(Y)\\] Hence, covariance: \\[\\mathbb{E}(x_k-x_l)(y_k-y_l)= 2~~ Cov(X,Y)\\] On a sample of N pairs of observations \\((x_i,y_i)~i=1,\\dots, N\\) the empirical covariance can also be computed as the average of the \\(N(N-1)\\) areas of the \\((x_i,y_i)\\) and \\((x_j,y_j)\\) vertex rectangles for \\(i=1,\\dots, N-1\\) and \\(j=1,\\dots, N\\) Note that the \\(N\\) rectangles (degenerate, of zero area) of vertex \\((x_i,y_i)\\) and \\((x_j,y_j)\\) with \\(i=j\\) should not be taken into account in the calculation. \\[Cov(x,~y)=\\frac{1}{N(N-1)}\\sum_{i=1}^{N-1} \\sum_{j=i+1}^{N}~(x_i-x_j)(y_i-y_j)\\] An equivalent definition is to consider half (due to symmetry) of the average of the \\(N(N-1)\\) areas of the rectangles of vertices \\((x_i,y_i)\\) and \\((x_j,y_j)\\) for \\(i=1,\\dots, N\\) and \\(j=1,\\dots, N\\) (evidence in annexes). \\[Cov(x,~y)=\\frac{1}{2N(N-1)} \\sum_{i=1}^{N} \\sum_{j=1}^{N}~(x_i-x_j)(y_i-y_j)\\] References "],["correlation.html", "Chapter 3 Correlation 3.1 From covariance to correlation 3.2 Correlation definition 3.3 Galton, a pioneer in the history of correlation 3.4 Thirteen ways to see correlation (Rodgers and co 1988) 3.5 Pearson correlation coefficient 3.6 Spearmans rank correlation coefficient 3.7 Partial Correlation 3.8 Semi partial Correlation 3.9 Transitivity correlation 3.10 Distance correlation 3.11 Other Correlations", " Chapter 3 Correlation 3.1 From covariance to correlation The normalized version of the covariance, the correlation coefficient, however, shows by its magnitude the strength of the linear relation. Both covariance and correlation measure the relationship and the dependency between two variables. Covariance indicates the direction of the linear relationship between variables. Correlation measures both the strength and direction of the linear relationship between two variables. Correlation values are standardized. Covariance values are not standardized. The normalized form of the covariance matrix is the correlation matrix. 3.2 Correlation definition In statistics, correlation or dependence is any statistical relationship, whether causal or not, between two random variables or bivariate data. In the broadest sense correlation is any statistical association, though it actually refers to the degree to which a pair of variables are linearly related. There are several correlation coefficients, often denoted \\(\\rho\\) or \\(r\\), measuring the degree of correlation. The most common of these is the Pearson correlation coefficient, which is sensitive only to a linear relationship between two variables (which may be present even when one variable is a nonlinear function of the other). Other correlation coefficients  such as Spearmans rank correlation  have been developed to be more robust than Pearsons, that is, more sensitive to nonlinear relationships. 3.3 Galton, a pioneer in the history of correlation I can only say that there is a vast field of topics that fall under the laws of correlation, which lies quite open to the research of any competent person who cares to investigate it. (Galton, 1890) Galtons 1888 paper, presented to the Royal Society in London, defines correlation as follows: &gt;&gt;Two variable organs are said to be co-related when the variation of the one is accompanied on the average by more or less variation of the other, and in the same direction. It is easy to see that co-relation must be the consequence of the variations of the two organs being partly due to common causes If they were in no respect due to common causes, the co-relation would be nil. Galtons definition reveals the properties of the correlation coefficient. It is a measure of the strength of a linear relationship; the closer it is to 1, the more two variables can be predicted from each other by a linear equation. It is a measure of direction: a positive correlation indicates that \\(X\\), \\(Y\\) increase together; a negative correlation indicates that one decreases as the other increases. Note that Galton does not claim that co-relation implies cause and effect (it would be absurd to assume that the size of one organ determines the size of another). Galton hypothesized that the correlation indicated the presence of common causes for the observed relationship between the variables (the size of each organ respectively). More technically Galton continues his presentation as follows: Let y = the deviation of the subject [in units of the probably error, Q], whichever of the two variables may be taken in that capacity; and let x1, x2, x3, &amp;c., be the corresponding deviations of the relative, and let the mean of these be X. Then we find: (1) that y = rX for all values of y; (2) that r is the same, whichever of the two variables is taken for the subject; (3) that r is always less that 1; (4) that r measures the closeness of co-relation. Galton particularly liked the correlation coefficient because it could be used to predict deviations \\(Y\\) from \\(X\\) or \\(X\\) from \\(Y\\). Thus, from the beginning, the correlation coefficient was closely related to the regression line. Originally, \\(r\\) meant the regression slope, but there was a problem in that the regression line of the slope was partly a function of the units of measurement chosen. Galton perceived the correlation coefficient as a unitless regression slope and appropriated the label \\(r\\). 3.4 Thirteen ways to see correlation (Rodgers and co 1988) In 1885, Sir Francis Galton first defined the term regression and completed the theory of bivariate correlation. A decade later, Karl Pearson developed the index that we still use to measure correlation, Pearsons r . Our article is written in recognition of the 100th anniversary of Galtons first discussion of regression and correlation. According Joseph Lee Rodgers and co article, Several ways to interpret the correlation: As standardized covariance \\[r=\\frac{cov(x,y)}{\\sigma^2_x\\sigma^2_y}\\] As standardized slope of regression line \\[r=b_{Y.X}(\\frac{\\sigma^2_x}{\\sigma^2_y})=b_{X.Y}(\\frac{\\sigma^2_y}{\\sigma^2_x})\\] As geometric mean of the two regression slopes}\\ \\[r=\\pm\\sqrt{b_{Y.X}\\times b_{X.Y}}\\] As the square root of the ratio of two variances} \\[r=\\sqrt{\\frac{\\sum(Y_i -\\hat{Y}_i)}{\\sum(Y_i -\\bar{Y}_i)}}=\\sqrt{\\frac{SS_{reg}}{SS_{tot}}}\\] And so many other 3.5 Pearson correlation coefficient some notations that may be useful: \\(S_{xx} = \\sum_{i=1}^n(x_i -\\bar{x})^2\\) \\(S_{yy} = \\sum_{i=1}^n(y_i -\\bar{y})^2\\) \\(S_{xy} = \\sum_{i=1}^n(x_i -\\bar{x})(y_i -\\bar{y})\\) Pearson correlation coefficient: \\[\\rho=\\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}}\\] The Pearson correlation coefficient is a bounded index (i.e., \\(-1 \\leq \\rho \\leq 1\\)) that provides a unitless measure for the strength and direction of the association between two variables. 3.6 Spearmans rank correlation coefficient measures the association based on the ranks of the variables. \\[\\hat{\\theta}=\\frac{\\sum_{i=1}^n(R_i-\\bar{R}(S_i-\\bar{S}))}{\\sqrt{\\sum_{i=1}^n(R_i-\\bar{R})^2\\sum_{i=1}^n(S_i-\\bar{S})^2}}\\] where \\(R_i\\) and \\(S_i\\) are the rank of the \\(x_i\\) and \\(y_i\\) values, respectively. Note that this is just the estimated Pearsons correlation coeffcient, but the values of the variables have been replaced by their respective ranks. The Spearman correlation is the non-parametric equivalent of the Pearson correlation. It measures the relationship between two variables. If the variables are ordinal, discrete or do not follow a normal distribution, the Spearman correlation is used. This correlation does not use the values of the data but their rank. In fact, nothing changes, everything is the same as calculating a Pearson correlation but on transformed variables. The interest of establishing a correlation on the ranks of the variables is to detect if there is a monotonic relationship, which may not be linear. 3.7 Partial Correlation The partial correlation coefficient, noted here \\(r_{AB.C}\\), allows us to know the value of the correlation between two variables A and B, if the variable C had remained constant for the series of observations considered. Put another way, the partial correlation coefficient \\(r_{AB.C}\\) is the total correlation coefficient between variables A and B when we have removed their best linear explanation in terms of C. It is given by the formula : \\[r_{AB.C}=\\frac{r_{AB}-r_{AC} \\cdot r_{BC}}{\\sqrt{1-r_{AC}^2} \\cdot \\sqrt{1-r_{BC}^2}}\\] Lets go a little further in understanding this coefficient: The OLS estimator of \\(\\beta\\) is written \\[\\hat{\\beta}=\\frac{Cov(y,x_1)}{\\mathbb{V}ar(x_1)}\\] The estimator \\(\\beta&#39;\\) is written \\[\\begin{align*} \\hat{\\beta&#39;} &amp;= \\frac{Cov(y,x_1)\\mathbb{V}(x_2)-Cov(y,x_2)Cov(x_1,x_2)}{ \\mathbb{V}(x_1)\\mathbb{V}(x_2)-Cov(x_1,x_2)^2} \\\\ &amp;= \\hat{\\beta&#39;}=\\frac{\\rho_{y1} \\sigma_y \\sigma_1\\sigma_2^2-\\rho_{y2} \\sigma_y \\sigma_2\\rho_{12} \\sigma_1 \\sigma_2}{\\sigma_1^2\\sigma_2^2-\\rho_{12}^2 \\sigma_1^2 \\sigma_2^2} \\\\ &amp;= \\hat{\\beta&#39;}={\\frac{\\rho_{y1}-\\rho_{y2}\\rho_{12}} {1-\\rho_{12}^2}}\\quad\\frac{\\sigma_y}{\\sigma_1} \\end{align*}\\] After some transformation we have: \\[\\hat{\\beta&#39;}=\\underbrace{\\underbrace{\\frac{\\rho_{y1}-\\rho_{y2}\\rho_{12}} {\\sqrt{1-\\rho_{12}^2}\\sqrt{1-\\rho_{y2}^2}}}_{\\text{Partial correlation}}}_{\\rho_{y1|2}} \\quad\\frac{\\sigma_y\\sqrt{1-\\rho_{y2}^2}}{\\sigma_1\\sqrt{1-\\rho_{12}^2}}\\] Pour comprendre cette expression, Consider the following two regressions: \\[x_1=\\kappa +\\tau x_2+\\varepsilon_{1|2}\\] \\[y=\\delta +\\gamma x_2+\\varepsilon_{y|2}\\] We have: \\[\\begin{align*} Cov(e_{1|2},e_{y|2})&amp;=Cov(x_1-\\hat{\\kappa}-\\hat{\\tau} x_2,~y-\\hat{\\delta} -\\hat{\\gamma} x_2)\\\\ &amp;=Cov(x_1,y)-\\hat{\\gamma}Cov(x_1,x_2)-\\hat{\\tau}Cov(x_1,y)+\\hat{\\gamma}\\hat{\\tau}\\mathbb{V}ar(x_2)\\\\ \\mathbb{V}ar(e_{y|2})&amp;=\\mathbb{V}ar(y-\\hat{\\delta} - \\hat{\\gamma} x_2)\\\\ &amp;=\\mathbb{V}ar(y)-2\\hat{\\gamma}Cov(x_1,y)+\\hat{\\gamma}^2\\mathbb{V}ar(x_2)\\\\ \\mathbb{V}ar(e_{1|2})&amp;=\\mathbb{V}ar(x_1-\\hat{\\kappa} - \\hat{\\tau} x_2)\\\\ &amp;=\\mathbb{V}ar(x_1)-2\\hat{\\tau}Cov(x_1,x_2)+\\hat{\\tau}^2\\mathbb{V}ar(x_2) \\end{align*}\\] The linear correlation coefficient between \\(e_{1|2}\\) and \\(e_{y|2}\\) corresponds to the correlation between \\(y\\) and \\(x_1\\) after taking into account the linear influence of \\(x_2\\) on these two respective variables: \\[\\begin{align*} \\rho_{yx_1|x_2}&amp;=\\frac{Cov(e_{1|2},e_{y|2})}{\\sqrt{\\mathbb{V}ar(e_{y|2})\\mathbb{V}ar(e_{1|2})}}\\\\ \\end{align*}\\] After simplification we obtain the expression of the partial correlation: \\[\\begin{align*} \\rho_{y1|2}&amp;=\\frac{\\rho_{y1}-\\rho_{y2}\\rho_{12}}{\\sqrt{(1-\\rho_{y2}^2)(1-\\rho_{12}^2)}} \\end{align*}\\] And so, the estimator \\(\\hat{\\beta&#39;}\\) can thus be written as that of a simple linear regression where the variables are the residuals of prior regressions of \\(y\\) respectively \\(x_1\\) on \\(x_2\\). \\[\\hat{\\beta&#39;}=\\rho_{y1|2}\\frac{\\sigma_y\\sqrt{1-\\rho_{y2}^2}}{\\sigma_1\\sqrt{1-\\rho_{12}^2}}\\] Il peut être assez laborieux de calculer la corrélation partielle lorsque le nombre de variables est plutôt grand. Il est conseillé dutiliser des techniques de régression lorsque le nombre de variable dépasse 3. Lalternative est de calculer les résidus des régressions des deux variables sur les variables dont on veut purger leffet. Cette approche permet daboutir sur les mêmes résultats car rappelons que la corrélation partielle consiste à mesure le lien entre linformation résiduelle de \\(X\\) et \\(Y\\) qui ne soit pas déjà expliquée par les variables dont on veut enlever leffet. La corrélation dordre \\(j\\) revient donc à calculer la corrélation entre les résidus des régressions. \\[\\rho_{xy.z_1, \\ldots, z_j} = \\rho_{e_xe_y}\\] 3.8 Semi partial Correlation A la différence de la corrélation partielle, la corrélation semi-partielle est résolument asymétrique, elle se rapproche de la régression multiple. On essaie de quantifier le pouvoir explicatif additionnel dune variable. Positionnons nous dans un premier temps dans le cadre à 3 variables \\(Y\\) , \\(X\\), et \\(Z\\) : \\(Y\\) est la variable dépendante que lon cherche à expliquer, \\(X\\) est la variable explicative que lon cherche à évaluer, \\(Z\\) est la variable de contrôle. Le carré de la corrélation semi-partielle, notée \\(r^2_{y(x.z)}\\), quantifie la proportion de variance de \\(Y\\) expliquée par \\(X\\), sachant que lon a retranché de cette dernière linformation apportée par \\(Z\\). En dautres termes, quelle est la part de \\(Y\\) quexplique linformation additionnelle de \\(X\\) par rapport à \\(Z\\). Notons bien la différence avec la corrélation partielle. Avec ce dernier, nous retranchons linformation apportée par \\(Z\\) sur à la fois \\(Y\\) et \\(X\\), et nous mesurons la liaison sur les résidus. Dans le cas de la corrélation semi-partielle, nous cherchons à quantifier la liaison de \\(Y\\) avec la partie résiduelle de \\(X\\) par rapport à \\(Z\\). La valeur absolue de la corrélation semi-partielle de \\(X\\) avec \\(Y\\) est toujours inférieure ou égale à celle de la corrélation partielle de \\(X\\) avec \\(Y\\). La raison en est la suivante : supposons que la corrélation de \\(X\\) avec \\(Z\\) a été supprimée de \\(X\\), donnant le vecteur résiduel \\(e_x\\) . Lors du calcul de la corrélation semi-partielle, \\(Y\\) contient toujours à la fois une variance unique et une variance en raison de son association avec \\(Z\\). Mais \\(e_x\\), étant non corrélé avec \\(Z\\) , ne peut expliquer quune partie de la partie unique de la variance de \\(Y\\) et non la partie liée à \\(Z\\). En revanche, avec la corrélation partielle, seule \\(e_y\\) (la partie de la variance de \\(Y\\) qui nest pas liée à \\(Z\\)) doit être expliquée, il y a donc moins de variance du type que \\(e_x\\) ne peut pas expliquer. \\[r_{y(x.z)}= \\frac{r_{yx} - r_{yz}r_{xz}}{\\sqrt{1-r^2_{xz}}}\\] Il est évident que si \\(X\\) et \\(Z\\) sont indépendant alors \\(r_{y(x.z)}= r_{yx}\\). De manière contraire, si \\(X\\) et \\(Z\\) sont parfaitement corrélés alors \\(r_{y(x.z)}\\) est indéfinie, il ne reste plus rien dans les résidus de \\(X\\) pour expliquer \\(Y\\). 3.9 Transitivity correlation Let \\(\\rho_{xy}\\) be the correlation between the variables \\(X\\) and \\(Y\\). Let \\(\\rho_{xz}\\) and \\(\\rho_{yz}\\) be the correlations of variables \\(X\\) and \\(Y\\) with respect to a third variable \\(Z\\). Given \\(\\rho_{xz}\\) and \\(\\rho_{yz}\\), can we deduce the possible values for \\(\\rho_{xy}\\)? \\(\\rho_{XY \\mid Z}={\\frac {\\rho_{XY}-\\rho _{XZ}\\rho_{YZ}}{{\\sqrt {1-\\rho_{XZ}^{2}}}{\\sqrt {1-\\rho_{YZ}^{2}}}}}\\) \\[\\begin{align*} \\rho_{XY} &amp;= \\left( \\rho_{XY \\mid Z} - \\frac{ - \\rho_{XZ} \\rho_{YZ}}{\\sqrt{1 - \\rho_{XZ}^{2}} \\sqrt{1 - \\rho_{YZ}^{2}}} \\right) \\sqrt{1 - \\rho_{XZ}^{2}} \\sqrt{1 - \\rho_{YZ}^{2}} \\\\ &amp;= \\rho_{XY \\mid Z} \\sqrt{1 - \\rho_{XZ}^{2}} \\sqrt{1 - \\rho_{YZ}^{2}} + \\rho_{XZ} \\rho_{YZ} \\end{align*}\\] \\(\\rho_{xy}\\) is in the range \\(\\rho_{XZ} \\rho_{YZ} \\pm \\sqrt{1 - \\rho_{XZ}^{2}} \\sqrt{1 - \\rho_{YZ}^{2}}\\) \\[\\begin{align*} \\rho_{XZ} \\rho_{YZ} - \\sqrt{1 - \\rho_{XZ}^{2}} \\sqrt{1 - \\rho_{YZ}^{2}} &amp;&gt; 0 \\\\ \\rho_{XZ} \\rho_{YZ} + \\sqrt{1 - \\rho_{XZ}^{2}} \\sqrt{1 - \\rho_{YZ}^{2}} &amp;&gt; 0 . \\end{align*}\\] \\[\\begin{align*} \\rho_{XZ}^2 \\rho_{YZ}^2 &amp;&gt; \\left ( 1 - \\rho_{XZ}^{2}\\right ) \\left (1 - \\rho_{YZ}^{2}\\right ) \\\\ &amp;= 1 - \\rho_{XZ}^{2} - \\rho_{YZ}^{2} + \\rho_{XZ}^2 \\rho_{YZ}^2 . \\end{align*}\\] Si \\(\\rho_{xy}&gt;0\\) alors on a \\[\\begin{align*} \\rho_{XZ}^2 \\rho_{YZ}^2 &amp;&gt; \\left ( 1 - \\rho_{XZ}^{2}\\right ) \\left (1 - \\rho_{YZ}^{2}\\right ) \\\\ &amp;= 1 - \\rho_{XZ}^{2} - \\rho_{YZ}^{2} + \\rho_{XZ}^2 \\rho_{YZ}^2 . \\end{align*}\\] \\[\\begin{equation*} \\text{sign}(\\rho_{XY}) = \\begin{cases} \\text{sign}(\\rho_{XZ}) \\text{sign}(\\rho_{YZ}) &amp; \\rho_{XZ}^2 + \\rho_{YZ}^2 &gt; 1 \\\\ \\text{not known} &amp; \\rho_{XZ}^2 + \\rho_{YZ}^2 \\leq 1. \\end{cases} \\end{equation*}\\] 3.10 Distance correlation The so-called association measures are an active and recent field of research that renews the well established and old field of correlation. The energy package developed under R as well as Gabors article (???) are good references. Let \\((x_i,y_i)\\), \\(i=1,2, \\dots, N\\) be a sample of pairs of observations of the variables \\(X\\) and \\(Y\\). We compute successively \\[dx_{ij}=\\lVert x_i-xj \\rVert\\] \\[dy_{ij}=\\lVert y_i-yj \\rVert\\] \\[\\overline{\\overline{dx_{ij}}}=dx_{ij}-\\overline{dx_{i.}}-\\overline{dx_{.j}}+\\overline{dx_{..}}\\] \\[\\overline{\\overline{dy_{ij}}}=dy_{ij}-\\overline{dy_{i.}}-\\overline{dy_{.j}}+\\overline{dy_{..}}\\] with \\(\\overline{dx_{i.}}=\\frac{1}{N}\\sum_{j=1}^{N} dx_{ij}\\) and \\(\\overline{dx_{..}}=\\frac{1}{N^2}\\sum_{i=1}^{N}\\sum_{j=1}^{N} dx_{ij}\\) \\[dCov(X,Y)=\\frac{1}{N^2}\\sum_{i=1}^{N}\\sum_{j=1}^{N} \\overline{\\overline{dx_{ij}}}~ \\overline{\\overline{dy_{ij}}}\\] \\[dVar(X)=dCov^2(X,X)=\\frac{1}{N^2}\\sum_{i=1}^{N}\\sum_{j=1}^{N} \\overline{\\overline{dx_{ij}}}~^2\\] \\[d\\rho=\\frac{dCov(X,Y)}{d\\sigma_X~d\\sigma_Y}\\] note bousquet : expliquer, commenter, représenter, tester sur un exmple, appliquer le package R, +bootstrap pour intervalles de confiance 3.11 Other Correlations On ne compte plus le nombre de correlation. Voici quelques dentre eux: \\begin{itemize}[label=] Biweight midcorrelation (Langfelder &amp; Horvath, 2012) : cest une mesure de la relation entre variables basée sur la médiane, et non sur la moyenne. Elle est donc moins sensible aux valeurs aberrantes. Elle est souvent utilisé comme alternative robuste au coefficient de corrélation linéaire de Pearson. Percentage bend correlation : Introduite par Wilcox en 1994, elle est basée sur une pondération descendante dun pourcentage spécifié dobservations marginales sécartant de la médiane (par défaut, 20%). Shepherds Pi correlation : elle est équivalente au à la corrélation de rang de Spearman aprés avoir supprimé les valeurs aberrantes. Ici, une valeur aberrante est définie au moyen de la distance de Mahalanobis bootstrap). Point-Biserial and biserial correlation : ces coefficients sutilise lorsque une variable est continue et lautre est dichotomique. Tandis que la point-biserial correlation est similaire à la corrélation de Pearson, la biserial correlation doit-être utilisé lorsque la variable dichotomique peut avoir une dimension continue sous-jacente. Par exemple, la réussite dun semestre peut être exprimé en note, mais peut aussi être classé Réussite ou Echec. Polychoric and tetrachoric : la polychoric correlation est la corrélation entre deux variables latentes continues théoriquement distribuées, à partir de deux variables ordinales observées. La tetrachoric correlation est un cas particulier applicable lorsque les deux variables observées sont dichotomiques. Blomqvists coefficient (Blomqvist, 1950) : également appelé Blomqvists Beta ou medial correlation, cest un coefficient de corrélation non paramétrique basée sur la médiane. Il a un avantage par rapport à certaines mesures comme les estimations de Spearman ou de Kendall %pourquoi ? à détailler Hoeffdings D : la statistique D de Hoeffding est une mesure dassociation non paramègtrique basée sur le rang. Il détecte les écart plus génééraux par rapport à lindépendance y compris les associations non linéaires. Cette métrique est compris entre -0,5 et 1. Gamma correlation : La statistique gamma de Goodman Kruskal est similaire au coefficient \\(\\tau\\) de Kendall. Il est relativement robuste aux valeurs aberrantes et traite bien les données qui ont de nombreux liens. Gaussian rank correlation (Boudt et al., 2012) : il est une alternative simple et performante pour des corrélations de rang robustes. Il est basé sur les quantiles gaussiens des rangs. Winsorized correlation : Cest une corrélation basée sur les variables transformées. La transformation dite winsorisées limite les valeurs extrêmes pour réduire leffet des valeurs aberrantes éventuellement fausses. References "],["vizualization-of-covariance.html", "Chapter 4 Vizualization of covariance 4.1 State of the art: different attempts to represent the covariance 4.2 Our current project: the package plotnetrec", " Chapter 4 Vizualization of covariance 4.1 State of the art: different attempts to represent the covariance 4.1.1 diagramme de Venn A Venn diagram is a widely used diagram style that shows the logical relation between sets, popularized by John Venn in the 1880s. The diagrams are used to teach elementary set theory, and to illustrate simple set relationships in probability, logic, statistics, linguistics and computer science. A Venn diagram uses simple closed curves drawn on a plane to represent sets. Very often, these curves are circles or ellipses. 4.1.2 Visualizing Distributions of Covariance Matrices Covariance matrices and their corresponding distributions play an important role in statistics. To understand the properties of distributions, we often rely on visualization methods. (Tokudaa and co 2011) Visualizing a distribution in a high-dimensional space is a challenge, with the additional difficulty that covariance matrices must be positive semi-definite, a restriction that forces the joint distribution of the covariances into an oddly-shaped subregion of the space. 4.1.3 A Geometrical Interpretation of an Alternative Formula for the Sample Covariance Kevin Hayes (Hayes 2011) proposes a new geometric and visual interpretation of covariance, based on the application of Heffermans formula for variance. He extends this formula to the covariance of a sample to extract his results. formula from heffernan definition of covariance : \\[cov(X,Y)= \\frac{2}{n(n-1)}\\sum_{i=1}^{n-1}\\sum_{j&gt;i}^{n}\\frac{1}{2}(x_i-x_j)(y_i - y_j)\\] Geometrically, \\(\\frac{1}{2}(x_i-x_j)(y_i - y_j)\\) is ±1 times the area right-triangle formed with the difference vector \\((x_i  x_j, y_j)\\) as its hypotenuse, where negatively sloped difference tors incur a \\((1)\\) sign and positively sloped difference vectors take a \\((+1)\\) sign. (Hayes 2011) 4.1.4 Covariance as Signed Area of Rectangles This article (Chudzicki 2014) was written following a very interesting conversation on the stats.statckexchange site. The initial topic of this conversation was: how to explain covariance to someone who only understands the notion of mean? instructions for use : Draw all possible such rectangles. Color them transparently, making the positive rectangles red (say) and the negative rectangles anti-red (blue). The covariance is the net amount of red in the plot (treating blue as negative values). Lets deduce some properties of covariance. Understanding of these properties will be accessible to anyone who has actually drawn a few of the rectangles. : Bilinearity. Because the amount of red depends on the size of the plot, covariance is directly proportional to the scale on the x-axis and to the scale on the y-axis. Correlation. Covariance increases as the points approximate an upward sloping line and decreases as the points approximate a downward sloping line. This is because in the former case most of the rectangles are positive and in the latter case, most are negative. Relationship to linear associations. Because non-linear associations can create mixtures of positive and negative rectangles, they lead to unpredictable (and not very useful) covariances. Linear associations can be fully interpreted by means of the preceding two characterizations. Sensitivity to outliers. A geometric outlier (one point standing away from the mass) will create many large rectangles in association with all the other points. It alone can create a net positive or negative amount of red in the overall picture. 4.2 Our current project: the package plotnetrec Le package plotnetrec et leurs graphiques associées est une représentation alternative dun scatterplot. Le but étant de déceler certaines particularités dun jeu de données. Il est temps de comparer avec certaines problèmatique. 4.2.1 Hetrogeneity 4.2.2 Heterosdasticity 4.2.3 Non linear relationship References "],["linear-regression-and-first-reliability-measure.html", "Chapter 5 Linear regression and first reliability measure 5.1 OLS two variables 5.2 OLS plus de deux variables", " Chapter 5 Linear regression and first reliability measure 5.1 OLS two variables 5.1.1 simple linear regression model \\[y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\] where \\(\\epsilon_i\\) is te error or deviation of \\(y_i\\) from the line \\(\\beta_0 + \\beta_1x_i\\) Ordinary least square We need to find the values of \\(\\beta_0\\) and \\(\\beta_1\\) that minimize the criterion : \\[S = \\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1x_i))^2\\] Minimize this sum gives : \\[\\hat{\\beta_0}= \\bar{y} - \\hat{\\beta_1}\\bar{x}\\] \\[\\hat{\\beta_1}= \\frac{\\sum_{i=1}^n(x_i -\\bar{x})(y_i -\\bar{y})}{\\sum_{i=1}^n(x_i -\\bar{x})^2}\\] estimated simple linear regression model : \\[y_i = \\hat{\\beta_0} + \\hat{\\beta_1x_i} + e_i\\] from which we can calculate a few additionnal quantities : \\(\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1x_i}\\) ; \\(\\hat{y_i}\\) is the predicted value (or predicted fit) of y for the \\(i^{th}\\) observation in the sample. \\(e_i=y_i -\\hat{y_i}\\) ; is the observed error (or residual) for the \\(i^{th}\\) observation in the sample. \\(SSE = \\sum_{i=1}^n (y_i -\\hat{y_i})^2\\) ; is the sum of squared observed errors for all observations in a sample of size \\(n\\) 5.1.2 Measuring overall variation from the sample line \\(MSE = \\frac{SSE}{n-p}\\) , where \\(p\\) is the number of parametrers of the regression equation. \\(p=2\\) for regression with only one variable. \\(s = RMSE = \\sqrt{(MSE)}\\) \\(SSTO=\\sum_{i=1}^n (y_i -\\bar{y_i})^2\\) coefficient of determination : \\(R^2 = \\frac{SSTO-SSE}{SSTO} = \\frac{SSR}{SSTO}\\) is the proportion of variation in \\(y\\) that is explained by \\(x\\). i.e. the coefficient of determination is then the ratio of the variance explained by the SSE regression to the total SST variance. The coefficient of determination is the square of the linear correlation coefficient R between the predicted values \\(\\hat{y}_{i}\\) and the measurements \\(y_i\\): \\[R^2=corr(\\hat{y}_{i},y_i)\\] \\(R^2\\) does not indicate whether: the independent variables are a cause of the changes in the dependent variable; omitted-variable bias exists; the correct regression was used; the most appropriate set of independent variables has been chosen; there is collinearity present in the data on the explanatory variables; the model might be improved by using transformed versions of the existing set of independent variables; there are enough data points to make a solid conclusion. 5.2 OLS plus de deux variables A remplir ou A enlever \\[{\\displaystyle{\\begin{cases}y_{1}=\\beta_{0}+\\beta_{1}x_{1,1}+\\ldots +\\beta_{p}x_{1,p}+\\varepsilon_{1}\\\\y_{2}=\\beta_{0}+\\beta_{1}x_{2,1}+\\ldots +\\beta_{p}x_{2,p}+\\varepsilon_{2}\\\\\\vdots \\\\y_{n}=\\beta_{0}+\\beta_{1}x_{n,1}+\\ldots +\\beta_{p}x_{n,p}+\\varepsilon _{n}\\end{cases}}}\\] Le but est de determiner les coefficients de cette équation: We need to use matrix writting to exprimer la regression linéaire multiple. \\[{\\displaystyle {\\begin{pmatrix}y_{1}\\\\\\vdots \\\\y_{n}\\end{pmatrix}}={\\begin{pmatrix}1&amp;x_{1,1}&amp;\\cdots &amp;x_{1,p}\\\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\\\1 &amp;x_{n,1}&amp;\\cdots &amp;x_{n,p}\\end{pmatrix}}{\\begin{pmatrix}\\beta_{0}\\\\\\beta_{1}\\\\\\vdots \\\\\\beta_{p}\\\\\\end{pmatrix}}+{\\begin{pmatrix}\\varepsilon_{1}\\\\\\vdots \\\\\\varepsilon _{n}\\\\\\end{pmatrix}}}\\] And so de manière compacte \\[Y = \\beta X + \\varepsilon \\] And like OLS two variables, we need to find \\(\\beta\\) vector that minimize the criterion : \\[{\\displaystyle \\min \\sum_{i=1}^{n}{\\hat{\\epsilon }}_{i}^{2}=\\min_{{\\hat{\\beta}}_{0},.,{\\hat{\\beta}}_{p}}\\sum_{i=1}^{n}(y_{i}-{\\hat{\\beta}}_{0}-{\\hat {\\beta}}_{1}x_{i,1}-\\cdots -{\\hat{\\beta}}_{p}x_{i,p})^{2}}\\] And so the OLS estimators is determined by: \\[\\displaystyle {\\hat {\\beta}}=(X^{T}X)^{-1}X^{T}Y\\qquad\\] Il est plutôt simple de prouver que la regression linéaire multiple est tout simplement des OLS simple successif. "],["vizualisation-of-regression-and-correlation-beetween-variable.html", "Chapter 6 Vizualisation of regression and correlation beetween variable 6.1 State of the art 6.2 Our current project, the plotnetrec package:", " Chapter 6 Vizualisation of regression and correlation beetween variable 6.1 State of the art 6.1.1 More on Venn Diagrams for Regression (figure 6.1) Kennedy (Kennedy 2002) extended the Venn diagram to the exposition of bias and variance in the context of the classical linear regression (CLR) model, written as y = Xb + e . Purple area: variation in y uniquely explained by variation in X A larger purple area means that more information is used in estimation, implying a smaller variance of the \\(\\beta_x\\) estimate. The black area: the variation in y that cannot be explained by X Another example of venn diagram: Relating species richness to the structure of continuous landscapes Venn diagram representing the partition of the variance of the response variable Y (species richness) between two sets of explanatory variables, namely landscape data (a) and space (c). The variance jointly explained by landscape data and space is represented by (b) in the diagram. The rectangle represents the total variance in Y (figure 6.2). 6.1.2 A Geometric Approach to Compare Variables in a Regression Model He proposes (Bring 1996) a geometric approaches to compare variables in a regression model. This article gives a brief introduction to the geometric approach in regres- sion analysis, and then geometry is used to shed some light on the problem of comparing the importance of the in- dependent variables in a multiple regression model. Even though no final answer of how to assess variable impor- tance is given, it is still useful to illustrate the different measures geometrically to gain a better understanding of their properties. The model vector, \\(\\hat{y}\\) is the perpendicular projection of y on x. Why it is the best estimate? The squared length of y, \\(||y||^2 = \\sum y_i^2\\) is the total sum of square, \\(SS_{tot}\\) The square length of \\(\\hat{y}\\), \\(||\\hat{y}||^2= \\sum \\hat{y}^2=SS_{reg}\\) The square length of the vector \\(y-\\hat{y}\\), \\(||y - \\hat{y}||^2=\\sum(y_i-\\hat{y}_i)^2 = SS_{res}\\) In other words, the observations vector is decomposed into a model vector and an error vector, and the shortest possible length of \\(y-\\hat{y}\\) is found by projecting y perpendicular in x. The shortest possible length of \\(y - \\hat{y}\\) is found by projecting \\(y\\) perpendicular on \\(x\\). \\(R^2 = \\frac{SS_{reg}}{SS_{tot}}= \\frac{||\\hat{y}||^2}{||y||^2}\\) (the \\(y\\) vector is standardized to have lenght 1.) \\[r_{xy} = \\begin{cases} ||\\hat{y}|| \\ \\ \\ \\text{if} \\ \\theta \\leq 90 \\degree \\ \\ \\ \\text{or} \\ \\theta \\geq 270 \\degree \\\\ -||\\hat{y}|| \\ \\ \\ \\text{if} \\ 90\\degree \\leq \\theta \\leq 270\\degree \\end{cases}\\] 6.1.3 Two Additional Views of Linear Regression Coefficients The author (Li 1964) proposes an interesting interpretation of the slope in the keast square method. The linear regression line of y on x, as determined by the method of least squares, passes through the central point with slope. A View of Linear Regression Coefficients as a weighted average slope Consider any point \\(P_i=(x_i,y_i)\\) and \\(P_o=(\\bar{x}, \\bar{y})\\), the slope of the line \\(P_i P_o\\) is : \\(b_i=\\frac{y_i -\\bar{y}}{x_i-\\bar{x}}\\) Lets attach a weight \\(w_i\\) to each \\(b_i\\), if 2 points are very close, their slope is not very reliable (need little weight) The distance between two points \\(\\to\\) the distance projected on the x-axis. To avoid negative weight, we may then take the weight, \\(w_i=(x_i-\\bar{x})^2\\) Adopting this system of weighting, we see that b is the weighted mean of the \\(b_i\\)s. \\[\\begin{equation} b=\\bar{b}=\\frac{\\sum b_i w_i}{\\sum w_i} = \\hat{\\beta_1} = \\frac{\\sum(x_i -\\bar{x})(y_i -\\bar{y})}{\\sum(x_i -\\bar{x})^2} \\end{equation}\\] This concept of weight for a slope is represented in the accompanying diagrams. (See Figure 1). The slope in the lefthand side diagram has a much larger weight than that in the righthand side for regression of y on x. If we were concerned with the regression of x on y, the reverse would be true. Note that the actual distance between the two points in the two diagrams is the same. 6.2 Our current project, the plotnetrec package: 6.2.1 OLS Diagramatic representation The principle of construction of the following diagram giving a diagrammatic representation of the regression and OLS is : we assume that the covariance (an average of all possible rectangles as seen previously among all pairs of points in a data sample) can be represented by a rectangle. This rectangle has by definition a known area given by the calculation of the empirical covariance \\(Cov(X,Y)\\). But the covariance depends on three parameters since \\(Cov(X,Y) =\\rho \\sigma_x \\sigma_y\\) . To represent the covariance and draw the corresponding the corresponding rectangle, we have to make an additional assumption i.e. choose a normalization . We assume that one side of this rectangle is normalized by \\(sigma_x\\). In this case, the other side necessarily has a length of \\(\\rho \\sigma_y\\). Following the same approach, we now assume that the variances of \\(X\\) and \\(Y\\) can be represented by two squares whose sides are of course the standard deviations of \\(X\\) and \\(Y\\) respectively. The diagram allows: to represent the OLS estimator by the slope of the diagonal of the rectangle representing the covariance. Geometrically, we obtain that if \\(X\\) increases by one standard deviation then \\(Y\\) increases by \\(\\rho\\) standard deviation of the dependent variable Y. to represent the explained variance in the total variance. Correctly represent the coefficient of determination as the relative share of the explained variance in the total variance. With Venn diagrams sometimes used to represent the \\(R^2\\), visual perception is based on the relative size of two surfaces and their intersection. The proposed diagram is not an ad hoc construction, it is based on a methodological and theoretical foundation allowing not only to represent the \\(R^2\\) but also to read its value correctly. reminder: \\[\\begin{align*} &amp; \\rho = \\frac{COV(X,Y)}{\\sigma_x \\sigma_y}\\\\ \\Leftrightarrow &amp; COV(X,Y) = \\rho \\sigma_y \\times \\sigma_x \\end{align*}\\] 6.2.1.1 reg lin multiple 6.2.2 Correlation representation Pour continuer avec des representations alternatives, nous proposons ici une représentaion alternative du coefficient de corrélation. For this, we need some reminders: \\[\\rho = \\frac{COV(X,Y)}{\\sigma_x \\sigma_y}\\] \\[V(X+Y) = V(X) + V(Y) + 2COV(X,Y)\\] Limits: \\(\\rho = 1\\) When \\(\\rho = 1\\) then \\(COV(X,Y) = \\sigma_x \\sigma_y\\) By replacing, we found: \\(V(X+Y) = V(X) + V(Y) + 2\\sigma_x \\sigma_y\\) Limits : \\(\\rho = -1\\) When \\(\\rho = -1\\) then \\(COV(X,Y) = -\\sigma_x \\sigma_y\\) By replacing, we found: \\(V(X+Y) = V(X) + V(Y) - 2\\sigma_x \\sigma_y\\) Limits: \\(\\rho = 0\\) When \\(\\rho = 0\\) then \\(COV(X,Y) = 0\\) By replacing, we found: : \\(V(X+Y) = V(X) + V(Y)\\) Interest Correlation matrix with distinct mathematical construction. Nous devons regarder dynamiquement ce qui se passe. En partant du cas où les variables sont indépendant (\\(\\rho = 0\\)), si la corrélation augmente de manière positive, le carré augmente son air et pivote vers le cas où la corrélation est parfaitement positive. Inversement, si la corrélation augmente de manière négative, le carré diminue dair et pivote vers le cas où la corrélation est parfaitement négatif References "],["causality.html", "Chapter 7 Causality 7.1 The main cases 7.2 Measurement error on the dependent variable 7.3 Measurement error on the independent variable 7.4 Instrumental variables", " Chapter 7 Causality In the basic model \\(y^=\\alpha+\\beta x+\\varepsilon\\) the OLS estimator \\(\\hat{\\beta}\\) is unbiased if \\(x\\) is independent of \\(\\varepsilon\\). We cannot know a priori if this is the case and in any case not a posteriori at the end of the estimation since the residuals are by construction independent of \\(X\\). We can easily show that there is no unique solution in \\(\\hat{\\beta}\\) of the maximum likelihood when we increase the model by a free parameter representing the correlation between \\(\\varepsilon\\) and \\(x\\). The prediction \\(\\mathbb{E}(y~/x)=\\hat{\\beta}x\\) is correct only if \\(\\mathbb{E}(\\varepsilon~/x)=0\\). Otherwise \\(\\mathbb{E}(\\varepsilon~/x)=0\\) there is an endogeneity and \\(\\hat{\\beta}\\) is not an exact measure of the marginal effect of an exogenous variation of \\(x\\). The main situations of endogeneity encountered and studied are related to the problems : Simultaneous causality Omitted variables Measurement errors on the variables Selection bias Poorly specified functional form 7.1 The main cases 7.1.1 Simultaneous causality Is it enough to simply show by a diagram that regressing y on x and then x on y does not give the same result? The answer is no, it is not enough! 7.1.2 Omitted variables . Repeat the previous results in a simplified way to illustrate the effect of omitting a variable and present the conditions under which the omission of a variable leads to a bias on the model parameters. 7.1.3 Measurement errors Lets consider the following RLS model \\[y^{\\star}=\\alpha+\\beta x^{\\star}+\\varepsilon\\] We do not observe \\(y^{\\star}\\) (but \\(y=y^{\\star}+v_y\\)), nor \\(x^{\\star}\\) (but \\(x=x^{\\star}+u_x\\)). Consider the following assumptions: \\(\\mathbb{E}(u_x)=\\mathbb{E}(v_y)=0\\) \\(\\mathbb{E}(y v_y)=\\mathbb{E}(y u_x)=\\mathbb{E}(x u_y)=\\mathbb{E}(x v_x)=0\\) \\(\\mathbb{E}(\\varepsilon u_y)=0\\) The measurement errors are of zero expectation, and independent of the variables and the error term. Under these assumptions, for two randomly chosen points \\(k\\) \\(l\\), the covariance between \\(X\\) and \\(Y\\) does not change. \\[\\mathbb{E}[(x_k-x_l)(y_k-y_l)]\\] \\[\\mathbb{E}[(x_k^{\\star}+u_k-(x_l^{\\star}+u_l))(y_k^{\\star}+v_k-(y_l^{\\star}+v_l))]\\] \\[\\mathbb{E}[(x_k^{\\star}+u_k)(y_k^{\\star}+v_k)] -\\mathbb{E}[x_k^{\\star}+u_k]\\mathbb{E}[y_l^{\\star}+v_l] -\\mathbb{E}[x_l^{\\star}+u_l]\\mathbb{E}[y_k^{\\star}+v_k]+ \\mathbb{E}[(x_l^{\\star}+u_l)(y_l^{\\star}+v_l)]\\] \\[2Cov(x^{\\star},y^{\\star})\\] Measurement errors are detrimental to accuracy but on average they do not affect the covariance. The only essential problem is therefore related to the correct estimation of the parameters, while the correlation between the dependent and independent variables does not change. On the other hand, one should not confuse the effect of \\(x\\) on \\(y\\) with the effect of the measurement error of \\(x\\) on \\(y\\) (which is zero). 7.2 Measurement error on the dependent variable Let us consider a measurement error on the dependent variable \\(y\\), i.e. let \\(\\sigma_v^2 &gt; 0\\) and \\(\\sigma_u^2 = 0\\). The model becomes: \\[y=alpha+beta x + v+ \\varepsilon\\] The measurement error on the dependent variable does not matter in the sense that it has no biasing effect on the estimated parameters of the model. In practice, it can be considered as contributing to the disturbance term of the model. It is obviously undesirable, as anything that increases the noise in the model will tend to make the regression estimates less accurate, but it has no impact in terms of bias in the estimates. In this case, we do not make a diagram for a graphical proof because it is so obvious! 7.3 Measurement error on the independent variable We consider the case where the measurement error is on the dependent variable \\(x\\), i.e. Let \\(sigma_v^2 =0\\) and \\(sigma_u^2 &gt;0\\). The model becomes: \\[y=alpha+beta x -beta u + \\varepsilon\\] In the regression of \\(y\\) on \\(x\\), the measurement error of \\(x\\) becomes part of the error in the regression equation related to the parameter \\(\\beta\\), thus creating an endogeneity bias. The OLS estimator \\(\\hat{\\beta}=\\frac{Cov(x,y)}{Var(x)}\\) is biased towards 0. The unbiased estimator is \\(\\beta=frac{Cov(x^\\star ,y)}{Var(x^\\star)}\\). Proof: Since \\(Cov(x^\\star,y)=Cov(x,y)\\), we have: \\[\\frac{\\hat{\\beta}}{\\beta}=\\frac{Var(x^\\star)}{Var(x)}=\\frac{\\sigma_x^{\\star 2}}{\\sigma_u^2+\\sigma_x^{\\star 2}} \\implies plim~\\hat{\\beta}=\\lambda \\beta\\] with \\(\\lambda=\\frac{\\sigma_x^{\\star 2}}{\\sigma_u^2+\\sigma_x^{\\star 2}}\\). Since \\(0&lt;lambda&lt;1\\) the coefficient \\(\\hat{\\beta}\\) is biased towards 0. The bias depends on the level and the sign of \\(\\beta\\): \\[plim~\\hat{\\beta}-\\beta=\\lambda \\beta-\\beta=-(1-\\lambda)\\beta=-\\frac{\\sigma_u^2}{\\sigma_u^2+\\sigma_x^{\\star 2}}\\beta\\] ### Diagram of the model with measurement error on the independent variable The following figure illustrates the problem in a very simple way. 7.4 Instrumental variables \\[Y=\\alpha + \\beta X +u\\] \\[X=\\theta + \\delta Z +v\\] \\[\\beta_{IV}=\\frac{Cov(Y,Z)}{Cov(X,Z)} =\\frac{\\rho_{zy} \\sigma_y}{\\rho_{zx} \\sigma_x}\\] "],["etat-des-lieux-des-package-r-en-économétrie-de-la-visualisation.html", "Chapter 8 Etat des lieux des package R en économétrie de la visualisation", " Chapter 8 Etat des lieux des package R en économétrie de la visualisation viscov Our visualizations follow the principle of decomposing a covariance matrix into scale parameters and correlations, pulling out marginal summaries where possible and using two and three-dimensional plots to reveal multivariate structure. Visualizing a distribution of covariance matrices is a step beyond visualizing a single covariance matrix or a single multivariate dataset. Colourpicker Colourpicker is a tool for Shiny framework and for selecting colours in plots. This tool supports various options, such as alpha opacity, custom colour palettes, and more. The most common uses of this tool include the utilisation of the colourInput() function to create a colour input in Shiny as well as the use of the plotHelper() function/RStudio Addin to select colours for a plot. Esquisse The esquisse package allows a user to interactively explore data by visualising it with the ggplot2 package. It allows a user to draw bar graphs, curves, scatter plots, histograms, export the graphs, and retrieve the code generating the graph. With the help of esquisse, one can quickly visualise the data according to their type as well as export to PNG or PowerPoint, and retrieve the code to reproduce the chart. ggplot2 ggplot is a popular package that is based on the grammar of graphics. The idea behind this library is that one can build every graph from the same components, such as a dataset, a coordinate system, and more. The package provides graphics language for creating intuitive and intricate plots. It allows a user to create graphs that represent both univariate and multivariate numerical and categorical data. ggvis ggvis is a data visualisation package for R that allows to declaratively describe data graphics with a syntax similar in spirit to ggplot2. It allows creating rich interactive graphics locally in Rstudio or in the browser as well as leverage the infrastructure of the Shiny package to publish interactive graphics usable from any browser. The goal of ggvis is to make it easy to build interactive graphics for exploratory data analysis. ggforce The ggforce is a package aimed at providing missing functionality to ggplot2 through the extension system introduced with ggplot2 v2.0.0. The goal of this package is to provide a repository of geoms, stats, among others. Using ggforce, one can enhance almost any ggplot by highlighting data groupings and focusing attention on interesting features of the plot. lattice Lattice is a powerful high-level data visualisation system for R that is designed with an emphasis on multivariate data and allows to create multiple small plots easily. The lattice package attempts to improve on base R graphics by providing better defaults and the ability to display multivariate relationships easily. Particularly, the package supports the creation of trellis graphs that show a variable or the relationship between variables, conditioned on one or more other variables. Plotly Plotly is an open-source R package for creating interactive web-based graphs via the open-source JavaScript graphing library plotly.js. The Plotlys R graphing library helps in creating interactive, publication-quality graphs including line plots, scatter plots, area charts, bar charts, error bars, etc. One can use Plotly for R to make, view and distribute charts and maps online as well as offline. patchwork patchwork is a package that expands the API to allow for the arbitrarily complex composition of plots by providing mathematical operators for combining multiple plots. The goal of patchwork is to make it simple to incorporate separate ggplots into the same graphic. quantmod quantmod is an R package that provides a framework for quantitative financial modelling and trading. It provides a rapid prototyping environment that makes modelling easier by removing the repetitive workflow issues surrounding data management and visualisation. RGL The RGL package is used to produce interactive 3-D plots using OpenGL. The library contains high-level graphics commands modelled loosely after classic R graphics and working in three dimensions. It also includes a low-level structure inspired by the grid package. RGL provides medium to high-level functions for 3D interactive graphics, including functions modelled on base graphics as well as functions for constructing representations of geometric objects. Highcharter is an R wrapper for Highcharts, an interactive visualization library in JavaScript. Like its predecessor, highcharter features a powerful API. Highcharter makes dynamic charting easy. It uses a single function, hchart(), to draw plots for all kinds of R object classes, from data frame to dendrogram to phylo. It also gives R coders a handy way to access the other popular Highcharts plot types, Highstock (for financial charting) and Highmaps (for schematic maps in web-based projects). Leaflet Like highcharter, Leaflet for R is another charting packaged based on a hugely-popular JavaScript library of the same name. Leaflet offers a lightweight but powerful way to build interactive maps, which youve probably seen in action (in their JS form) on sites ranging from The New York Times and The Washington Post to GitHub and GIS specialists like Mapbox and CartoDB. The R interface for Leaflet was developed using the htmlwidgets framework, which makes it easy to control and integrate Leaflet maps right in R Markdown documents (v2), RStudio, or Shiny apps. RcolorBrewer RColorBrewer makes it easy to take advantage of one of Rs great strengths: manipulating colors in plots, graphs, and maps. The package is based on Cynthia Brewers work on the use of color in cartography (check out Colorbrewer to learn more), and it lets you create nice-looking sequential, diverging, or qualitative color palettes. It also plays nicely with Plotly, as these examples by Plotly demonstrate. dygraphs This package provides an R interface for dygraphs, a fast, flexible JavaScript charting library for exploring time-series data sets. Whats powerful about dygraphs is that its interactive right out of the box, with default mouse-over labels, zooming, and panning. Its got lots of nifty other interactivity features, like synchronization or the range selector shown above. But dygraphs interactivity doesnt come at the expense of speed: it can handle huge datasets with millions of points without slowing its roll. And you can use RColorBrewer with dygraphs to choose a different color palette for your time series check out this example to see how. sunburst pas forcément utile pour notre sujet VennDiagram Create a Venn diagram and save it into a file. The function venn.diagram() takes a list and creates a file containing a publication-quality Venn Diagram. "],["state-of-the-art-of-r-packages-in-visualization-econometrics-and-data-viz-more-broadly.html", "Chapter 9 State of the art of R packages in visualization econometrics (and data-viz more broadly)", " Chapter 9 State of the art of R packages in visualization econometrics (and data-viz more broadly) R energy A compléter viscov Our visualizations follow the principle of decomposing a covariance matrix into scale parameters and correlations, pulling out marginal summaries where possible and using two and three-dimensional plots to reveal multivariate structure. Visualizing a distribution of covariance matrices is a step beyond visualizing a single covariance matrix or a single multivariate dataset. Colourpicker Colourpicker is a tool for Shiny framework and for selecting colours in plots. This tool supports various options, such as alpha opacity, custom colour palettes, and more. The most common uses of this tool include the utilisation of the colourInput() function to create a colour input in Shiny as well as the use of the plotHelper() function/RStudio Addin to select colours for a plot. Esquisse The esquisse package allows a user to interactively explore data by visualising it with the ggplot2 package. It allows a user to draw bar graphs, curves, scatter plots, histograms, export the graphs, and retrieve the code generating the graph. With the help of esquisse, one can quickly visualise the data according to their type as well as export to PNG or PowerPoint, and retrieve the code to reproduce the chart. ggplot2 ggplot is a popular package that is based on the grammar of graphics. The idea behind this library is that one can build every graph from the same components, such as a dataset, a coordinate system, and more. The package provides graphics language for creating intuitive and intricate plots. It allows a user to create graphs that represent both univariate and multivariate numerical and categorical data. ggvis ggvis is a data visualisation package for R that allows to declaratively describe data graphics with a syntax similar in spirit to ggplot2. It allows creating rich interactive graphics locally in Rstudio or in the browser as well as leverage the infrastructure of the Shiny package to publish interactive graphics usable from any browser. The goal of ggvis is to make it easy to build interactive graphics for exploratory data analysis. ggforce The ggforce is a package aimed at providing missing functionality to ggplot2 through the extension system introduced with ggplot2 v2.0.0. The goal of this package is to provide a repository of geoms, stats, among others. Using ggforce, one can enhance almost any ggplot by highlighting data groupings and focusing attention on interesting features of the plot. lattice Lattice is a powerful high-level data visualisation system for R that is designed with an emphasis on multivariate data and allows to create multiple small plots easily. The lattice package attempts to improve on base R graphics by providing better defaults and the ability to display multivariate relationships easily. Particularly, the package supports the creation of trellis graphs that show a variable or the relationship between variables, conditioned on one or more other variables. Plotly Plotly is an open-source R package for creating interactive web-based graphs via the open-source JavaScript graphing library plotly.js. The Plotlys R graphing library helps in creating interactive, publication-quality graphs including line plots, scatter plots, area charts, bar charts, error bars, etc. One can use Plotly for R to make, view and distribute charts and maps online as well as offline. patchwork patchwork is a package that expands the API to allow for the arbitrarily complex composition of plots by providing mathematical operators for combining multiple plots. The goal of patchwork is to make it simple to incorporate separate ggplots into the same graphic. quantmod quantmod is an R package that provides a framework for quantitative financial modelling and trading. It provides a rapid prototyping environment that makes modelling easier by removing the repetitive workflow issues surrounding data management and visualisation. RGL The RGL package is used to produce interactive 3-D plots using OpenGL. The library contains high-level graphics commands modelled loosely after classic R graphics and working in three dimensions. It also includes a low-level structure inspired by the grid package. RGL provides medium to high-level functions for 3D interactive graphics, including functions modelled on base graphics as well as functions for constructing representations of geometric objects. Highcharter is an R wrapper for Highcharts, an interactive visualization library in JavaScript. Like its predecessor, highcharter features a powerful API. Highcharter makes dynamic charting easy. It uses a single function, hchart(), to draw plots for all kinds of R object classes, from data frame to dendrogram to phylo. It also gives R coders a handy way to access the other popular Highcharts plot types, Highstock (for financial charting) and Highmaps (for schematic maps in web-based projects). Leaflet Like highcharter, Leaflet for R is another charting packaged based on a hugely-popular JavaScript library of the same name. Leaflet offers a lightweight but powerful way to build interactive maps, which youve probably seen in action (in their JS form) on sites ranging from The New York Times and The Washington Post to GitHub and GIS specialists like Mapbox and CartoDB. The R interface for Leaflet was developed using the htmlwidgets framework, which makes it easy to control and integrate Leaflet maps right in R Markdown documents (v2), RStudio, or Shiny apps. RcolorBrewer RColorBrewer makes it easy to take advantage of one of Rs great strengths: manipulating colors in plots, graphs, and maps. The package is based on Cynthia Brewers work on the use of color in cartography (check out Colorbrewer to learn more), and it lets you create nice-looking sequential, diverging, or qualitative color palettes. It also plays nicely with Plotly, as these examples by Plotly demonstrate. dygraphs This package provides an R interface for dygraphs, a fast, flexible JavaScript charting library for exploring time-series data sets. Whats powerful about dygraphs is that its interactive right out of the box, with default mouse-over labels, zooming, and panning. Its got lots of nifty other interactivity features, like synchronization or the range selector shown above. But dygraphs interactivity doesnt come at the expense of speed: it can handle huge datasets with millions of points without slowing its roll. And you can use RColorBrewer with dygraphs to choose a different color palette for your time series check out this example to see how. sunburst pas forcément utile pour notre sujet VennDiagram Create a Venn diagram and save it into a file. The function venn.diagram() takes a list and creates a file containing a publication-quality Venn Diagram. "],["references.html", "References", " References "],["references-1.html", "References", " References "],["annexes.html", "Chapter 10 annexes 10.1 Correction de Bessels proof 10.2 Proof for \\(Cov(x,~y)=\\frac{1}{2N(N-1)}\\sum_{i=1}^{N} \\sum_{j=1}^{N}~(x_i-x_j)(y_i-y_j)\\) 10.3 a little bit of calculation", " Chapter 10 annexes 10.1 Correction de Bessels proof \\[\\begin{align*} (n-1)S_{xy} &amp;= \\sum_{i=1}^{N}(x_i-\\bar x)(y_i - \\bar y) \\\\ &amp;= \\sum_{i=1}^{N} x_i y_i -N\\bar x \\bar y\\\\ &amp;= \\sum_{i=1}^{N} x_i y_i - \\frac{1}{N}\\sum_{i=1}^{N} x_i \\sum_{i=1}^{N} y_i\\\\ (N-1) \\mathbb{E}(S_{xy}) &amp;= \\mathbb{E}\\left(\\sum_{i=1}^{N} x_i y_i\\right) - \\frac{1}{N}\\mathbb{E}\\left(\\sum_{i=1}^{N} x_i \\sum_{i=1}^{N} y_i\\right)\\\\ &amp;= N\\mu_{xy} - \\frac{1}{N}[N\\mu_{xy} + N(N-1)\\mu_x \\mu_y]\\\\ &amp;= (N-1)[\\mu_{xy}-\\mu_x\\mu_y]\\\\ &amp;= (N-1)Cov(x,~y) \\end{align*}\\] 10.2 Proof for \\(Cov(x,~y)=\\frac{1}{2N(N-1)}\\sum_{i=1}^{N} \\sum_{j=1}^{N}~(x_i-x_j)(y_i-y_j)\\) Proof 1 \\[Cov(x,~y)=\\frac{1}{2N(N-1)} \\sum_{i=1}^{N} \\sum_{j=1}^{N}~(x_i-x_j)(y_i-y_j) =\\] \\[\\frac{1}{2N(N-1)} \\sum_{i=1}^{N} N~x_i~y_i -x_i~N~\\overline{y}-y_i~N~\\overline{x}+N~\\overline{xy}\\] \\[\\frac{2N^2}{2N(N-1)}(\\overline{xy}-\\overline{x}~\\overline{y})=\\] \\[\\frac{N}{N-1}(\\overline{xy}-\\overline{x}~\\overline{y}))\\] Proof 2 another way, not uninteresting, to show the equivalence between the two formulations of the covariance but starting from the usual form. \\[Cov(x,~y)=\\frac{1}{(N-1)} \\sum_{i=1}^{N} (x_i-\\overline{x})(y_i -\\overline{y})\\] \\[Cov(x,~y)=\\frac{1}{2(N-1)} \\left\\{ \\sum_{i=1}^{N} (x_i-\\overline{x})(y_i -\\overline{y}) + \\sum_{j=1}^{N} (x_j-\\overline{x})(y_j -\\overline{y}) \\right\\}\\] \\[Cov(x,~y)=\\frac{1}{2N(N-1)} \\sum_{i=1}^{N} \\sum_{j=1}^{N}\\left\\{(x_i-\\overline{x})(y_i -\\overline{y}) + (x_j-\\overline{x})(y_j -\\overline{y}) \\right\\}\\] \\[Cov(x,~y)=\\frac{1}{2N(N-1)} \\sum_{i=1}^{N} \\sum_{j=1}^{N}\\left\\{(x_i-x_j)(y_i-y_j)+x_iy_j+x_jy_i-x_i\\overline{y}-x_j\\overline{y}-y_i\\overline{x}-y_j\\overline{x}+2~\\overline{x}~\\overline{y}\\right\\}\\] \\[Cov(x,~y)=\\frac{1}{2N(N-1)} \\sum_{i=1}^{N} \\sum_{j=1}^{N}\\left\\{(x_i-x_j)(y_i-y_j)\\right\\}\\] 10.3 a little bit of calculation 10.3.0.1 Variance of the sum of two random variables \\[\\begin{align*} Var(X+Y) &amp;= Cov(X+Y,X+Y) \\\\ &amp;= E((X+Y)^2)-E(X+Y)E(X+Y) \\\\ \\text{en développant,} \\\\ &amp;= E(X^2) - (E(X))^2 + E(Y^2) - (E(Y))^2 + 2(E(XY) - E(X)E(Y)) \\\\ &amp;= Var(X) + Var(Y) + 2(E(XY)) - E(X)E(Y)) \\\\ &amp;= Var(X) + Var(Y) + 2 Cov(X,~Y) \\\\ \\end{align*}\\] If the variables \\(X,~Y\\) are independent \\(E(XY) = E(X)E(Y)\\) then \\(Var(X+Y) = Var(X) + Var(Y)\\) \\[\\begin{align*} \\Delta &amp;= 4Cov(X,~Y)^2-4Var(X)Var(Y)\\\\ &amp;=4[Cov(X,~Y)^2-Var(X)Var(Y)] \\leq 0\\\\ \\end{align*}\\] So $ Cov(X,Y)^2 Var(X) Var(Y)$ (In probability theory, the Cauchy-Schwarz inequality allows the proof of the result because of the inequality \\(E(XY)\\leq \\sqrt{E(X^2)E(Y^2)}\\)) or $-(X,Y) _x_y _x_y $. Let us notice that if \\(\\Delta=0\\) then we have the equality $ Cov(X,Y)^2=Var(X) Var(Y)$, i.e. if there exists \\(~\\lambda\\) such that \\(Var(\\lambda X+Y)=0\\)\\ But to conclude, if we have \\(~lambda X+Y\\) equal with probability 1 to a constant, lets say \\(c\\), it is indeed that \\(Y = c -~lambda X\\) almost surely, in other words a perfect linear link between the two variables. 10.3.0.2 Definition of the linear correlation coefficient (Bravais-Pearson) case \\(Cov(X,~Y)\\geq 0\\) We define \\(\\rho = \\frac{2Cov(X,~Y)}{Var_{max}-Var_{min}}\\) \\(Var_{max} = \\sigma_X^2+\\sigma_Y^2+2\\sigma_X\\sigma_Y\\) \\(Var_{min} = \\sigma_X^2+\\sigma_Y^2\\) \\(Var_{max}-Var_{minx} = 2\\sigma_X\\sigma_Y\\) from which, simplifying \\(\\rho = \\frac{Cov(X,~Y)}{\\sigma_X\\sigma_Y}\\) case \\(Cov(X,~Y)\\leq 0\\) We define \\(\\rho = \\frac{2Cov(X,~Y)}{Var_{max}-Var_{min}}\\) \\(Var_{max} = \\sigma_X^2+\\sigma_Y^2\\) \\(Var_{min} = \\sigma_X^2+\\sigma_Y^2-2\\sigma_X\\sigma_Y\\) \\(Var_{max}-Var_{minx} = 2\\sigma_X\\sigma_Y\\) from which, simplifying \\(\\rho = \\frac{Cov(X,~Y)}{\\sigma_X\\sigma_Y}\\) 10.3.0.3 Covariance framing \\[-\\sqrt{(Var(X)Var(Y)} \\leq Cov(X,Y) \\leq \\sqrt{(Var(X)Var(Y)}\\] With in the particular case of standardized variables \\[\\begin{align*} Var(\\frac{X}{\\sigma_X}+\\frac{Y}{\\sigma_Y})&amp;= Var(\\frac{X}{\\sigma_X})+Var(\\frac{Y}{\\sigma_Y})+2Cov(\\frac{X}{\\sigma_X},\\frac{Y}{\\sigma_Y})\\\\ &amp;=2(1+\\rho)\\\\ Var(\\frac{X}{\\sigma_X}-\\frac{Y}{\\sigma_Y})&amp;= Var(\\frac{X}{\\sigma_X})+Var(\\frac{Y}{\\sigma_Y})-2Cov(\\frac{X}{\\sigma_X},\\frac{Y}{\\sigma_Y})\\\\ &amp;=2(1-\\rho) \\end{align*}\\] One finds, knowing that \\(2(1+\\rho) \\geq 0\\) and \\(2(1-\\rho)\\geq 0\\) that one has \\(-1 \\le \\rho \\le 1\\) "],["notes.html", "Chapter 11 Notes", " Chapter 11 Notes titre graphique; positionnement et taille graphique reference aux graphiques et bibliographies tout est ajoutable, déplacable, enlevable, modifiable. AUcun soucis Bibliographie : parcourir doc et rajouter aticles oubliés dans bibliographie position graphiques mal centrés ou mal dimensionné. possibilité de mettre code R directement Certaines parties vides: a remplir ou à mettre de coté pour linstant ( on verra au second semestre) rajouter plus dapplications R pour illustrer? plutot au second semestre je pense prendre dautres passages du beamer possible le code latek brut ne sort pas ds le html "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
