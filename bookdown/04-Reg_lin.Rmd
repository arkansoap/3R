# Linear regression and first reliability measure

## OLS two variables 

### simple linear regression model 

$$y_i = \beta_0 + \beta_1x_i +  \epsilon_i$$ 
where $\epsilon_i$ is te error or deviation of $y_i$ from the line $\beta_0 + \beta_1x_i$

- Ordinary least square 

We need to find the values of $\beta_0$ and $\beta_1$ that minimize the criterion : 
$$S = \sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n (y_i - (\beta_0 + \beta_1x_i))^2$$

Minimize this sum gives :

$$\hat{\beta_0}= \bar{y} - \hat{\beta_1}\bar{x}$$
$$\hat{\beta_1}= \frac{\sum_{i=1}^n(x_i -\bar{x})(y_i -\bar{y})}{\sum_{i=1}^n(x_i -\bar{x})^2}$$

- estimated simple linear regression model :

$$y_i = \hat{\beta_0} + \hat{\beta_1x_i} +  e_i$$ 

from which we can calculate a few additionnal quantities :

- $\hat{y_i} = \hat{\beta_0} + \hat{\beta_1x_i}$ ; $\hat{y_i}$ is the **predicted value** (or predicted fit) of y for the $i^{th}$ observation in the sample.

- $e_i=y_i -\hat{y_i}$ ; is the **observed error** (or residual) for the $i^{th}$ observation in the sample.

- $SSE = \sum_{i=1}^n (y_i -\hat{y_i})^2$ ; is the **sum of squared observed errors** for all observations in a sample of size $n$

### Measuring overall variation from the sample line

- $MSE = \frac{SSE}{n-p}$ , where $p$ is the number of parametrers of the regression equation. $p=2$ for regression with only one variable. 

- $s = RMSE = \sqrt{(MSE)}$

- $SSTO=\sum_{i=1}^n (y_i -\bar{y_i})^2$

- coefficient of determination :

$R^2 = \frac{SSTO-SSE}{SSTO} = \frac{SSR}{SSTO}$ is the proportion of variation in $y$ that is explained by $x$.

i.e. the coefficient of determination is then the ratio of the variance explained by the SSE regression to the total SST variance.

The coefficient of determination is the square of the linear correlation coefficient R between the predicted values $\hat{y}_{i}$ and the measurements $y_i$:

$$R^2=corr(\hat{y}_{i},y_i)$$

$R^2$ does not indicate whether:

- the independent variables are a cause of the changes in the dependent variable;
- omitted-variable bias exists;
- the correct regression was used;
- the most appropriate set of independent variables has been chosen;
- there is collinearity present in the data on the explanatory variables;
- the model might be improved by using transformed versions of the existing set of independent variables;
- there are enough data points to make a solid conclusion.

## OLS plus de deux variables

**A remplir ou A enlever**

$${\displaystyle{\begin{cases}y_{1}=\beta_{0}+\beta_{1}x_{1,1}+\ldots +\beta_{p}x_{1,p}+\varepsilon_{1}\\y_{2}=\beta_{0}+\beta_{1}x_{2,1}+\ldots +\beta_{p}x_{2,p}+\varepsilon_{2}\\\vdots \\y_{n}=\beta_{0}+\beta_{1}x_{n,1}+\ldots +\beta_{p}x_{n,p}+\varepsilon _{n}\end{cases}}}$$

Le but est de determiner les coefficients de cette équation:

We need to use matrix writting to exprimer la regression linéaire multiple.

$${\displaystyle {\begin{pmatrix}y_{1}\\\vdots \\y_{n}\end{pmatrix}}={\begin{pmatrix}1&x_{1,1}&\cdots &x_{1,p}\\\vdots &\vdots &\ddots &\vdots \\1 &x_{n,1}&\cdots &x_{n,p}\end{pmatrix}}{\begin{pmatrix}\beta_{0}\\\beta_{1}\\\vdots \\\beta_{p}\\\end{pmatrix}}+{\begin{pmatrix}\varepsilon_{1}\\\vdots \\\varepsilon _{n}\\\end{pmatrix}}}$$

And so de manière compacte

$$Y = \beta X + \varepsilon $$

And like OLS two variables, we need to find $\beta$ vector that minimize the criterion : 

$${\displaystyle \min \sum_{i=1}^{n}{\hat{\epsilon }}_{i}^{2}=\min_{{\hat{\beta}}_{0},.,{\hat{\beta}}_{p}}\sum_{i=1}^{n}(y_{i}-{\hat{\beta}}_{0}-{\hat {\beta}}_{1}x_{i,1}-\cdots -{\hat{\beta}}_{p}x_{i,p})^{2}}$$

And so the OLS estimators is determined by: 
$$\displaystyle {\hat {\beta}}=(X^{T}X)^{-1}X^{T}Y\qquad$$

Il est plutôt simple de prouver que la regression linéaire multiple est tout simplement des OLS simple successif.