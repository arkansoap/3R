---
output:
  pdf_document: default
  html_document: default
---
# Covariance 

## Variance definition reminder

In the fields of statistics and probability theory, the variance can be define as a measure of a sample's values dispersion or a measure of a probability distribution. 
According to the KÃ¶nig-Huygens theorem, the variance show the gap between the squares of the values of the variable and the square of the mean.

- Classical formula of variance:  

$$\sigma^2_x=\frac{1}{N}\sum_{x=1}^{N}(x_i - \bar{x})^2 = \frac{1}{N}\sum_{x=1}^{N}x_i^2 - \bar{x}^2$$

- A new proposition for variance formula[@Heffernan]: 

$$\sigma^2_x= \frac{1}{N(N-1)}\sum_{i=1}^{N-1}\sum_{j>i}^{N}(x_i-x_j)^2$$

This formulation (extended to covariance) will be discussed again in this study. Intuitively, we can see here the geometrical aspect of the variance of $x$ perceived as the expression of a square.

- Variance versus Covariance: 

Variance and covariance are mathematical terms frequently used in statistics and probability theory. Variance refers to the spread of a data set around its mean value, while a covariance refers to the measure of the directional relationship between two random variables.

## Covariance standard definition

Covariance as an extension of variance notion. The covariance between two random variables is a number which provide us the joint difference to there respective means in quantity.  

A covariance refers to the measure of how two random variables will change when they are compared to each other.

Intuitively, covariance is a measure of the simultaneous variation of two random variables. That is, the covariance becomes more positive for each pair of values that differ from their mean in the same direction, and more negative for each pair of values that differ from their mean in the opposite direction.

The sign of the covariance therefore shows the tendency in the linear relationship between the variables. The magnitude of the covariance is not easy to interpret because it is not normalized and hence depends on the magnitudes of the variables.

The covariance of two independent random variables is zero, although the converse is not always true.

When we have more than 2 variables, the concept naturally generalizes through the covariance matrix (or the variance-covariance matrix). Let be a set of $p$ real random variables $X_1, X_2, \dots, X_p$, then the covariance matrix is the square matrix of which the term line $i$ and the term column $j$ is the covariance of the variables $X_i$ and $X_j$. 
This matrix quantifies the variation of each variables compared to each of the others. 

## Mathematical standard and alternatives covariance definition

### Standard definition

- Covariance formula:

For two jointly distributed real-valued random variables $X$ and $Y$ with finite, the covariance is defined as the expected value (or mean) of the product of their deviations from their individual expected values.
$$
Cov(X,Y)=\mathbb{E}((X-\mathbb{E}(X))\times(Y-\mathbb{E}(Y)))
$$

Where $\mathbb{E}(X)$ is the expected value of $X$, also known as the mean of $X$. The covariance is also sometimes denoted  $\sigma_{X,Y}$, in analogy to variance. By using the linearity property of expectations, this can be simplified to the expected value of their product minus the product of their expected values:

\begin{align*}
  Cov(X,Y) &=\mathbb{E}((X-\mathbb{E}(X))\times(Y-\mathbb{E}(Y))) \\
  &=\mathbb{E}(XY- X\mathbb{E}(Y) - \mathbb{E}(X)Y + \mathbb{E}(x)\mathbb{E}(Y)) \\
  &= \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y) - \mathbb{E}(x)\mathbb{E}(Y) + \mathbb{E}(x)\mathbb{E}(Y) \\
  &= \mathbb{E}(XY)-\mathbb{E}(X)\mathbb{E}(Y)
\end{align*}

The empirical covariance of a sample is defined by:
$$
Cov(X,~Y)=\frac{1}{n}
\sum_{i=1}^{n} (x_i-\overline{x})(y_i -\overline{y})
$$
With, $\overline{x}=\frac{1}{n}\sum_{i=1}^{n} x_j$ and $\overline{y}=\frac{1}{n}\sum_{i=1}^{n} y_j$
an unbiased estimator of the population.  

Covariance is defined by:

$$
Cov(X,~Y)=\frac{1}{(n-1)}
\sum_{i=1}^{n} (x_i-\overline{x})(y_i -\overline{y})
$$

or equivalently:
$$
Cov(X,~Y)=\frac{n}{n-1}(\overline{xy}-\overline{x}~\overline{y})
$$

### Alternative definition, a story of rectangles 

- Formula from Heffernan definition of covariance: 

$$
Cov(X,Y)= \frac{2}{n(n-1)}\sum_{i=1}^{n-1}\sum_{j>i}^{n}\frac{1}{2}(x_i-x_j)(y_i - y_j)
$$

Let be two random variables $(X,~Y)$, and a sample of $N$ pairs of independent observations. 

$$(x_1,~y_1),(x_2,~y_2),\dotsi, (x_N,~y_N)$$

Let us consider two obervations in a chart $k$ and $l$, and let us calculate the mathematical expectation of the area of the rectangle formed by both points.

$$\mathbb{E}(x_k-x_l)(y_k-y_l)$$

So

$$\mathbb{E}(x_ky_k-x_ky_l-x_ly_k+x_ly_l)$$
$$\mathbb{E}(x_ky_k) -\mathbb{E}(x_k)\mathbb{E}(y_l) -\mathbb{E}(x_l)\mathbb{E}(y_k)+ \mathbb{E}(x_ly_l)$$
$$2\mathbb{E}(XY)-2\mathbb{E}(X)\mathbb{E}(Y)$$

Hence, covariance equal to:

$$\mathbb{E}(x_k-x_l)(y_k-y_l)= 2~~ Cov(X,Y)$$
\begin{paracol}{2}
    \begin{leftcolumn}
\begin{figure}[H] 
\begin{tikzpicture}
\draw  [-,ultra thick](0,0) -- (4,0) -- (4,6) -- (0,6) -- (0,0);
%\node[text width=2cm] at (3,-0.5){$\sqrt{2}~\sigma_x$};
%\node[text width=2cm] at (5.5,3){$\beta\sqrt{2}~\sigma_x$};
\filldraw[draw=black,fill=blue!20] (0,0) rectangle (4,6);
\draw[->,dotted] (0,0)--(4,6);
\node[text width=2cm] at (4.8,6.5){$(x_k,y_k)$};
\node[text width=2cm] at (-0.2,0){$(x_l,y_l)$};
\fill (4,6)  circle[radius=3pt];
\fill (0,0)  circle[radius=3pt];
\node[text width=2cm] at (2,3){$2Cov(X,Y)$};
\end{tikzpicture}
\caption{Positive Correlation}
\end{figure}
 \end{leftcolumn}
  \begin{rightcolumn} %{0.48\textwidth}
  \begin{figure}[H]
    \begin{tikzpicture}
\draw [-,ultra thick](0,0) -- (4,0) -- (4,6) -- (0,6) -- (0,0);
%\node[text width=2cm] at (3,-0.5){$\sqrt{2}~\sigma_x$};
%\node[text width=2cm] at (5.5,3){$\beta\sqrt{2}~\sigma_x$};
\filldraw[draw=black,fill=red!20] (0,0) rectangle (4,6);
\draw[->,dotted] (0,6)--(4,0);
\node[text width=2cm] at (-0.1,6.5){$(x_k,y_k)$};
\node[text width=2cm] at (5.2,0){$(x_l,y_l)$};
%\draw (0,6) node[left] {$(x_k,y_k)$};
%\draw (6,0) node[left] {$(x_l,y_l)$};
\fill (0,6)  circle[radius=3pt];
\fill (4,0)  circle[radius=3pt];
\node[text width=2cm] at (2,3){$2Cov(X,Y)$};
\end{tikzpicture}
\caption{Negative Correlation}
\end{figure}
    \end{rightcolumn}
\end{paracol}

On a sample of $n$ pairs of observations $(x_i,y_i)$ with $i=1,\dots, n$ the empirical covariance can also be computed as the average of the $n(n-1)$ areas of the $(x_i,y_i)$ and $(x_j,y_j)$ vertex rectangles for $i=1,\dots, n-1$ and $j=1,\dots, n$  
Note that the $n$ rectangles (degenerate, of zero area) of vertex $(x_i,y_i)$ and $(x_j,y_j)$ with $i=j$ should not be taken into account in the calculation. 
$$
Cov(X,~Y)=\frac{1}{n(n-1)}\sum_{i=1}^{n-1} \sum_{j=i+1}^{n}~(x_i-x_j)(y_i-y_j)
$$

An equivalent definition is to consider half (due to symmetry) of the average of the $n(n-1)$ areas of the rectangles of vertices $(x_i,y_i)$ and $(x_j,y_j)$ for $i=1,\dots, n$ and $j=1,\dots, n$ (evidence in annexes).

$$
Cov(X,~Y)=\frac{1}{2n(n-1)}
\sum_{i=1}^{n} \sum_{j=1}^{n}~(x_i-x_j)(y_i-y_j)
$$

