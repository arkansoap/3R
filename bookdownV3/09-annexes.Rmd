# annexes

## Correction of Bessel's proof

\begin{align*}
(n-1)S_{xy} &= \sum_{i=1}^{N}(x_i-\bar x)(y_i - \bar y) \\
&= \sum_{i=1}^{N} x_i y_i -N\bar x \bar y\\
&= \sum_{i=1}^{N} x_i y_i - \frac{1}{N}\sum_{i=1}^{N} x_i \sum_{i=1}^{N} y_i\\
(N-1) \mathbb{E}(S_{xy}) &= \mathbb{E}\left(\sum_{i=1}^{N} x_i y_i\right) 
- \frac{1}{N}\mathbb{E}\left(\sum_{i=1}^{N} x_i \sum_{i=1}^{N} y_i\right)\\
&= N\mu_{xy} - \frac{1}{N}[N\mu_{xy} + N(N-1)\mu_x \mu_y]\\ 
&= (N-1)[\mu_{xy}-\mu_x\mu_y]\\
&= (N-1)Cov(x,~y)
\end{align*}

## Proof for $Cov(x,~y)=\frac{1}{2N(N-1)}\sum_{i=1}^{N} \sum_{j=1}^{N}~(x_i-x_j)(y_i-y_j)$

- Proof 1

$$Cov(x,~y)=\frac{1}{2N(N-1)}
\sum_{i=1}^{N} \sum_{j=1}^{N}~(x_i-x_j)(y_i-y_j)
=$$

$$\frac{1}{2N(N-1)}
\sum_{i=1}^{N} N~x_i~y_i -x_i~N~\overline{y}-y_i~N~\overline{x}+N~\overline{xy}$$

$$\frac{2N^2}{2N(N-1)}(\overline{xy}-\overline{x}~\overline{y})=$$

$$\frac{N}{N-1}(\overline{xy}-\overline{x}~\overline{y}))$$

- Proof 2

another way, not uninteresting, to show the equivalence between the two formulations of the covariance but starting from the usual form.
$$Cov(x,~y)=\frac{1}{(N-1)}
\sum_{i=1}^{N} (x_i-\overline{x})(y_i -\overline{y})$$

$$Cov(x,~y)=\frac{1}{2(N-1)} \left\{
\sum_{i=1}^{N} (x_i-\overline{x})(y_i -\overline{y}) +
\sum_{j=1}^{N} (x_j-\overline{x})(y_j -\overline{y}) \right\}$$

$$Cov(x,~y)=\frac{1}{2N(N-1)} 
\sum_{i=1}^{N} \sum_{j=1}^{N}\left\{(x_i-\overline{x})(y_i -\overline{y}) + (x_j-\overline{x})(y_j -\overline{y}) \right\}$$

$$Cov(x,~y)=\frac{1}{2N(N-1)} 
\sum_{i=1}^{N} \sum_{j=1}^{N}\left\{(x_i-x_j)(y_i-y_j)+x_iy_j+x_jy_i-x_i\overline{y}-x_j\overline{y}-y_i\overline{x}-y_j\overline{x}+2~\overline{x}~\overline{y}\right\}$$

$$Cov(x,~y)=\frac{1}{2N(N-1)} 
\sum_{i=1}^{N} \sum_{j=1}^{N}\left\{(x_i-x_j)(y_i-y_j)\right\}$$

## a little bit of calculation

#### Variance of the sum of two random variables

\begin{align*}
Var(X+Y) &= Cov(X+Y,X+Y)  \\
 &= E((X+Y)^2)-E(X+Y)E(X+Y)   \\
\text{en d√©veloppant,}  \\
 &= E(X^2) - (E(X))^2 + E(Y^2) - (E(Y))^2 + 2(E(XY) - E(X)E(Y))  \\
 &= Var(X) + Var(Y) + 2(E(XY)) - E(X)E(Y))  \\
 &= Var(X) + Var(Y) + 2 Cov(X,~Y)  \\
\end{align*}

If the variables $X,~Y$ are independent $E(XY) = E(X)E(Y)$ then
$Var(X+Y) = Var(X) + Var(Y)$


\begin{align*}
\Delta &= 4Cov(X,~Y)^2-4Var(X)Var(Y)\\
&=4[Cov(X,~Y)^2-Var(X)Var(Y)] \leq 0\\
\end{align*}

So $ Cov(X,Y)^2 \leq Var(X) Var(Y)$ (In probability theory, the Cauchy-Schwarz inequality allows the proof of the result because of the inequality $E(XY)\leq \sqrt{E(X^2)E(Y^2)}$)
or $-(X,Y) \leq \sigma_x\sigma_y \leq \sigma_x\sigma_y $.
Let us notice that if $\Delta=0$ then we have the equality $ Cov(X,Y)^2=Var(X) Var(Y)$, i.e. if there exists $~\lambda$ such that $Var(\lambda X+Y)=0$\\
But to conclude, if we have $~lambda X+Y$ equal with probability 1 to a constant, let's say $c$, it is indeed that $Y = c -~lambda X$ almost surely, in other words a perfect linear link between the two variables.    

#### Definition of the linear correlation coefficient (Bravais-Pearson)

- case $Cov(X,~Y)\geq 0$

We define
$\rho = \frac{2Cov(X,~Y)}{Var_{max}-Var_{min}}$ 

$Var_{max} = \sigma_X^2+\sigma_Y^2+2\sigma_X\sigma_Y$ 

$Var_{min} = \sigma_X^2+\sigma_Y^2$ 

$Var_{max}-Var_{minx} = 2\sigma_X\sigma_Y$ 

From which, simplifying

$\rho = \frac{Cov(X,~Y)}{\sigma_X\sigma_Y}$


- case $Cov(X,~Y)\leq 0$

We define
$\rho = \frac{2Cov(X,~Y)}{Var_{max}-Var_{min}}$ 

$Var_{max} = \sigma_X^2+\sigma_Y^2$ 

$Var_{min} = \sigma_X^2+\sigma_Y^2-2\sigma_X\sigma_Y$

$Var_{max}-Var_{minx} = 2\sigma_X\sigma_Y$ 

from which, simplifying

$\rho = \frac{Cov(X,~Y)}{\sigma_X\sigma_Y}$

#### Covariance framing

$$-\sqrt{(Var(X)Var(Y)} \leq Cov(X,Y) \leq \sqrt{(Var(X)Var(Y)}$$

With in the particular case of standardized variables

\begin{align*}
Var(\frac{X}{\sigma_X}+\frac{Y}{\sigma_Y})&= Var(\frac{X}{\sigma_X})+Var(\frac{Y}{\sigma_Y})+2Cov(\frac{X}{\sigma_X},\frac{Y}{\sigma_Y})\\
&=2(1+\rho)\\
Var(\frac{X}{\sigma_X}-\frac{Y}{\sigma_Y})&= Var(\frac{X}{\sigma_X})+Var(\frac{Y}{\sigma_Y})-2Cov(\frac{X}{\sigma_X},\frac{Y}{\sigma_Y})\\
&=2(1-\rho)
\end{align*}
One finds, knowing that $2(1+\rho) \geq 0$ and $2(1-\rho)\geq 0$ that one has $-1 \le \rho \le 1$

