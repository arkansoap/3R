% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{report}
\title{Econometrics visualization}
\author{Lucas Chaveneau, Thibault Fuchez, Allan Guichard}
\date{2021-12-21}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Econometrics visualization},
  pdfauthor={Lucas Chaveneau, Thibault Fuchez, Allan Guichard},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{microtype}
\microtypesetup{nopatch=item}
\usepackage{tikz}
\usepackage{tkz-euclide}
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage{paracol}
\usepackage{graphicx}
\usepackage{gensymb}
\usepackage{float}
\usetikzlibrary{arrows,automata}
\usepackage{geometry}



\geometry{hmargin=3cm,vmargin=3cm}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

\hypertarget{what-is-econometrics}{%
\section{What is Econometrics?}\label{what-is-econometrics}}

\textless\textless\textless\textless\textless\textless\textless{} HEAD
Econometrics is a branch of economics that aims to estimate and test economic models (simplified representation of reality). Thus, the econometer tries to identify the parameters of a model by means of statistical estimation, and thus tries to induce the characteristics of a general group (the population) from those of a particular group (the sample).
=======
BLABLABLA

Econometrics is a branch of economics that aims to estimate and test economic models (simplified representation of reality). Thus, the econometrician tries to identify the parameters of a model by means of statistical estimation, and thus tries to induce the characteristics of a general group (the population) from those of a particular group (the sample).
\textgreater\textgreater\textgreater\textgreater\textgreater\textgreater\textgreater{} deedb0dc1cd0094bc1d3cb0eaf99e5e078b9f27a

Three essential words in the language of the econometer are: correlation, regression, and causality.

To better understand the relationships between different variables, visualization is a key tool to help interpret mathematical and statistical results.

\hypertarget{what-about-econometric-visualization}{%
\section{What about econometric visualization?}\label{what-about-econometric-visualization}}

We can start by saying what it is not: it is not strictly speaking data visualization.
Data visualization ``the graphical display of data'' is often seen as something trivial, to be quickly scrolled through to show stakeholders simple but salient facts of a possibly very complex problem\ldots{}

The analyst consciously chooses what to include in a visualization in order to identify intuitively relevant patterns and trends in the data in the most efficient way. The analyst then makes choices in this representation. Modern data sets tend to be very large (in terms of number of observations) and broad (in terms of number of variables). Therefore, it is hard to simply represent all the data. The difficulty is that everything must be made as simple as possible - but not simpler.

Visualization is used to explain, notably to teach, econometrics in order to facilitate the understanding of the fundamental concepts without necessarily using a mathematically complex corpus.
Diagrammatic representations of models, methods and diagrams can facilitate the understanding and reading of results.

\hypertarget{state-of-the-art}{%
\section{State of the art}\label{state-of-the-art}}

Data visualization is an important part of any statistical analysis. Any good statistician will tell you that it is dangerous to undertake an econometric analysis without first examining the data.
Scatterplots are used as a tool to diagnose the overall trend, the relationship between variables, the variation around the perceived trend, the exceptions whether they are outliers or distinct groups of observations, the discontinuities. The analysis of residuals largely uses the same tools.

A bibliographic search on the theme of visualization in econometrics does not yield much. Nevertheless, we will mention in this document some articles that deal with the subject in a direct or indirect way, and that have been an anchor point in our reflections.

Moreover, some packages related to econometric visualization have already been developed in R (or other languages). At the end of this document, we will briefly describe the state of the art of these packages.

\hypertarget{our-project-an-r-package}{%
\section{Our project, an R package}\label{our-project-an-r-package}}

Our project stands to propose a library developed under R which proposes different ways to visualize the essential elements of the econometer.

Our package will therefore focus on the representation of correlations. We will try to put forward a clear and synthetic representation of the covariance, a representation of the elements that allow to visualize the quality of a regression, as well as the part of each variable in the explanation of the variable to predict.

This document is intended to be a guide for the package. It proposes a definition of the econometric tools usefull to understand the package. It also proposes an overview of the different visualization methods that already exist.

\hypertarget{covariance}{%
\chapter{Covariance}\label{covariance}}

\hypertarget{variance-definition-reminder}{%
\section{Variance definition reminder}\label{variance-definition-reminder}}

In the fields of statistics and probability theory, the variance can be define as a measure of a sample's values dispersion or a measure of a probability distribution.
According to the KÃ¶nig-Huygens theorem, the variance show the gap between the squares of the values of the variable and the square of the mean.

\begin{itemize}
\tightlist
\item
  Classical formula of variance:
\end{itemize}

\[\sigma^2_x=\frac{1}{N}\sum_{x=1}^{N}(x_i - \bar{x})^2 = \frac{1}{N}\sum_{x=1}^{N}x_i^2 - \bar{x}^2\]

\begin{itemize}
\tightlist
\item
  A new proposition for variance formula\citep{Heffernan}:
\end{itemize}

\[\sigma^2_x= \frac{1}{N(N-1)}\sum_{i=1}^{N-1}\sum_{j>i}^{N}(x_i-x_j)^2\]

This formulation (extended to covariance) will be discussed again in this study. Intuitively, we can see here the geometrical aspect of the variance of \(x\) perceived as the expression of a square.

\begin{itemize}
\tightlist
\item
  Variance versus Covariance:
\end{itemize}

Variance and covariance are mathematical terms frequently used in statistics and probability theory. Variance refers to the spread of a data set around its mean value, while a covariance refers to the measure of the directional relationship between two random variables.

\hypertarget{covariance-standard-definition}{%
\section{Covariance standard definition}\label{covariance-standard-definition}}

Covariance as an extension of variance notion. The covariance between two random variables is a number which provide us the joint difference to there respective means in quantity.

A covariance refers to the measure of how two random variables will change when they are compared to each other.

Intuitively, covariance is a measure of the simultaneous variation of two random variables. That is, the covariance becomes more positive for each pair of values that differ from their mean in the same direction, and more negative for each pair of values that differ from their mean in the opposite direction.

The sign of the covariance therefore shows the tendency in the linear relationship between the variables. The magnitude of the covariance is not easy to interpret because it is not normalized and hence depends on the magnitudes of the variables.

The covariance of two independent random variables is zero, although the converse is not always true.

When we have more than 2 variables, the concept naturally generalizes through the covariance matrix (or the variance-covariance matrix). Let be a set of \(p\) real random variables \(X_1, X_2, \dots, X_p\), then the covariance matrix is the square matrix of which the term line \(i\) and the term column \(j\) is the covariance of the variables \(X_i\) and \(X_j\).
This matrix quantifies the variation of each variables compared to each of the others.

\hypertarget{mathematical-standard-and-alternatives-covariance-definition}{%
\section{Mathematical standard and alternatives covariance definition}\label{mathematical-standard-and-alternatives-covariance-definition}}

\hypertarget{standard-definition}{%
\subsection{Standard definition}\label{standard-definition}}

\begin{itemize}
\tightlist
\item
  Covariance formula:
\end{itemize}

For two jointly distributed real-valued random variables \(X\) and \(Y\) with finite, the covariance is defined as the expected value (or mean) of the product of their deviations from their individual expected values.
\[
Cov(X,Y)=\mathbb{E}((X-\mathbb{E}(X))\times(Y-\mathbb{E}(Y)))
\]

Where \(\mathbb{E}(X)\) is the expected value of \(X\), also known as the mean of \(X\). The covariance is also sometimes denoted \(\sigma_{X,Y}\), in analogy to variance. By using the linearity property of expectations, this can be simplified to the expected value of their product minus the product of their expected values:

\begin{align*}
  Cov(X,Y) &=\mathbb{E}((X-\mathbb{E}(X))\times(Y-\mathbb{E}(Y))) \\
  &=\mathbb{E}(XY- X\mathbb{E}(Y) - \mathbb{E}(X)Y + \mathbb{E}(x)\mathbb{E}(Y)) \\
  &= \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y) - \mathbb{E}(x)\mathbb{E}(Y) + \mathbb{E}(x)\mathbb{E}(Y) \\
  &= \mathbb{E}(XY)-\mathbb{E}(X)\mathbb{E}(Y)
\end{align*}

The empirical covariance of a sample is defined by:
\[
Cov(X,~Y)=\frac{1}{n}
\sum_{i=1}^{n} (x_i-\overline{x})(y_i -\overline{y})
\]
With, \(\overline{x}=\frac{1}{n}\sum_{i=1}^{n} x_j\) and \(\overline{y}=\frac{1}{n}\sum_{i=1}^{n} y_j\)
an unbiased estimator of the population.

Covariance is defined by:

\[
Cov(X,~Y)=\frac{1}{(n-1)}
\sum_{i=1}^{n} (x_i-\overline{x})(y_i -\overline{y})
\]

or equivalently:
\[
Cov(X,~Y)=\frac{n}{n-1}(\overline{xy}-\overline{x}~\overline{y})
\]

\hypertarget{alternative-definition-a-story-of-rectangles}{%
\subsection{Alternative definition, a story of rectangles}\label{alternative-definition-a-story-of-rectangles}}

\begin{itemize}
\tightlist
\item
  Formula from Heffernan definition of covariance:
\end{itemize}

\[
Cov(X,Y)= \frac{2}{n(n-1)}\sum_{i=1}^{n-1}\sum_{j>i}^{n}\frac{1}{2}(x_i-x_j)(y_i - y_j)
\]

Let be two random variables \((X,~Y)\), and a sample of \(N\) pairs of independent observations.

\[(x_1,~y_1),(x_2,~y_2),\dotsi, (x_N,~y_N)\]

Let us consider two obervations in a chart \(k\) and \(l\), and let us calculate the mathematical expectation of the area of the rectangle formed by both points.

\[\mathbb{E}(x_k-x_l)(y_k-y_l)\]

So

\[\mathbb{E}(x_ky_k-x_ky_l-x_ly_k+x_ly_l)\]
\[\mathbb{E}(x_ky_k) -\mathbb{E}(x_k)\mathbb{E}(y_l) -\mathbb{E}(x_l)\mathbb{E}(y_k)+ \mathbb{E}(x_ly_l)\]
\[2\mathbb{E}(XY)-2\mathbb{E}(X)\mathbb{E}(Y)\]

Hence, covariance equal to:

\[\mathbb{E}(x_k-x_l)(y_k-y_l)= 2~~ Cov(X,Y)\]

\begin{paracol}{2}
    \begin{leftcolumn}
\begin{figure}[H] 
\begin{tikzpicture}
\draw  [-,ultra thick](0,0) -- (4,0) -- (4,6) -- (0,6) -- (0,0);
%\node[text width=2cm] at (3,-0.5){$\sqrt{2}~\sigma_x$};
%\node[text width=2cm] at (5.5,3){$\beta\sqrt{2}~\sigma_x$};
\filldraw[draw=black,fill=blue!20] (0,0) rectangle (4,6);
\draw[->,dotted] (0,0)--(4,6);
\node[text width=2cm] at (4.8,6.5){$(x_k,y_k)$};
\node[text width=2cm] at (-0.2,0){$(x_l,y_l)$};
\fill (4,6)  circle[radius=3pt];
\fill (0,0)  circle[radius=3pt];
\node[text width=2cm] at (2,3){$2Cov(X,Y)$};
\end{tikzpicture}
\caption{Positive Correlation}
\end{figure}
 \end{leftcolumn}
  \begin{rightcolumn} %{0.48\textwidth}
  \begin{figure}[H]
    \begin{tikzpicture}
\draw [-,ultra thick](0,0) -- (4,0) -- (4,6) -- (0,6) -- (0,0);
%\node[text width=2cm] at (3,-0.5){$\sqrt{2}~\sigma_x$};
%\node[text width=2cm] at (5.5,3){$\beta\sqrt{2}~\sigma_x$};
\filldraw[draw=black,fill=red!20] (0,0) rectangle (4,6);
\draw[->,dotted] (0,6)--(4,0);
\node[text width=2cm] at (-0.1,6.5){$(x_k,y_k)$};
\node[text width=2cm] at (5.2,0){$(x_l,y_l)$};
%\draw (0,6) node[left] {$(x_k,y_k)$};
%\draw (6,0) node[left] {$(x_l,y_l)$};
\fill (0,6)  circle[radius=3pt];
\fill (4,0)  circle[radius=3pt];
\node[text width=2cm] at (2,3){$2Cov(X,Y)$};
\end{tikzpicture}
\caption{Negative Correlation}
\end{figure}
    \end{rightcolumn}
\end{paracol}

On a sample of \(n\) pairs of observations \((x_i,y_i)\) with \(i=1,\dots, n\) the empirical covariance can also be computed as the average of the \(n(n-1)\) areas of the \((x_i,y_i)\) and \((x_j,y_j)\) vertex rectangles for \(i=1,\dots, n-1\) and \(j=1,\dots, n\)\\
Note that the \(n\) rectangles (degenerate, of zero area) of vertex \((x_i,y_i)\) and \((x_j,y_j)\) with \(i=j\) should not be taken into account in the calculation.
\[
Cov(X,~Y)=\frac{1}{n(n-1)}\sum_{i=1}^{n-1} \sum_{j=i+1}^{n}~(x_i-x_j)(y_i-y_j)
\]

An equivalent definition is to consider half (due to symmetry) of the average of the \(n(n-1)\) areas of the rectangles of vertices \((x_i,y_i)\) and \((x_j,y_j)\) for \(i=1,\dots, n\) and \(j=1,\dots, n\) (evidence in annexes).

\[
Cov(X,~Y)=\frac{1}{2n(n-1)}
\sum_{i=1}^{n} \sum_{j=1}^{n}~(x_i-x_j)(y_i-y_j)
\]

\hypertarget{correlation}{%
\chapter{Correlation}\label{correlation}}

\hypertarget{correlation-overview}{%
\section{Correlation overview}\label{correlation-overview}}

\hypertarget{from-covariance-to-correlation}{%
\subsection{From covariance to correlation}\label{from-covariance-to-correlation}}

The normalized version of the covariance, the correlation coefficient, however, shows by its magnitude the strength of the linear relation.

\begin{itemize}
\tightlist
\item
  Both covariance and correlation measure the relationship and the dependency between two variables.
\item
  Correlation values are standardized while covariance values are not.
\item
  Covariance indicates the direction of the linear relationship between variables as well as the correlation but the latter measure also the strengh of the relationship.
\end{itemize}

The normalized form of the covariance matrix is the correlation matrix.

\hypertarget{correlation-definition}{%
\subsection{Correlation definition}\label{correlation-definition}}

In statistics, correlation or dependence is any statistical relationship, whether causal or not, between two random variables or bivariate data.

In the broadest sense correlation is any statistical association, though it actually refers to the degree to which a pair of variables are linearly related.

There are several correlation coefficients, often denoted \(\rho\) or \(r\), measuring the degree of correlation. The most common of these is the Pearson correlation coefficient, which is sensitive only to a linear relationship between two variables (which may be present even when one variable is a nonlinear function of the other). Other correlation coefficients -- such as Spearman's rank correlation -- have been developed to be more robust than Pearson's, that is, more sensitive to nonlinear relationships.

\hypertarget{galton-a-pioneer-in-the-history-of-correlation}{%
\subsection{Galton, a pioneer in the history of correlation}\label{galton-a-pioneer-in-the-history-of-correlation}}

\begin{quote}
\begin{quote}
``I can only say that there is a vast field of topics that fall under the laws of correlation, which lies quite open to the research of any competent person who cares to investigate it.'' (Galton, 1890)
\end{quote}
\end{quote}

Galton's 1888 paper \citep{galton}, presented to the Royal Society in London, defines correlation as follows:

\begin{quote}
\begin{quote}
``Two variable organs are said to be co-related when the variation of the one is accompanied on the average by more or less variation of the other, and in the same direction\ldots. It is easy to see that co-relation must be the consequence of the variations of the two organs being partly due to common causes\ldots{} If they were in no respect due to common causes, the co-relation would be nil.''
\end{quote}
\end{quote}

Galton's definition reveals the properties of the correlation coefficient. It is a measure of the strength of a linear relationship; the closer it is to 1, the more two variables can be predicted from each other by a linear equation. It is a measure of direction: a positive correlation indicates that \(X\), \(Y\) increase together; a negative correlation indicates that one decreases as the other increases. Note that Galton does not claim that co-relation implies cause and effect (it would be absurd to assume that the size of one organ determines the size of another). Galton hypothesized that the correlation indicated the presence of ``common causes'' for the observed relationship between the variables (the size of each organ respectively).

More technically Galton continues his presentation as follows:

\begin{quote}
\begin{quote}
``Let y = the deviation of the subject {[}in units of the probably error, Q{]}, whichever of the two variables may be taken in that capacity; and let x1, x2, x3, \&c., be the corresponding deviations of the relative, and let the mean of these be X. Then we find: (1) that y = rX for all values of y; (2) that r is the same, whichever of the two variables is taken for the subject; (3) that r is always less that 1; (4) that r measures the closeness of co-relation.''
\end{quote}
\end{quote}

Galton particularly liked the correlation coefficient because it could be used to predict deviations \(Y\) from \(X\) or \(X\) from \(Y\). Thus, from the beginning, the correlation coefficient was closely related to the regression line. Originally, \(r\) meant the regression slope, but there was a problem in that the regression line of the slope was partly a function of the units of measurement chosen. Galton perceived the correlation coefficient as a unitless regression slope and appropriated the label \(r\).

\begin{figure}
    \centering
    \includegraphics[width= 250 pt]{Galton.PNG}
    \caption{The first Bivariate Scatterplot}
    \label{fig:my_label}
\end{figure}

\textless\textless\textless\textless\textless\textless\textless{} HEAD
\#\#\# Thirteen ways to see correlation \citep{13cor}

\begin{quote}
In 1885, Sir Francis Galton first defined the term ``regression'' and completed the theory of bivariate correlation. A decade later, Karl Pearson developed the index that we still use to measure correlation, Pearson's r . Our article is written in recognition of the 100th anniversary of Galton's first discussion of regression and correlation.
\end{quote}

According Joseph Lee Rodgers and co article, Several ways to interpret the correlation:

\begin{itemize}
\tightlist
\item
  As standardized covariance
  \[r=\frac{Cov(x,y)}{\sigma^2_x\sigma^2_y}\]
\item
  As standardized slope of regression line
  \[r=b_{Y.X}\left(\frac{\sigma^2_x}{\sigma^2_y}\right)=b_{X.Y}\left(\frac{\sigma^2_y}{\sigma^2_x}\right)\]
\item
  As geometric mean of the two regression slopes
  \[r=\pm\sqrt{b_{Y.X}\times b_{X.Y}}\]
\item
  As the square root of the ratio of two variances
  \[r=\sqrt{\frac{\sum(Y_i -\hat{Y}_i)}{\sum(Y_i -\bar{Y}_i)}}=\sqrt{\frac{SS_{reg}}{SS_{tot}}}\]
\item
  And so many other\ldots{}
\end{itemize}

\hypertarget{pearson-correlation-coefficient}{%
\subsection{Pearson correlation coefficient}\label{pearson-correlation-coefficient}}

\begin{itemize}
\tightlist
\item
  Some notations that may be useful:
\end{itemize}

\(S_{xx} = \sum_{i=1}^n(x_i -\bar{x})^2\)

\(S_{yy} = \sum_{i=1}^n(y_i -\bar{y})^2\)

\(S_{xy} = \sum_{i=1}^n(x_i -\bar{x})(y_i -\bar{y})\)

\begin{itemize}
\tightlist
\item
  Pearson correlation coefficient:
\end{itemize}

\[\rho=\frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}\]

The Pearson correlation coefficient is a bounded index (i.e., \(-1 \leq \rho \leq 1\)) that provides a unitless measure for the strength and direction of the association between two variables.

\hypertarget{alternatives-correlation-measurement}{%
\section{Alternatives correlation measurement}\label{alternatives-correlation-measurement}}

\hypertarget{spearmans-rank-correlation-coefficient}{%
\subsection{Spearman's rank correlation coefficient}\label{spearmans-rank-correlation-coefficient}}

Measures the association based on the ranks of the variables.

\[\hat{\theta}=\frac{\sum_{i=1}^n(R_i-\bar{R}(S_i-\bar{S}))}{\sqrt{\sum_{i=1}^n(R_i-\bar{R})^2\sum_{i=1}^n(S_i-\bar{S})^2}}\]

Where \(R_i\) and \(S_i\) are the rank of the \(x_i\) and \(y_i\) values, respectively.

Note that this is just the estimated Pearson's correlation coeffcient, but the values of the variables have been replaced by their respective ranks.

The Spearman correlation is the non-parametric equivalent of the Pearson correlation. It measures the relationship between two variables. If the variables are ordinal, discrete or do not follow a normal distribution, the Spearman correlation is used. This correlation does not use the values of the data but their rank.
In fact, nothing changes, everything is the same as calculating a Pearson correlation but on transformed variables.
The interest of establishing a correlation on the ranks of the variables is to detect if there is a monotonic relationship, which may not be linear.

\hypertarget{partial-correlation}{%
\subsection{Partial Correlation}\label{partial-correlation}}

The partial correlation coefficient, noted here \(r_{AB.C}\), allows us to know the value of the correlation between two variables A and B, if the variable C had remained constant for the series of observations considered.

Put another way, the partial correlation coefficient \(r_{AB.C}\) is the total correlation coefficient between variables A and B when we have removed their best linear explanation in terms of C. It is given by the formula :

\[r_{AB.C}=\frac{r_{AB}-r_{AC} \cdot r_{BC}}{\sqrt{1-r_{AC}^2} \cdot \sqrt{1-r_{BC}^2}}\]
Let's go a little further in understanding this coefficient:

The OLS estimator of \(\beta\) is written
\[\hat{\beta}=\frac{Cov(y,x_1)}{\mathbb{V}ar(x_1)}\]
The estimator \(\beta'\) is written

\begin{align*}
  \hat{\beta'} &= \frac{Cov(y,x_1)\mathbb{V}(x_2)-Cov(y,x_2)Cov(x_1,x_2)}{
  \mathbb{V}(x_1)\mathbb{V}(x_2)-Cov(x_1,x_2)^2} \\
&= \hat{\beta'}=\frac{\rho_{y1} \sigma_y \sigma_1\sigma_2^2-\rho_{y2} \sigma_y     \sigma_2\rho_{12} \sigma_1 \sigma_2}{\sigma_1^2\sigma_2^2-\rho_{12}^2 \sigma_1^2 \sigma_2^2} \\
&= \hat{\beta'}={\frac{\rho_{y1}-\rho_{y2}\rho_{12}} 
{1-\rho_{12}^2}}\quad\frac{\sigma_y}{\sigma_1}
\end{align*}

After some transformation we have:
\[\hat{\beta'}=\underbrace{\underbrace{\frac{\rho_{y1}-\rho_{y2}\rho_{12}} 
{\sqrt{1-\rho_{12}^2}\sqrt{1-\rho_{y2}^2}}}_{\text{Partial correlation}}}_{\rho_{y1|2}}
\quad\frac{\sigma_y\sqrt{1-\rho_{y2}^2}}{\sigma_1\sqrt{1-\rho_{12}^2}}\]

\textless\textless\textless\textless\textless\textless\textless{} HEAD
In order to understand this expression, consider the following two regressions:

\[x_1=\kappa +\tau x_2+\varepsilon_{1|2}\]\\
\[y=\delta +\gamma x_2+\varepsilon_{y|2}\]

We have:
\begin{align*}
Cov(e_{1|2},e_{y|2})&=Cov(x_1-\hat{\kappa}-\hat{\tau} x_2,~y-\hat{\delta} -\hat{\gamma} x_2)\\
&=Cov(x_1,y)-\hat{\gamma}Cov(x_1,x_2)-\hat{\tau}Cov(x_1,y)+\hat{\gamma}\hat{\tau}\mathbb{V}ar(x_2)\\
\mathbb{V}ar(e_{y|2})&=\mathbb{V}ar(y-\hat{\delta} - \hat{\gamma} x_2)\\
&=\mathbb{V}ar(y)-2\hat{\gamma}Cov(x_1,y)+\hat{\gamma}^2\mathbb{V}ar(x_2)\\
\mathbb{V}ar(e_{1|2})&=\mathbb{V}ar(x_1-\hat{\kappa} - \hat{\tau} x_2)\\
&=\mathbb{V}ar(x_1)-2\hat{\tau}Cov(x_1,x_2)+\hat{\tau}^2\mathbb{V}ar(x_2)
\end{align*}

The linear correlation coefficient between \(e_{1|2}\) and \(e_{y|2}\) corresponds to the correlation between \(y\) and \(x_1\) after taking into account the linear influence of \(x_2\) on these two respective variables:

\begin{align*}
\rho_{yx_1|x_2}&=\frac{Cov(e_{1|2},e_{y|2})}{\sqrt{\mathbb{V}ar(e_{y|2})\mathbb{V}ar(e_{1|2})}}\\
\end{align*}

After simplification we obtain the expression of the partial correlation:

\begin{align*}
\rho_{y1|2}&=\frac{\rho_{y1}-\rho_{y2}\rho_{12}}{\sqrt{(1-\rho_{y2}^2)(1-\rho_{12}^2)}}
\end{align*}

And so, the estimator \(\hat{\beta'}\) can thus be written as that of a simple linear regression where the variables are the residuals of prior regressions of \(y\) respectively \(x_1\) on \(x_2\).

\[\hat{\beta'}=\rho_{y1|2}\frac{\sigma_y\sqrt{1-\rho_{y2}^2}}{\sigma_1\sqrt{1-\rho_{12}^2}}\]

\begin{tikzpicture}
  \draw[<-,thick] (2,1) node[right,state,pattern=dots] (Y) {$y$} -- (0,0) node[left,state] (Y) {$x_2$} node[midway,above] {$\gamma$};
  \draw[<-,thick] (2,-1) node[right,state,pattern=dots] (X1) {$x_1$} -- (0,0) node[left,state] (Y) {$x_2$} node[midway,above] {$\tau$};
  \path[->,ultra thick] (3,-1) edge[bend right=90] node[right]{$\beta'$} (2.9,1);
\end{tikzpicture}

\textless\textless\textless\textless\textless\textless\textless{} HEAD

When the number of variable is quite high, computing the partial correlation coefficient can be quite laborious. It is advised to use some regression methods when there are more than 3 variables. The alternative is to compute regression's residuals of both chosen variables on the other variables.
This approach leads to the same results. Let's remind that partial correlation measuring the link between the residual information of \(X\) and \(Y\) which is not already explained by the other variables.
The \(j\)-order partial correlation amounts to calculating the correlation between the regression's residuals.

\[\rho_{x_1y.x_2, \ldots, x_j} = \rho_{e_{x_1}e_y}\]

\hypertarget{semi-partial-correlation}{%
\subsection{Semi partial Correlation}\label{semi-partial-correlation}}

Unlike to partial correlation, semi-partial correlation is asymetrical. it get closer from multiple regression. We try to quantify, for one variable, its additional ability to explain.

For more accuracy, we take off the third variable information from one of both variables. Thanks to it, We are seeking to quantify the link between \(y\) and the residuals parts of \(x\) in relation to the third variable.

\[\rho_{xy.z_1, \ldots, z_j} = \rho_{e_xe_y}\]

\begin{tikzpicture}
  \draw[-] (2,1) node[right,state] (Y) {$y$}  (0,0) node[left,state] (Y) {$Z$};
  \draw[<-,thick] (2,-1) node[right,state,pattern=dots] (X1) {$x$} -- (0,0) node[left,state] (Y) {$Z$} node[midway,above] {$\tau$};
  \path[->,ultra thick] (3,-1) edge[bend right=90] node[right]{$coef$} (2.9,1);
\end{tikzpicture}

\textless\textless\textless\textless\textless\textless\textless{} HEAD
\[r_{y(x.z)}= \frac{r_{yx} - r_{yz}r_{xz}}{\sqrt{1-r^2_{xz}}}\]
It is obvious that if \(X\) and \(Z\) are indepedent so \(r_{y(x.z)}= r_{yx}\). Unlike, If \(X\) and \(Z\) are perfectly correlated, \(r_{y(x.z)}\) is undefined, which means nothing remains in the \(X\)-residuals to explain \(Z\).

\hypertarget{transitivity-correlation}{%
\subsection{Transitivity correlation}\label{transitivity-correlation}}

Let \(\rho_{xy}\) be the correlation between the variables \(X\) and \(Y\). Let \(\rho_{xz}\) and \(\rho_{yz}\) be the correlations of variables \(X\) and \(Y\) with respect to a third variable \(Z\).

Given \(\rho_{xz}\) and \(\rho_{yz}\), can we deduce the possible values for \(\rho_{xy}\)?

\[\rho_{XY \mid Z}={\frac {\rho_{XY}-\rho _{XZ}\rho_{YZ}}{{\sqrt {1-\rho_{XZ}^{2}}}{\sqrt {1-\rho_{YZ}^{2}}}}}\]

\begin{align*}
  \rho_{XY} 
  &=  \left( \rho_{XY \mid Z} - \frac{ - \rho_{XZ} \rho_{YZ}}{\sqrt{1 - \rho_{XZ}^{2}} \sqrt{1 - \rho_{YZ}^{2}}} \right)  \sqrt{1 - \rho_{XZ}^{2}} \sqrt{1 - \rho_{YZ}^{2}}  \\
  &=  \rho_{XY \mid Z} \sqrt{1 - \rho_{XZ}^{2}} \sqrt{1 - \rho_{YZ}^{2}} +  \rho_{XZ} \rho_{YZ}
\end{align*}

\(\rho_{xy}\) is in the range \(\rho_{XZ} \rho_{YZ} \pm \sqrt{1 - \rho_{XZ}^{2}} \sqrt{1 - \rho_{YZ}^{2}}\)

\begin{align*}
  \rho_{XZ} \rho_{YZ} - \sqrt{1 - \rho_{XZ}^{2}} \sqrt{1 - \rho_{YZ}^{2}} &> 0 \\
  \rho_{XZ} \rho_{YZ} + \sqrt{1 - \rho_{XZ}^{2}} \sqrt{1 - \rho_{YZ}^{2}} &> 0 .
\end{align*}

\begin{align*}
  \rho_{XZ}^2 \rho_{YZ}^2 
  &> \left ( 1 - \rho_{XZ}^{2}\right ) \left (1 - \rho_{YZ}^{2}\right ) \\
  &= 1 - \rho_{XZ}^{2} - \rho_{YZ}^{2} + \rho_{XZ}^2 \rho_{YZ}^2 .
\end{align*}
Si \(\rho_{xy}>0\) alors on a

\begin{align*}
  \rho_{XZ}^2 \rho_{YZ}^2 
  &> \left ( 1 - \rho_{XZ}^{2}\right ) \left (1 - \rho_{YZ}^{2}\right ) \\
  &= 1 - \rho_{XZ}^{2} - \rho_{YZ}^{2} + \rho_{XZ}^2 \rho_{YZ}^2 .
\end{align*}

\begin{equation*}
\text{sign}(\rho_{XY}) 
  =
\begin{cases}
    \text{sign}(\rho_{XZ}) \text{sign}(\rho_{YZ}) & \text{ if } \rho_{XZ}^2 + \rho_{YZ}^2 > 1  \\
    \text{not known} & \text{ if } \rho_{XZ}^2 + \rho_{YZ}^2 \leq 1.
  \end{cases}
\end{equation*}

\begin{figure}[H]
\begin{tikzpicture}
  \draw [thick,fill=red!80] (0,4) rectangle (4,8);
  \draw [thick,fill=red!80] (4,0) rectangle (8,4);
  \draw [thick,fill=blue!80] (0,0) rectangle (4,4);
  \draw [thick,fill=blue!80] (4,4) rectangle (8,8);
  \draw[ultra thick,green,fill=gray!20] (4,4) circle (4cm);


  \draw[->] (-1,-1) -- (9,-1) node[below] {$\rho_{xz}$} node[below] at(0,-1) {-1} node[below] at(4,-1) {0} node[below] at(8,-1) {1};
  \draw[->] (-1,-1) -- (-1,9) node[left] {$\rho_{yz}$}
node[left] at(-1,0) {-1} node[left] at(-1,4) {0} node[left] at(-1,8) {1};

  \matrix [draw,below left] at (11,8) {
    \node [rectangle,fill=red!80,label=right:$\rho_{xy}<0$] {};\\
    \node [rectangle,fill=gray!20,label=right:$?$] {};\\  
    \node [rectangle,fill=blue!80,label=right:$\rho_{xy}>0$] {};\\
};
%current bounding box.north east
\end{tikzpicture}
\caption{Value of $\rho_{x,y}$ correlated with a third variable}
\end{figure}

\hypertarget{distance-correlation}{%
\section{Distance correlation}\label{distance-correlation}}

The so-called association measures are an active and recent field of research that renews the well established and old field of correlation. The \emph{energy} package developed under R as well as Gabor's article \citep{discor} are good references.

Let \((x_i,y_i)\), \(i=1,2, \dots, N\) be a sample of pairs of observations of the variables \(X\) and \(Y\).

We compute successively

\[dx_{ij}=\lVert x_i-xj \rVert\]
\[dy_{ij}=\lVert y_i-yj \rVert\]

\[\overline{\overline{dx_{ij}}}=dx_{ij}-\overline{dx_{i.}}-\overline{dx_{.j}}+\overline{dx_{..}}\]
\[\overline{\overline{dy_{ij}}}=dy_{ij}-\overline{dy_{i.}}-\overline{dy_{.j}}+\overline{dy_{..}}\]
with

\(\overline{dx_{i.}}=\frac{1}{N}\sum_{j=1}^{N} dx_{ij}\)

and

\(\overline{dx_{..}}=\frac{1}{N^2}\sum_{i=1}^{N}\sum_{j=1}^{N} dx_{ij}\)

\[dCov(X,Y)=\frac{1}{N^2}\sum_{i=1}^{N}\sum_{j=1}^{N}
\overline{\overline{dx_{ij}}}~ 
\overline{\overline{dy_{ij}}}\]

\[dVar(X)=dCov^2(X,X)=\frac{1}{N^2}\sum_{i=1}^{N}\sum_{j=1}^{N} 
\overline{\overline{dx_{ij}}}~^2\]

\[d\rho=\frac{dCov(X,Y)}{d\sigma_X~d\sigma_Y}\]

\hypertarget{other-correlations}{%
\subsection{Other Correlations}\label{other-correlations}}

Here we introduced some correlation type \citep{mako}:

\begin{itemize}
\item
  Kendall's rank correlation: In the normal case, the Kendall correlation is preferred
  to the Spearman correlation because of a smaller gross error sensitivity (GES) and a
  smaller asymptotic variance (AV), making it more robust and more efficient. However,
  the interpretation of Kendall's tau is less direct compared to that of the Spearman's rho,
  in the sense that it quantifies the difference between the \% of concordant and discordant
  pairs among all possible pairwise events. Confidence Intervals (CI) for Kendall's correlations are computed using the Fieller et al.~(1957) correction (see Bishara \& Hittner,
  2017).
\item
  Hoeffding's D: The Hoeffding's D statistic is a non-parametric rank based measure of association
  that detects more general departures from independence (Hoeffding 1948), including non-linear
  associations. Hoeffding's D varies between -0.5 and 1 (if there are no tied ranks, otherwise it can
  have lower values), with larger values indicating a stronger relationship between the variables.
\item
  Gamma correlation: The Goodman-Kruskal gamma statistic is similar to Kendall's Tau coefficient. It
  is relatively robust to outliers and deals well with data that have many ties.
\item
  Gaussian rank correlation: The Gaussian rank correlation estimator is a simple and wellperforming alternative for robust rank correlations (Boudt et al., 2012). It is based on the Gaussian
  quantiles of the ranks.
\item
  Winsorized correlation: Correlation of variables that have been Winsorized, i.e., transformed by
  limiting extreme values to reduce the effect of possibly spurious outliers.
\item
  Biweight midcorrelation: A measure of similarity that is median-based, instead of
  the traditional mean-based, thus being less sensitive to outliers. It can be used as a
  robust alternative to other similarity metrics, such as Pearson correlation (Langfelder \&
  Horvath, 2012).
\item
  Percentage bend correlation: Introduced by Wilcox (1994), it is based on a downweight of a specified percentage of marginal observations deviating from the median (by
  default, 20 percent).
\item
  Shepherd's Pi correlation: Equivalent to a Spearman's rank correlation after outliers
  removal (by means of bootstrapped Mahalanobis distance).
\item
  Point-Biserial and biserial correlation: Correlation coefficient used when one variable
  is continuous and the other is dichotomous (binary). Point-Biserial is equivalent to a
  Pearson's correlation, while Biserial should be used when the binary variable is assumed
  to have an underlying continuity. For example, anxiety level can be measured on a
  continuous scale, but can be classified dichotomously as high/low.
\item
  Tetrachoric correlation: Special case of the polychoric correlation applicable when
  both observed variables are dichotomous.
\end{itemize}

\hypertarget{visualization-of-covariance}{%
\chapter{Visualization of covariance}\label{visualization-of-covariance}}

\hypertarget{state-of-the-art-different-attempts-to-represent-the-covariance}{%
\section{State of the art: different attempts to represent the covariance}\label{state-of-the-art-different-attempts-to-represent-the-covariance}}

\hypertarget{venn-diagramm}{%
\subsection{Venn diagramm}\label{venn-diagramm}}

\begin{quote}
\begin{quote}
A Venn diagram is a widely used diagram style that shows the logical relation between sets, popularized by John Venn in the 1880s. The diagrams are used to teach elementary set theory, and to illustrate simple set relationships in probability, logic, statistics, linguistics and computer science. A Venn diagram uses simple closed curves drawn on a plane to represent sets. Very often, these curves are circles or ellipses.
\end{quote}
\end{quote}

\begin{figure}
    \centering
    \includegraphics[width= 250 pt]{venn_colinearity.PNG}
    \caption{Ballentine Venn diagrams displaying modest and considerable collinearity}
    \label{fig:my_label}
\end{figure}

\hypertarget{visualizing-distributions-of-covariance-matrices}{%
\subsection{Visualizing distributions of covariance matrices}\label{visualizing-distributions-of-covariance-matrices}}

\begin{quote}
\begin{quote}
Covariance matrices and their corresponding distributions play an important role in statistics. To understand the properties of distributions, we often rely on visualization methods. \citep{VisCov}
\end{quote}
\end{quote}

Visualizing a distribution in a high-dimensional space is a challenge, with the additional difficulty that covariance matrices must be positive semi-definite, a restriction that forces the joint distribution of the covariances into an oddly-shaped subregion of the space.

\begin{figure}
    \centering
    \includegraphics{wtf_covViz.PNG}
    \caption{Visualize distributions of covariance matrices}
    \label{fig:my_label}
\end{figure}

\hypertarget{a-geometrical-interpretation-of-an-alternative-formula-for-the-sample-covariance}{%
\subsection{A geometrical interpretation of an alternative formula for the sample covariance}\label{a-geometrical-interpretation-of-an-alternative-formula-for-the-sample-covariance}}

Kevin Hayes \citep{Hayes} proposes a new geometric and visual interpretation of covariance, based on the application of Hefferman's variance formula. He extends this formula to the covariance of a sample to extract his results.

\begin{itemize}
\tightlist
\item
  Formula from Heffernan definition of covariance:
\end{itemize}

\[
cov(X,Y)= \frac{2}{n(n-1)}\sum_{i=1}^{n-1}\sum_{j>i}^{n}\frac{1}{2}(x_i-x_j)(y_i - y_j)
\]

\begin{quote}
\begin{quote}
Geometrically, \(\frac{1}{2}(x_i-x_j)(y_i - y_j)\) is Â±1 times the area right-triangle formed with the difference vector \((x_i â x_j, y_j)\) as its hypotenuse, where negatively sloped difference tors incur a \((â1)\) sign and positively sloped difference vectors take a \((+1)\) sign. \citep{Hayes}
\end{quote}
\end{quote}

\begin{figure}
    \centering
    \includegraphics{hayes.PNG}
    \caption{Some examples of covariance as area of rectangles}
    \label{fig:my_label}
\end{figure}

\hypertarget{covariance-as-signed-area-of-rectangles}{%
\subsection{Covariance as signed area of rectangles}\label{covariance-as-signed-area-of-rectangles}}

This article \citep{chudzicki} was written following a very interesting conversation on the \textbf{stats.statckexchange} site. The initial topic of this conversation was: ``How to explain covariance to someone who only understands the notion of mean?''.

\textbf{Instructions for use:}

\begin{itemize}
\tightlist
\item
  Draw all possible such rectangles. Color them transparently, making the positive rectangles red (say) and the negative rectangles ``anti-red'' (blue).
\item
  The covariance is the net amount of red in the plot (treating blue as negative values).
\end{itemize}

Let's deduce some properties of covariance. Understanding of these properties will be accessible to anyone who has actually drawn a few of the rectangles.

\begin{itemize}
\tightlist
\item
  Bilinearity:
\end{itemize}

Because the amount of red depends on the size of the plot, covariance is directly proportional to the scale on the \(x\)-axis and to the scale on the \(y\)-axis.

\begin{itemize}
\tightlist
\item
  Correlation:
\end{itemize}

Covariance increases as the points approximate an upward sloping line and decreases as the points approximate a downward sloping line. This is because in the former case most of the rectangles are positive and in the latter case, most are negative.

\begin{itemize}
\tightlist
\item
  Relationship to linear associations:
\end{itemize}

Because non-linear associations can create mixtures of positive and negative rectangles, they lead to unpredictable (and not very useful) covariances. Linear associations can be fully interpreted by means of the preceding two characterizations.

\begin{itemize}
\tightlist
\item
  Sensitivity to outliers:
\end{itemize}

A geometric outlier (one point standing away from the mass) will create many large rectangles in association with all the other points. It alone can create a net positive or negative amount of red in the overall picture.

\begin{figure}
    \centering
    \includegraphics{covRect.PNG}
    \caption{Covariance rectangle}
    \label{fig:my_label}
\end{figure}

\hypertarget{our-current-project-the-package-plotnetrec}{%
\section{Our current project: the package Plotnetrec}\label{our-current-project-the-package-plotnetrec}}

The Plonetrec package and its associated graphs is an alternative representation of a scatterplot. The aim is to detect certain particularities in a dataset.

\begin{figure}[H]
\centering
   \includegraphics{all.png}
\end{figure}

We locate in a standard case, whitout issues in he dataset.

\begin{itemize}
\item
  The graph at the top right relates all the observations to their respective averages and we draw all corresponding rectangles . If the slope of the diagonal is negative, the rectangle is red and conversely, the rectangle is blue.
\item
  The bottom-left graphic relates all observations in pairs to draw all rectangle. In this figure, their is superposition of the rectangles. The sum of all rectangles areas gives two times the Covariance.
\item
  The bottom-righ graphic differs from the bottom-left by the coloring. We apply a transparency effect according to the net amount of the two colors. A superposition of red and blue will cancel each other and it results a lack of color. The more a color is represented, the more blue is intense.
\end{itemize}

Let's compare with some problematic.

\hypertarget{heterogeneity}{%
\subsection{Heterogeneity}\label{heterogeneity}}

\vspace{0.6cm}

\begin{minipage}{0.70\textwidth}
    \includegraphics{plotnetrec_heterogeneity.png}
\end{minipage}
\hspace{3.5ex}
\begin{minipage}{0.25\textwidth}
   We clearly notice the heterogeneous character of $Y$. The Plotnetrec graphic seems to identify the split area of the groups.
\end{minipage}

\hypertarget{heteroskedasticity}{%
\subsection{Heteroskedasticity}\label{heteroskedasticity}}

\vspace{0.6cm}

\begin{minipage}{0.70\textwidth}
    \includegraphics{plotnetrec_heteroscedasticite.png}
\end{minipage}
\hspace{3.5ex}
\begin{minipage}{0.25\textwidth}
  The heteroskedasticity case has to be redesigned.
\end{minipage}

\hypertarget{non-linear-relationship}{%
\subsection{Non linear relationship}\label{non-linear-relationship}}

\vspace{0.6cm}

\begin{minipage}{0.70\textwidth}
    \includegraphics{plotetrect_non_linear.png}
\end{minipage}
\hspace{3.5ex}
\begin{minipage}{0.25\textwidth}
    Non linear case seems very expressive in the panel of graphics beside.
\end{minipage}

\hypertarget{linear-regression-and-first-reliability-measure}{%
\chapter{Linear regression and first reliability measure}\label{linear-regression-and-first-reliability-measure}}

\hypertarget{simple-linear-regression}{%
\section{Simple linear regression}\label{simple-linear-regression}}

\hypertarget{estimation-by-ordinary-least-squares}{%
\subsection{Estimation by ordinary least squares}\label{estimation-by-ordinary-least-squares}}

\[y_i = \beta_0 + \beta_1x_i +  \epsilon_i\]

Where \(\epsilon_i\) is the residual or deviation of \(y_i\) from the line \(\beta_0 + \beta_1x_i\).

\begin{itemize}
\tightlist
\item
  Ordinary least square
\end{itemize}

We need to find the values of \(\beta_0\) and \(\beta_1\) that minimizes the criterion:
\[S = \sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n (y_i - (\beta_0 + \beta_1x_i))^2\]

Minimize this sum gives:

\[\hat{\beta_0}= \bar{y} - \hat{\beta_1}\bar{x}\]
\[\hat{\beta_1}= \frac{\sum_{i=1}^n(x_i -\bar{x})(y_i -\bar{y})}{\sum_{i=1}^n(x_i -\bar{x})^2}\]

\begin{itemize}
\tightlist
\item
  Estimated simple linear regression model:
\end{itemize}

\[y_i = \hat{\beta_0} + \hat{\beta_1x_i} +  e_i\]

From which we can calculate a few additionnal quantities:

\begin{itemize}
\item
  \(\hat{y_i} = \hat{\beta_0} + \hat{\beta_1x_i}\) ; \(\hat{y_i}\) is the \textbf{predicted value} (or predicted fit) of \(y\) for the \(i^{th}\) observation in the sample.
\item
  \(e_i=y_i -\hat{y_i}\) ; is the \textbf{observed error} (or residual) for the \(i^{th}\) observation in the sample.
\item
  \(SSE = \sum_{i=1}^n (y_i -\hat{y_i})^2\) ; is the \textbf{sum of squared observed errors} for all observations in a sample of size \(n\).
\end{itemize}

\hypertarget{measuring-overall-variation-from-the-sample-line}{%
\subsection{Measuring overall variation from the sample line}\label{measuring-overall-variation-from-the-sample-line}}

\begin{itemize}
\item
  \(MSE = \frac{SSE}{n-p}\) , where \(p\) is the number of parameters of the regression equation. \(p=2\) for regression with only one variable.
\item
  \(s = RMSE = \sqrt{(MSE)}\)
\item
  \(SSTO=\sum_{i=1}^n (y_i -\bar{y_i})^2\)
\item
  Coefficient of determination:
\end{itemize}

\(R^2 = \frac{SSTO-SSE}{SSTO} = \frac{SSR}{SSTO}\) is the proportion of variation in \(y\) that is explained by \(x\).

In other words, the coefficient of determination is then the ratio of the variance explained by the SSE regression to the total SST variance.

The coefficient of determination is the square of the linear correlation coefficient \(R^2\) between the predicted values \(\hat{y}_{i}\) and the measurements \(y_i\):

\[R^2=corr(\hat{y}_{i},y_i)\]

\(R^2\) does not indicate whether:

\begin{itemize}
\tightlist
\item
  the independent variables are a cause of the changes in the dependent variable;
\item
  omitted-variable bias exists;
\item
  the correct regression was used;
\item
  the most appropriate set of independent variables has been chosen;
\item
  there is colinearity present in the data on the explanatory variables;
\item
  the model might be improved by using transformed versions of the existing set of independent variables;
\item
  there are enough data points to make a solid conclusion.
\end{itemize}

\hypertarget{multiple-linear-regression}{%
\section{Multiple linear regression}\label{multiple-linear-regression}}

\[{\displaystyle{\begin{cases}y_{1}=\beta_{0}+\beta_{1}x_{1,1}+\ldots +\beta_{p}x_{1,p}+\varepsilon_{1}\\y_{2}=\beta_{0}+\beta_{1}x_{2,1}+\ldots +\beta_{p}x_{2,p}+\varepsilon_{2}\\\vdots \\y_{n}=\beta_{0}+\beta_{1}x_{n,1}+\ldots +\beta_{p}x_{n,p}+\varepsilon _{n}\end{cases}}}\]

We aim to determine the coefficients of this regression. We need to use matrix writting style in order to express the multiple linear regression.

\[{\displaystyle {\begin{pmatrix}y_{1}\\\vdots \\y_{n}\end{pmatrix}}={\begin{pmatrix}1&x_{1,1}&\cdots &x_{1,p}\\\vdots &\vdots &\ddots &\vdots \\1 &x_{n,1}&\cdots &x_{n,p}\end{pmatrix}}{\begin{pmatrix}\beta_{0}\\\beta_{1}\\\vdots \\\beta_{p}\\\end{pmatrix}}+{\begin{pmatrix}\varepsilon_{1}\\\vdots \\\varepsilon _{n}\\\end{pmatrix}}}\]

With a stacked notation, it gives:

\[Y = \beta X + \varepsilon \]
And like simple linear regression, the ordinary least squares method search \(\beta\)'s vector that minimize the criterion.

\[{\displaystyle \min \sum_{i=1}^{n}{\hat{\epsilon }}_{i}^{2}=\min_{{\hat{\beta}}_{0},.,{\hat{\beta}}_{p}}\sum_{i=1}^{n}(y_{i}-{\hat{\beta}}_{0}-{\hat {\beta}}_{1}x_{i,1}-\cdots -{\hat{\beta}}_{p}x_{i,p})^{2}}\]
And so the ordinary least squares estimators is determined by:

\[\displaystyle {\hat {\beta}}=(X^{T}X)^{-1}X^{T}Y\qquad\]

\hypertarget{visualisation-of-regression-and-correlation-beetween-variable}{%
\chapter{Visualisation of regression and correlation beetween variable}\label{visualisation-of-regression-and-correlation-beetween-variable}}

\hypertarget{state-of-the-art-1}{%
\section{State of the art}\label{state-of-the-art-1}}

\hypertarget{more-on-venn-diagrams-for-regression-figure-6.1}{%
\subsection{More on Venn Diagrams for Regression (figure 6.1)}\label{more-on-venn-diagrams-for-regression-figure-6.1}}

Kennedy \citep{kennedy} extended the Venn diagram to the exposition of bias and variance in the context of the classical linear regression (CLR) model, written as \(y = Xb + e\).

\begin{itemize}
\tightlist
\item
  Purple area: variation in \(y\) uniquely explained by variation in \(X\)
\item
  A larger purple area means that more information is used in estimation, implying a smaller variance of the \(\beta_x\) estimate.
\item
  The black area: the variation in \(y\) that cannot be explained by \(X\)
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width= 250 pt]{vennken.PNG}
    \caption{Venn Diagram in the context of classical linear regression}
\end{figure}

\textbf{Another example of Venn Diagram: relating species richness to the structure of continuous landscapes (figure 6.2).}

\begin{itemize}
\tightlist
\item
  Venn diagram representing the partition of the variance of the response variable \(Y\) (species richness) between two sets of explanatory variables, namely landscape data (a) and space (c).
\item
  The variance jointly explained by landscape data and space is represented by (b) in the diagram. The rectangle represents the total variance in \(Y\).
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width= 250 pt]{venn.PNG}
    \caption{Venn Diagram relating species richness to the structure of continuous landscapes}
\end{figure}

\hypertarget{a-geometric-approach-to-compare-variables-in-a-regression-model}{%
\subsection{A geometric approach to compare variables in a regression model}\label{a-geometric-approach-to-compare-variables-in-a-regression-model}}

The article of Brings \citep{Bring}, proposes a geometric approache to compare variables in a regression model.

\begin{quote}
\begin{quote}
``This article gives a brief introduction to the geometric approach in regression analysis, and then geometry is used to shed some light on the problem of comparing the''importance'' of the independent variables in a multiple regression model. Even though no final answer of how to assess variable importance is given, it is still useful to illustrate the different measures geometrically to gain a better understanding of their properties''.
\end{quote}
\end{quote}

The model vector, \(\hat{y}\) is the perpendicular projection of \(y\) on \(x\). Why it is the best estimate?

\begin{itemize}
\tightlist
\item
  The squared length of \(y\), \(||y||^2 = \sum y_i^2\) is the total sum of square, \(SS_{tot}\)
\item
  The square length of \(\hat{y}\), \(||\hat{y}||^2= \sum \hat{y}^2=SS_{reg}\)
\item
  The square length of the vector \(y-\hat{y}\), \(||y - \hat{y}||^2=\sum(y_i-\hat{y}_i)^2 = SS_{res}\)
\end{itemize}

In other words, the observations vector is decomposed into a model vector and an error vector, and the shortest possible length of \(y-\hat{y}\) is found by projecting \(y\) perpendicular in \(x\).

\begin{itemize}
\tightlist
\item
  The shortest possible length of \(y - \hat{y}\) is found by projecting \(y\) perpendicular on \(x\).
\item
  \(R^2 = \frac{SS_{reg}}{SS_{tot}}= \frac{||\hat{y}||^2}{||y||^2}\) (the \(y\) vector is standardized to have lenght 1.)
\item
  \[r_{xy} = 
  \begin{cases} ||\hat{y}||  \ \ \ \text{if}  \  \theta \leq 90 \degree \ \ \ \text{or} \ \theta \geq 270 \degree  \\ -||\hat{y}|| \ \ \ \text{if}  \ 90\degree \leq \theta \leq 270\degree
  \end{cases}\]
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width= 250 pt]{bring1.PNG}
    \caption{Geometrical representation of linear regression}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width= 250 pt]{Bring.PNG}
    \caption{Geometrical representation of the product measure, $Z_i = B_i r_i$}
\end{figure}

\hypertarget{two-additional-views-of-linear-regression-coefficients}{%
\subsection{Two Additional Views of Linear Regression Coefficients}\label{two-additional-views-of-linear-regression-coefficients}}

The author \citep{Cli} proposes an interesting interpretation of the slope in the least square method. The linear regression line of \(y\) on \(x\), as determined by the method of least squares, passes through the central point with slope.

\textbf{A View of Linear Regression Coefficients as a weighted average slope}

\begin{itemize}
\tightlist
\item
  Consider any point \(P_i=(x_i,y_i)\) and \(P_o=(\bar{x}, \bar{y})\), the slope of the line \(P_i P_o\) is: \(b_i=\frac{y_i -\bar{y}}{x_i-\bar{x}}\).
\item
  Let's attach a weight \(w_i\) to each \(b_i\), if 2 points are very close, their slope is not very reliable (need little weight).
\item
  The ``distance'' between two points \(\to\) the distance projected on the \(x\)-axis. To avoid negative weight, we may then take the weight, \(w_i=(x_i-\bar{x})^2\).
\item
  Adopting this system of weighting, we see that \(b\) is the weighted mean of the \(b_i\)'s.
\end{itemize}

\begin{equation}
    b=\bar{b}=\frac{\sum b_i w_i}{\sum w_i} = \hat{\beta_1} =        \frac{\sum(x_i -\bar{x})(y_i -\bar{y})}{\sum(x_i -\bar{x})^2}
\end{equation}

\begin{quote}
\begin{quote}
``This concept of weight for a slope is represented in the accompanying diagrams. (See Figure 1). The slope in the lefthand side diagram has a much larger weight than that in the righthand side for regression of \(y\) on \(x\). If we were concerned with the regression of \(x\) on \(y\), the reverse would be true. Note that the actual distance between the two points in the two diagrams is the same.''
\end{quote}
\end{quote}

\begin{figure}
    \centering
    \includegraphics[width= 250 pt]{LI_slope.PNG}
    \caption{Representation of the weight equal to the square of the base, based on the slope from $P_1$ to $P_0$}
\end{figure}

\hypertarget{our-current-project-the-plotnetrec-package}{%
\section{Our current project, the Plotnetrec package:}\label{our-current-project-the-plotnetrec-package}}

\hypertarget{ols-diagramatic-representation}{%
\subsection{OLS diagramatic representation}\label{ols-diagramatic-representation}}

The principles of construction of the following diagram giving a diagrammatic representation of the regression and OLS are:

\begin{itemize}
\tightlist
\item
  We assume that the covariance (an average of all possible rectangles as seen previously among all pairs of points in a data sample) can be represented by a rectangle.
\item
  This rectangle has by definition a known area given by the calculation of the empirical covariance \(Cov(X,Y)\). But the covariance depends on three parameters since \(Cov(X,Y) =\rho \sigma_x \sigma_y\). To represent the covariance and draw the corresponding rectangle, we have to make an additional assumption i.e.~choose a normalization.
  We assume that one side of this rectangle is normalized by \(sigma_x\). In this case, the other side necessarily has a length of \(\rho \sigma_y\).
\item
  Following the same approach, we now assume that the variances of \(X\) and \(Y\) can be represented by two squares whose sides are of course the standard deviations of \(X\) and \(Y\) respectively.
\end{itemize}

The diagram allows:

\begin{itemize}
\tightlist
\item
  To represent the OLS estimator by the slope of the diagonal of the rectangle representing the covariance. Geometrically, we obtain that if \(X\) increases by one standard deviation then \(Y\) increases by \(\rho\) standard deviation of the dependent variable \(Y\).
\item
  To represent the explained variance in the total variance.
\item
  Correctly represent the coefficient of determination as the relative share of the explained variance in the total variance. With Venn diagrams sometimes used to represent the \(R^2\), visual perception is based on the relative size of two surfaces and their intersection. The proposed diagram is not an ad hoc construction, it is based on a methodological and theoretical foundation allowing not only to represent the \(R^2\) but also to read its value correctly.
\end{itemize}

Reminder:

\begin{align*}
& \rho = \frac{COV(X,Y)}{\sigma_x \sigma_y}\\
\Leftrightarrow & COV(X,Y) = \rho \sigma_y \times \sigma_x
\end{align*}

\begin{figure}[H]
\centering
  \begin{tikzpicture}[scale=1.2]
    \def\sx{3};
    \def\sy{4};
    \def\r{0.5}; %postive%
    \coordinate (O) at (0,0);
    \coordinate (A) at (-\sx,-\sx);
    \coordinate (B) at (\sy,\sy);
    \coordinate (C) at (-\sx,{\sy*\r});
    \draw[fill=green!20!white, draw=black] (O) rectangle (A); %variance for X%
    \draw[fill=orange!20!white, draw=black] (O)rectangle (B); %variance for Y%
    \draw[<->,fill=blue!40!white, draw=black] (O)rectangle(C);%covariance%
    \draw[<->,thick] (A) -- node[below, scale = 0.8] {$\sigma_x$} (0,-\sx);
    \draw[<->,thick] (O) -- node[below, scale = 0.8] {$\sigma_y$} (\sy,0);
    \draw[<->,thick] (O) -- node[right, scale = 0.8] {$\rho\sigma_y$} (0,\r*\sy);
    \draw[<->,thick] (O) -- node[left, scale = 0.8] {$\rho\sigma_y$} (0,\r*\sy);
    \tkzDefPoint(0,0){t}
    \tkzDefPoint(-\sx,0){tt};
    \tkzDefPoint(0,\r*\sy){ttt};
    \tkzMarkAngle[size=1, draw=black,fill=blue!20,mark=|](t,tt,ttt)];
    \node[right, scale = 0.58] at (-\sx*0.6,0.3){$\arctan(\hat{\beta}=\frac{\rho\sigma_y}{\sigma_x})$};
    % Regression line
    \draw[-,ultra thick] (-\sx,0) -- node[midway,sloped,above, scale = 0.6] {Regression line} (0,\r*\sy);
    %exlained variance%
    \draw[fill=orange!40!white, draw=black] (O)rectangle (\r*\sy,\r*\sy);
    % function r^2%
    \draw[red,name path=curve 1,samples=200] plot[variable=\x,domain=0:\sy]
    ({\x},{(\x^2/\sy});
    % read the value for RÃÂ² and put a new scaled axe%
    \draw[red] (\sy*1.1,0) node[below, scale = 0.6] {0} --(\sy*1.1,\sy) node [above, scale = 0.6] {1};
    \draw[-stealth,dotted,red] (\r*\sy,\r^2*\sy) -- (\sy*1.1,\r^2*\sy) node[right,rotate=0]{$R^2=\rho^2$};
  \end{tikzpicture}
\end{figure}

\hypertarget{multiple-linear-regression-diagram}{%
\subsubsection{Multiple linear regression diagram}\label{multiple-linear-regression-diagram}}

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=1.2]
  \def\sx{3};
  \def\sy{4};
  \def\sz{2};
  \def\rxy{0.8}; %positive%
  \def\rxz{0.5}; %positive%
  \def\ryz{0.5}; %positive%
  %$$$$$$$$$$$$$$$$$$$$$%
  \def\vrxz{0.8660254};
  \def\vryz{0.8660254};
  \def\ryxz{0.7333333};
  %\def\truca{(sqrt(1-(\rxz)^2))};
  %\def\truc{(-\sx*(sqrt(1-(\rxz)^2)))};
  %\def\truc{-\sx*0.5};
  \coordinate (O) at (0,0);
  \coordinate (A) at (-\sx,-\sx);
  \coordinate (B) at (\sy,\sy);
  \coordinate (C) at (-\sx,{\sy*\rxy});
  \coordinate (D) at (-\sx*\vrxz,-\sx*\vrxz);
  \coordinate (E) at (\sy*\vryz,\sy*\vryz);
  \coordinate (F) at (-\sx-\sz,-\sx-\sz);
  \coordinate (G) at (\sy+\sz,\sy+\sz);
  \coordinate (H) at (\ryxz*\vryz*\sy,\ryxz*\vryz*\sy);
  \coordinate (I) at (\rxy*\sy,\rxy*\sy);

  \draw[fill=magenta!80!white, draw=black] (A) rectangle (F); %variance for z%
  \draw[fill=magenta!80!white, draw=black] (\sy,\sy) rectangle (G); %variance for z%

  \draw[fill=green!20!white, draw=black] (O) rectangle (A); %variance for X%
  \draw[fill=orange!20!white, draw=black] (O)rectangle (B); %variance for Y%
  \draw[<->,fill=blue!20!white, draw=black] (O)rectangle(C);%covariance X Y%

  %\draw[<->,thick] (O) -- node[left] {$\rho_{xy}\sigma_y$} (0,\rxy*\sy);

  %\draw[fill=green!50!white, draw=black] (O) rectangle (D); %residual variance x.z%
  \shade[inner color=magenta!40, outer color=green!60,opacity=1, draw=black](O) rectangle (D); %variance rÃ©siduelle x.z%

  %\draw[fill=orange!50!white, draw=black] (O) rectangle (E);%variance rÃ©siduelle y.z%
  \shade[top color=orange,bottom color=magenta!30,opacity=1,, draw=black] (O) rectangle (E);%variance rÃ©siduelle y.z%
  %\draw[fill=red!50!white, draw=black] (O) rectangle (H);%R2 partiel de X sur Y purgÃ© de l'effet de Z%
  \shade[inner color=green,outer color=orange,draw=black] (O) rectangle (H);%R2 partiel de X sur Y purgÃ© de l'effet de Z%

  %\draw[fill=gray!50!white, draw=black,opacity=0.5] (O) rectangle (I); %R2 RLS Y=a+bX%
  \shade[top color=green!50,bottom color=orange,opacity=0.5, draw=black] (O) rectangle (I); %R2 RLS Y=a+bX%

  \draw[<->,thick] (A) -- node[below] {$\sigma_x$} (0,-\sx);
  \draw[<->,thick] (O) -- node[below] {$\sigma_y$} (\sy,0);
  \draw[<->,thick] (F) -- node[below] {$\sigma_z$} (-\sx,-\sx-\sz);
  \draw[<->,thick] (\sy,\sy) -- node[below] {$\sigma_z$} (\sy+\sz,\sy);
  % Regression line
  \draw[-,ultra thick] (-\sx,0) -- node[midway,sloped,above] {RLS line} (0,\rxy*\sy);
  \draw[-,ultra thick] (-\sx*\vrxz,0) -- node[midway,sloped,above] {RLM line} (0,\ryxz*\vryz*\sy);

\end{tikzpicture}
\end{figure}

\hypertarget{correlation-representation}{%
\subsection{Correlation representation}\label{correlation-representation}}

Keep going with alternative representations, we introduce visualization of the correlation coefficient.

First of all, we need some reminders:

\[\rho = \frac{COV(X,Y)}{\sigma_x \sigma_y}\]

\[V(X+Y) = V(X) + V(Y) + 2COV(X,Y)\]

\begin{itemize}
\tightlist
\item
  Limits: \(\rho = 1\)
\end{itemize}

When \(\rho = 1\) then \(COV(X,Y) = \sigma_x \sigma_y\)

By replacing, we found: \(V(X+Y) = V(X) + V(Y) + 2\sigma_x \sigma_y\)

\begin{minipage}{0.60\textwidth}
    \begin{tikzpicture}[scale=0.7]
      \def\sx{3};
      \def\sy{4};
      \def\r{1}; %postive%
      \coordinate (O) at (0,0);
      \coordinate (A) at (-\sx,-\sx);
      \coordinate (B) at (\sy,\sy);
      \coordinate (C) at (-\sx,{\sy*\r});
      \draw[fill=green!580!white, draw=black] (O) rectangle (A); %variance for X%
      \draw[fill=orange!80!white, draw=black] (O)rectangle (B); %variance for Y%
      \draw[<->,fill=blue!80!white, draw=black] (O)rectangle(C);%covariance%
      \draw[<->,fill=blue!80!white, draw=black] (O)rectangle(\sy,-\sx);%covariance%
      \draw[<->,thick] (A) -- node[below] {$\sigma_x$} (0,-\sx);
      \draw[<->,thick] (O) -- node[below] {$\sigma_y$} (\sy,0);
      \shade[left color=green,outer color=yellow,opacity=0.5,draw=black,very thick](-\sx,-\sx) -- (-\sx,\sy) -- (\sy,\sy) -- (\sy,-\sx)-- cycle;
    \end{tikzpicture}
\end{minipage} 
\hspace{3.5ex}
\begin{minipage}{0.35\textwidth}
Here we decide to represent $V(X+Y)$ like a square in the middle. And due to this last equation, we represent $V(X+Y)$ as an area which equals to the sum of $4$ rectangles area: $V(X) + V(Y) + 2\sigma_x \sigma_y$
\end{minipage}

\begin{itemize}
\tightlist
\item
  Limits : \(\rho = -1\)
\end{itemize}

When \(\rho = -1\) then \(COV(X,Y) = -\sigma_x \sigma_y\)

By replacing, we found: \(V(X+Y) = V(X) + V(Y) - 2\sigma_x \sigma_y\)

\begin{minipage}{0.60\textwidth}
    \begin{tikzpicture}[scale=0.7]
      \def\sx{3};
      \def\sy{4};
      \def\r{1}; %postive%
      \coordinate (O) at (0,0);
      \coordinate (A) at (-\sx,-\sx);
      \coordinate (B) at (\sy,\sy);
      \coordinate (C) at (-\sx,{\sy*\r});
      \draw[fill=green!580!white, draw=black] (O) rectangle (A); %variance for X%
      \draw[fill=orange!80!white, draw=black] (O)rectangle (B); %variance for Y%
      \draw[<->,fill=blue!80!white, draw=black] (O)rectangle(C);%covariance%
      \draw[<->,fill=blue!80!white, draw=black] (O)rectangle(\sy,-\sx);%covariance%
      \draw[<->,thick] (A) -- node[below] {$\sigma_x$} (0,-\sx);
      \draw[<->,thick] (O) -- node[below] {$\sigma_y$} (\sy,0);
      \shade[left color=green,outer color=yellow,opacity=0.5,draw=black,very thick](-\sx,-\sx) -- (-\sx,\sy) -- (\sy,\sy) -- (\sy,-\sx)-- cycle;
      \shade[inner color=green,outer color=orange,opacity=3.5,draw=black,very thick] (0,0) -- (0,\sy-\sx) -- (\sy-\sx,\sy-\sx) -- (\sy-\sx,0)-- cycle;
<<<<<<< HEAD
    \end{tikzpicture}
\end{minipage} 
\hspace{3.5ex}
\begin{minipage}{0.35\textwidth}
 Here we decide to represent $V(X+Y)$ like a square in the middle. And due to this last equation, we represent $V(X+Y)$ as an area which equals to the sum of $4$ rectangles area: $V(X) + V(Y) - 2\sigma_x \sigma_y$
\end{minipage}

\begin{itemize}
\tightlist
\item
  Limits: \(\rho = 0\)
\end{itemize}

When \(\rho = 0\) then \(COV(X,Y) = 0\)

By replacing, we found: : \(V(X+Y) = V(X) + V(Y)\)

\begin{minipage}{0.60\textwidth}
    \begin{tikzpicture}[scale=0.7]
      \def\sx{3};
      \def\sy{4};
      \def\r{1}; %postive%
      \coordinate (O) at (0,0);
      \coordinate (A) at (-\sx,-\sx);
      \coordinate (B) at (\sy,\sy);
      \coordinate (C) at (-\sx,{\sy*\r});
      \draw[fill=green!580!white, draw=black] (O) rectangle (A); %variance for X%
      \draw[fill=orange!80!white, draw=black] (O)rectangle (B); %variance for Y%
      \draw[<->,fill=blue!80!white, draw=black] (O)rectangle(C);%covariance%
      \draw[<->,fill=blue!80!white, draw=black] (O)rectangle(\sy,-\sx);%covariance%
      \draw[<->,thick] (A) -- node[below] {$\sigma_x$} (0,-\sx);
      \draw[<->,thick] (O) -- node[below] {$\sigma_y$} (\sy,0);
      \shade[left color=green,outer color=yellow,opacity=0.5,draw=black,very thick](-\sx,-\sx) -- (-\sx,\sy) -- (\sy,\sy) -- (\sy,-\sx)-- cycle;
      \shade[inner color=green,outer color=orange,opacity=3.5,draw=black,very thick] (0,0) -- (0,\sy-\sx) -- (\sy-\sx,\sy-\sx) -- (\sy-\sx,0)-- cycle;
      \draw [<->,fill=gray!40!white, opacity=0.4,draw=black,very thick] (-\sx,0) -- (0,\sy) -- (\sy,\sy-\sx) -- (\sy-\sx,-\sx)-- cycle;
      \draw [<->,fill=gray!80!white, opacity=0.4,draw=black,very thick] (0,0) -- (0,\sy-\sx) -- (\sy-\sx,\sy-\sx) -- (\sy-\sx,0)-- cycle;
    \end{tikzpicture}
\end{minipage} 
\hspace{3.5ex}
\begin{minipage}{0.35\textwidth}
Here we decide to represent $V(X+Y)$ like a square in the middle. And due to this last equation, we represent $V(X+Y)$ as an area which equals to the sum of $2$ rectangles area: $V(X) + V(Y)$
\end{minipage}

\begin{itemize}
\tightlist
\item
  Interest
\end{itemize}

Correlation matrix with distinct mathematical construction.

\definecolor{bleu}{RGB}{112,204,215}

\begin{figure}
\centering
  \begin{tikzpicture}[scale=0.7]
    \def\sx{3};
    \def\sy{4};
    \def\r{1}; %postive%
    \coordinate (O) at (0,0);
    \coordinate (A) at (-\sx,-\sx);
    \coordinate (B) at (\sy,\sy);
    \coordinate (C) at (-\sx,{\sy*\r});
    \draw[<->,fill=bleu, draw=black] (-\sx,0) -- (-\sx,\sy) --(0,\sy);
    \draw[<->,fill=bleu, draw=black] (-\sx,0) -- (-\sx,-\sx) --(\sy-\sx,-\sx);
    \draw[<->,fill=bleu, draw=black] (\sy-\sx,-\sx) -- (\sy,-\sx) --(\sy,\sy-\sx);
    \draw[<->,fill=bleu, draw=black] (0,\sy)--(\sy,\sy) --(\sy,\sy-\sx);
    \shade[top color=red,bottom color=red,opacity=3.5,draw=black,very thick] (0,0) -- (0,\sy-\sx) -- (\sy-\sx,\sy-\sx) -- (\sy-\sx,0)-- cycle;
    \draw [<->,fill=gray!40!white, opacity=0.4,draw=black,very thick] (-\sx,0) -- (0,\sy) -- (\sy,\sy-\sx) -- (\sy-\sx,-\sx)-- cycle;
    \draw [<->,fill=gray!80!white, opacity=0.4,draw=black,very thick] (0,0) -- (0,\sy-\sx) -- (\sy-\sx,\sy-\sx) -- (\sy-\sx,0)-- cycle;
    \draw [-stealth, color = blue,line width=3pt](0,\sy) -- (-\sx,\sy);
    \draw [-stealth,color = blue,  line width=3pt](-\sx,0) -- (-\sx,-\sx);
    \draw [-stealth,color = blue, line width=3pt](\sy-\sx,-\sx) -- (\sy,-\sx);
    \draw [-stealth,color = blue, line width=3pt](\sy,\sy-\sx) -- (\sy,\sy);
    \draw [-stealth, color = red, line width=3pt](0,\sy) -- (0,\sy - \sx);
    \draw [-stealth,color = red, line width=3pt](-\sx,0) -- (0, 0);
    \draw [-stealth,color = red, line width=3pt](\sy-\sx,-\sx) -- (\sy-\sx,0);
    \draw [-stealth,color = red, line width=3pt](\sy,\sy-\sx) -- (\sy-\sx,\sy-\sx);
  
  \end{tikzpicture}
\end{figure}

Let's observe what dynamically happens starting from the independant case (\(\rho = 0\)). If the correlation increases positively, the square increases its area and pivots toward the case where the correlation is perfectly positive. Conversely, if the correlation increases negatively, the area's square decreases and pivote towards the perfectly negative correlation place.

\[V(X+Y) = V(X) + V(Y) + 2COV(X,Y)\]

\hypertarget{causality}{%
\chapter{Causality}\label{causality}}

In the basic model \(y=\alpha+\beta x+\varepsilon\) the OLS estimator \(\hat{\beta}\) is unbiased if \(x\) is independent of \(\varepsilon\). We cannot know a priori if this is the case and in any case not a posteriori at the end of the estimation since the residuals are by construction independent of \(X\). We can easily show that there is no unique solution in \(\hat{\beta}\) of the maximum likelihood when we increase the model by a free parameter representing the correlation between \(\varepsilon\) and \(x\).
The prediction \(\mathbb{E}(y~/x)=\hat{\beta}x\) is correct only if \(\mathbb{E}(\varepsilon~/x)=0\).
Otherwise \(\mathbb{E}(\varepsilon~/x)=0\) there is an endogeneity and \(\hat{\beta}\) is not an exact measure of the marginal effect of an exogenous variation of \(x\).
The main situations of endogeneity encountered and studied are related to the problems:

\begin{itemize}
\tightlist
\item
  Simultaneous causality
\item
  Omitted variables
\item
  Measurement errors on the variables
\item
  Selection bias
\item
  Poorly specified functional form
\end{itemize}

\hypertarget{the-main-cases}{%
\section{The main cases}\label{the-main-cases}}

\hypertarget{simultaneous-causality}{%
\subsection{Simultaneous causality}\label{simultaneous-causality}}

Is it enough to simply show by a diagram that regressing \(y\) on \(x\) and then \(x\) on \(y\) does not give the same result?
The answer is no, it is not enough!

\hypertarget{omitted-variables}{%
\subsection{Omitted variables}\label{omitted-variables}}

\ldots. Repeat the previous results in a simplified way to illustrate the effect of omitting a variable and present the conditions under which the omission of a variable leads to a bias on the model parameters.

\hypertarget{measurement-errors}{%
\subsection{Measurement errors}\label{measurement-errors}}

Let's consider the following RLS model

\[y^{\star}=\alpha+\beta x^{\star}+\varepsilon\]

We do not observe \(y^{\star}\) (but \(y=y^{\star}+v_y\)), nor \(x^{\star}\) (but \(x=x^{\star}+u_x\)).

Consider the following assumptions:

\(\mathbb{E}(u_x)=\mathbb{E}(v_y)=0\)

\(\mathbb{E}(y v_y)=\mathbb{E}(y u_x)=\mathbb{E}(x u_y)=\mathbb{E}(x v_x)=0\)

\(\mathbb{E}(\varepsilon u_y)=0\)

The measurement errors are of zero expectation, and independent of the variables and the error term.
Under these assumptions, for two randomly chosen points \(k\) and \(l\), the covariance between \(X\) and \(Y\) does not change.

\[\mathbb{E}[(x_k-x_l)(y_k-y_l)]\]

\[\mathbb{E}[(x_k^{\star}+u_k-(x_l^{\star}+u_l))(y_k^{\star}+v_k-(y_l^{\star}+v_l))]\]

\[\mathbb{E}[(x_k^{\star}+u_k)(y_k^{\star}+v_k)] -\mathbb{E}[x_k^{\star}+u_k]\mathbb{E}[y_l^{\star}+v_l] -\mathbb{E}[x_l^{\star}+u_l]\mathbb{E}[y_k^{\star}+v_k]+ \mathbb{E}[(x_l^{\star}+u_l)(y_l^{\star}+v_l)]\]

\[2Cov(x^{\star},y^{\star})\]

Measurement errors are detrimental to accuracy but on average they do not affect the covariance. The only essential problem is therefore related to the correct estimation of the parameters, while the correlation between the dependent and independent variables does not change. On the other hand, one should not confuse the effect of \(x\) on \(y\) with the effect of the measurement error of \(x\) on \(y\) (which is zero).

\hypertarget{measurement-error-on-the-dependent-variable}{%
\section{Measurement error on the dependent variable}\label{measurement-error-on-the-dependent-variable}}

Let us consider a measurement error on the dependent variable \(y\), i.e.~let \(\sigma_v^2 > 0\) and \(\sigma_u^2 = 0\). The model becomes:
\[y=alpha+beta x + v+ \varepsilon\]
The measurement error on the dependent variable does not matter in the sense that it has no biasing effect on the estimated parameters of the model. In practice, it can be considered as contributing to the disturbance term of the model. It is obviously undesirable, as anything that increases the noise in the model will tend to make the regression estimates less accurate, but it has no impact in terms of bias in the estimates.

In this case, we do not make a diagram for a graphical proof because it is so obvious!

\hypertarget{measurement-error-on-the-independent-variable}{%
\section{Measurement error on the independent variable}\label{measurement-error-on-the-independent-variable}}

We consider the case where the measurement error is on the dependent variable \(x\), i.e.~Let \(sigma_v^2 =0\) and \(sigma_u^2 >0\).
The model becomes:
\[y=alpha+beta x -beta u + \varepsilon\]

In the regression of \(y\) on \(x\), the measurement error of \(x\) becomes part of the error in the regression equation related to the parameter \(\beta\), thus creating an endogeneity bias. The OLS estimator \(\hat{\beta}=\frac{Cov(x,y)}{Var(x)}\) is biased towards 0. The unbiased estimator is \(\beta=frac{Cov(x^\star ,y)}{Var(x^\star)}\).

Proof:

Since \(Cov(x^\star,y)=Cov(x,y)\), we have:
\[\frac{\hat{\beta}}{\beta}=\frac{Var(x^\star)}{Var(x)}=\frac{\sigma_x^{\star 2}}{\sigma_u^2+\sigma_x^{\star 2}} \implies plim~\hat{\beta}=\lambda \beta\]
With \(\lambda=\frac{\sigma_x^{\star 2}}{\sigma_u^2+\sigma_x^{\star 2}}\). Since \(0<lambda<1\) the coefficient \(\hat{\beta}\) is biased towards 0.
The bias depends on the level and the sign of \(\beta\):
\[
plim~\hat{\beta}-\beta=\lambda \beta-\beta=-(1-\lambda)\beta=-\frac{\sigma_u^2}{\sigma_u^2+\sigma_x^{\star 2}}\beta
\]

\hypertarget{diagram-of-the-model-with-measurement-error-on-the-independent-variable}{%
\subsection{Diagram of the model with measurement error on the independent variable}\label{diagram-of-the-model-with-measurement-error-on-the-independent-variable}}

The following figure illustrates the problem in a very simple way.

\begin{figure}[H]
\begin{tikzpicture} [scale=1.5]
\def\sx{3};
\def\su{1.5};
\def\sxt{3.354101966};
\def\sy{4};
\def\r{0.5}; %positive%
\def\truc{1.788854382}
\coordinate (O) at (0,0);
\coordinate (A) at (-\sx,-\sx);
\coordinate (B) at (\sy,\sy);
\coordinate (C) at (-\sx,{\sy*\r});
\coordinate (D) at (-\sx,0);
\coordinate (E) at (-\sx-\su,-\su);
\coordinate (F) at (-\sxt,-\sxt);

\draw[fill=green!20!white, draw=black] (O) rectangle (A); %variance for X%
\draw[fill=orange!20!white, draw=black] (O)rectangle (B); %variance for Y%
\draw[<->,fill=blue!40!white, draw=black] (O)rectangle(C);%covariance%
\draw[<->,fill=green!10!white, draw=black] (D)rectangle(E);%variance for u%
\draw[fill=green!5!white, draw=black,opacity=0.5] (O) rectangle (F); %variance for Xt%
\draw[<->,fill=blue!30!white, draw=black,opacity=0.5] (-\sxt,0)rectangle(0,\truc);%covariance%

\draw[<->,thick] (A) -- node[below] {$\sigma_x^\star$} (0,-\sx);
\draw[<->,thick] (O) -- node[below] {$\sigma_y$} (\sy,0);
\draw[<->,thick] (O) -- node[left] {$\rho\sigma_y$} (0,\r*\sy);
\draw[<->,thick] (E) -- node[below] {$\sigma_u$} (-\sx,-\su);
\draw[<->,thick] (F) -- node[below] {$\sigma_x$} (0,-\sxt);
 
% Regression line
\draw[-,ultra thick] (-\sx,0) -- node[midway,sloped,above] {True Regression line} (0,\r*\sy);
% Biasied Regression line
\draw[-,ultra thick,red] (-\sxt,0) -- node[midway,sloped,below] {Biased Regression line} (0,\truc);
\end{tikzpicture}
\caption{\label{measurement error on x} Regression of $y$ on $x$ with measurement error on $X$}
\end{figure}

\hypertarget{instrumental-variables}{%
\section{Instrumental variables}\label{instrumental-variables}}

\[Y=\alpha + \beta X +u\]
\[X=\theta + \delta Z +v\]
\[\beta_{IV}=\frac{Cov(Y,Z)}{Cov(X,Z)} =\frac{\rho_{zy} \sigma_y}{\rho_{zx} \sigma_x}\]

\hypertarget{state-of-the-art-of-r-packages-in-visualization-econometrics-and-data-viz-more-broadly}{%
\chapter{State of the art of R packages in visualization econometrics (and data-viz more broadly)}\label{state-of-the-art-of-r-packages-in-visualization-econometrics-and-data-viz-more-broadly}}

\begin{itemize}
\tightlist
\item
  R energy
\end{itemize}

\textbf{A complÃ©ter}

\begin{itemize}
\tightlist
\item
  viscov
\end{itemize}

Our visualizations follow the principle of decomposing a covariance matrix into scale parameters and correlations, pulling out marginal summaries where possible and using two and three-dimensional plots to reveal multivariate structure. Visualizing a distribution of covariance matrices is a step beyond visualizing a single covariance matrix or a single multivariate dataset.

\begin{itemize}
\tightlist
\item
  Colourpicker
\end{itemize}

Colourpicker is a tool for Shiny framework and for selecting colours in plots. This tool supports various options, such as alpha opacity, custom colour palettes, and more. The most common uses of this tool include the utilisation of the colourInput() function to create a colour input in Shiny as well as the use of the plotHelper() function/RStudio Addin to select colours for a plot.

\begin{itemize}
\tightlist
\item
  Esquisse
\end{itemize}

The esquisse package allows a user to interactively explore data by visualising it with the ggplot2 package. It allows a user to draw bar graphs, curves, scatter plots, histograms, export the graphs, and retrieve the code generating the graph. With the help of esquisse, one can quickly visualise the data according to their type as well as export to PNG or PowerPoint, and retrieve the code to reproduce the chart.

\begin{itemize}
\tightlist
\item
  ggplot2
\end{itemize}

ggplot is a popular package that is based on the grammar of graphics. The idea behind this library is that one can build every graph from the same components, such as a dataset, a coordinate system, and more. The package provides graphics language for creating intuitive and intricate plots. It allows a user to create graphs that represent both univariate and multivariate numerical and categorical data.

\begin{itemize}
\tightlist
\item
  ggvis
\end{itemize}

ggvis is a data visualisation package for R that allows to declaratively describe data graphics with a syntax similar in spirit to ggplot2. It allows creating rich interactive graphics locally in Rstudio or in the browser as well as leverage the infrastructure of the Shiny package to publish interactive graphics usable from any browser. The goal of ggvis is to make it easy to build interactive graphics for exploratory data analysis.

\begin{itemize}
\tightlist
\item
  ggforce
\end{itemize}

The ggforce is a package aimed at providing missing functionality to ggplot2 through the extension system introduced with ggplot2 v2.0.0. The goal of this package is to provide a repository of geoms, stats, among others. Using ggforce, one can enhance almost any ggplot by highlighting data groupings and focusing attention on interesting features of the plot.

\begin{itemize}
\tightlist
\item
  lattice
\end{itemize}

Lattice is a powerful high-level data visualisation system for R that is designed with an emphasis on multivariate data and allows to create multiple small plots easily. The lattice package attempts to improve on base R graphics by providing better defaults and the ability to display multivariate relationships easily. Particularly, the package supports the creation of trellis graphs that show a variable or the relationship between variables, conditioned on one or more other variables.

\begin{itemize}
\tightlist
\item
  Plotly
\end{itemize}

Plotly is an open-source R package for creating interactive web-based graphs via the open-source JavaScript graphing library plotly.js. The Plotly's R graphing library helps in creating interactive, publication-quality graphs including line plots, scatter plots, area charts, bar charts, error bars, etc. One can use Plotly for R to make, view and distribute charts and maps online as well as offline.

\begin{itemize}
\tightlist
\item
  patchwork
\end{itemize}

patchwork is a package that expands the API to allow for the arbitrarily complex composition of plots by providing mathematical operators for combining multiple plots. The goal of patchwork is to make it simple to incorporate separate ggplots into the same graphic.

\begin{itemize}
\tightlist
\item
  quantmod
\end{itemize}

quantmod is an R package that provides a framework for quantitative financial modelling and trading. It provides a rapid prototyping environment that makes modelling easier by removing the repetitive workflow issues surrounding data management and visualisation.

\begin{itemize}
\tightlist
\item
  RGL
\end{itemize}

The RGL package is used to produce interactive 3-D plots using OpenGL. The library contains high-level graphics commands modelled loosely after classic R graphics and working in three dimensions. It also includes a low-level structure inspired by the grid package. RGL provides medium to high-level functions for 3D interactive graphics, including functions modelled on base graphics as well as functions for constructing representations of geometric objects.

\begin{itemize}
\tightlist
\item
  Highcharter
\end{itemize}

is an R wrapper for Highcharts, an interactive visualization library in JavaScript. Like its predecessor, highcharter features a powerful API.

Highcharter makes dynamic charting easy. It uses a single function, hchart(), to draw plots for all kinds of R object classes, from data frame to dendrogram to phylo. It also gives R coders a handy way to access the other popular Highcharts plot types, Highstock (for financial charting) and Highmaps (for schematic maps in web-based projects).

\begin{itemize}
\tightlist
\item
  Leaflet
\end{itemize}

Like highcharter, Leaflet for R is another charting packaged based on a hugely-popular JavaScript library of the same name.

Leaflet offers a lightweight but powerful way to build interactive maps, which you've probably seen in action (in their JS form) on sites ranging from The New York Times and The Washington Post to GitHub and GIS specialists like Mapbox and CartoDB.

The R interface for Leaflet was developed using the htmlwidgets framework, which makes it easy to control and integrate Leaflet maps right in R Markdown documents (v2), RStudio, or Shiny apps.

\begin{itemize}
\tightlist
\item
  RcolorBrewer
\end{itemize}

RColorBrewer makes it easy to take advantage of one of R's great strengths: manipulating colors in plots, graphs, and maps.

The package is based on Cynthia Brewer's work on the use of color in cartography (check out Colorbrewer to learn more), and it lets you create nice-looking sequential, diverging, or qualitative color palettes. It also plays nicely with Plotly, as these examples by Plotly demonstrate.

\begin{itemize}
\tightlist
\item
  dygraphs
\end{itemize}

This package provides an R interface for dygraphs, a fast, flexible JavaScript charting library for exploring time-series data sets. What's powerful about dygraphs is that it's interactive right out of the box, with default mouse-over labels, zooming, and panning. It's got lots of nifty other interactivity features, like synchronization or the range selector shown above.

But dygraph's interactivity doesn't come at the expense of speed: it can handle huge datasets with millions of points without slowing its roll. And you can use RColorBrewer with dygraphs to choose a different color palette for your time series--- check out this example to see how.

\begin{itemize}
\tightlist
\item
  sunburst
\end{itemize}

pas forcÃ©ment utile pour notre sujet

\begin{itemize}
\tightlist
\item
  VennDiagram
\end{itemize}

Create a Venn diagram and save it into a file. The function venn.diagram() takes a list and creates a file containing a publication-quality Venn Diagram.

\hypertarget{annexes}{%
\chapter{annexes}\label{annexes}}

\hypertarget{correction-of-bessels-proof}{%
\section{Correction of Bessel's proof}\label{correction-of-bessels-proof}}

\begin{align*}
(n-1)S_{xy} &= \sum_{i=1}^{N}(x_i-\bar x)(y_i - \bar y) \\
&= \sum_{i=1}^{N} x_i y_i -N\bar x \bar y\\
&= \sum_{i=1}^{N} x_i y_i - \frac{1}{N}\sum_{i=1}^{N} x_i \sum_{i=1}^{N} y_i\\
(N-1) \mathbb{E}(S_{xy}) &= \mathbb{E}\left(\sum_{i=1}^{N} x_i y_i\right) 
- \frac{1}{N}\mathbb{E}\left(\sum_{i=1}^{N} x_i \sum_{i=1}^{N} y_i\right)\\
&= N\mu_{xy} - \frac{1}{N}[N\mu_{xy} + N(N-1)\mu_x \mu_y]\\ 
&= (N-1)[\mu_{xy}-\mu_x\mu_y]\\
&= (N-1)Cov(x,~y)
\end{align*}

\hypertarget{proof-for-covxyfrac12nn-1sum_i1n-sum_j1nx_i-x_jy_i-y_j}{%
\section{\texorpdfstring{Proof for \(Cov(x,~y)=\frac{1}{2N(N-1)}\sum_{i=1}^{N} \sum_{j=1}^{N}~(x_i-x_j)(y_i-y_j)\)}{Proof for Cov(x,\textasciitilde y)=\textbackslash frac\{1\}\{2N(N-1)\}\textbackslash sum\_\{i=1\}\^{}\{N\} \textbackslash sum\_\{j=1\}\^{}\{N\}\textasciitilde(x\_i-x\_j)(y\_i-y\_j)}}\label{proof-for-covxyfrac12nn-1sum_i1n-sum_j1nx_i-x_jy_i-y_j}}

\begin{itemize}
\tightlist
\item
  Proof 1
\end{itemize}

\[Cov(x,~y)=\frac{1}{2N(N-1)}
\sum_{i=1}^{N} \sum_{j=1}^{N}~(x_i-x_j)(y_i-y_j)
=\]

\[\frac{1}{2N(N-1)}
\sum_{i=1}^{N} N~x_i~y_i -x_i~N~\overline{y}-y_i~N~\overline{x}+N~\overline{xy}\]

\[\frac{2N^2}{2N(N-1)}(\overline{xy}-\overline{x}~\overline{y})=\]

\[\frac{N}{N-1}(\overline{xy}-\overline{x}~\overline{y}))\]

\begin{itemize}
\tightlist
\item
  Proof 2
\end{itemize}

another way, not uninteresting, to show the equivalence between the two formulations of the covariance but starting from the usual form.
\[Cov(x,~y)=\frac{1}{(N-1)}
\sum_{i=1}^{N} (x_i-\overline{x})(y_i -\overline{y})\]

\[Cov(x,~y)=\frac{1}{2(N-1)} \left\{
\sum_{i=1}^{N} (x_i-\overline{x})(y_i -\overline{y}) +
\sum_{j=1}^{N} (x_j-\overline{x})(y_j -\overline{y}) \right\}\]

\[Cov(x,~y)=\frac{1}{2N(N-1)} 
\sum_{i=1}^{N} \sum_{j=1}^{N}\left\{(x_i-\overline{x})(y_i -\overline{y}) + (x_j-\overline{x})(y_j -\overline{y}) \right\}\]

\[Cov(x,~y)=\frac{1}{2N(N-1)} 
\sum_{i=1}^{N} \sum_{j=1}^{N}\left\{(x_i-x_j)(y_i-y_j)+x_iy_j+x_jy_i-x_i\overline{y}-x_j\overline{y}-y_i\overline{x}-y_j\overline{x}+2~\overline{x}~\overline{y}\right\}\]

\[Cov(x,~y)=\frac{1}{2N(N-1)} 
\sum_{i=1}^{N} \sum_{j=1}^{N}\left\{(x_i-x_j)(y_i-y_j)\right\}\]

\hypertarget{a-little-bit-of-calculation}{%
\section{a little bit of calculation}\label{a-little-bit-of-calculation}}

\hypertarget{variance-of-the-sum-of-two-random-variables}{%
\subsubsection{Variance of the sum of two random variables}\label{variance-of-the-sum-of-two-random-variables}}

\begin{align*}
Var(X+Y) &= Cov(X+Y,X+Y)  \\
 &= E((X+Y)^2)-E(X+Y)E(X+Y)   \\
\text{en dÃ©veloppant,}  \\
 &= E(X^2) - (E(X))^2 + E(Y^2) - (E(Y))^2 + 2(E(XY) - E(X)E(Y))  \\
 &= Var(X) + Var(Y) + 2(E(XY)) - E(X)E(Y))  \\
 &= Var(X) + Var(Y) + 2 Cov(X,~Y)  \\
\end{align*}

If the variables \(X,~Y\) are independent \(E(XY) = E(X)E(Y)\) then
\(Var(X+Y) = Var(X) + Var(Y)\)

\begin{align*}
\Delta &= 4Cov(X,~Y)^2-4Var(X)Var(Y)\\
&=4[Cov(X,~Y)^2-Var(X)Var(Y)] \leq 0\\
\end{align*}

So \$ Cov(X,Y)\^{}2 \leq Var(X) Var(Y)\$ (In probability theory, the Cauchy-Schwarz inequality allows the proof of the result because of the inequality \(E(XY)\leq \sqrt{E(X^2)E(Y^2)}\))
or \$-(X,Y) \leq \sigma\_x\sigma\_y \leq \sigma\_x\sigma\_y \$.
Let us notice that if \(\Delta=0\) then we have the equality \$ Cov(X,Y)\^{}2=Var(X) Var(Y)\$, i.e.~if there exists \(~\lambda\) such that \(Var(\lambda X+Y)=0\)\textbackslash{}
But to conclude, if we have \(~lambda X+Y\) equal with probability 1 to a constant, let's say \(c\), it is indeed that \(Y = c -~lambda X\) almost surely, in other words a perfect linear link between the two variables.

\hypertarget{definition-of-the-linear-correlation-coefficient-bravais-pearson}{%
\subsubsection{Definition of the linear correlation coefficient (Bravais-Pearson)}\label{definition-of-the-linear-correlation-coefficient-bravais-pearson}}

\begin{itemize}
\tightlist
\item
  case \(Cov(X,~Y)\geq 0\)
\end{itemize}

We define
\(\rho = \frac{2Cov(X,~Y)}{Var_{max}-Var_{min}}\)

\(Var_{max} = \sigma_X^2+\sigma_Y^2+2\sigma_X\sigma_Y\)

\(Var_{min} = \sigma_X^2+\sigma_Y^2\)

\(Var_{max}-Var_{minx} = 2\sigma_X\sigma_Y\)

From which, simplifying

\(\rho = \frac{Cov(X,~Y)}{\sigma_X\sigma_Y}\)

\begin{itemize}
\tightlist
\item
  case \(Cov(X,~Y)\leq 0\)
\end{itemize}

We define
\(\rho = \frac{2Cov(X,~Y)}{Var_{max}-Var_{min}}\)

\(Var_{max} = \sigma_X^2+\sigma_Y^2\)

\(Var_{min} = \sigma_X^2+\sigma_Y^2-2\sigma_X\sigma_Y\)

\(Var_{max}-Var_{minx} = 2\sigma_X\sigma_Y\)

from which, simplifying

\(\rho = \frac{Cov(X,~Y)}{\sigma_X\sigma_Y}\)

\hypertarget{covariance-framing}{%
\subsubsection{Covariance framing}\label{covariance-framing}}

\[-\sqrt{(Var(X)Var(Y)} \leq Cov(X,Y) \leq \sqrt{(Var(X)Var(Y)}\]

With in the particular case of standardized variables

\begin{align*}
Var(\frac{X}{\sigma_X}+\frac{Y}{\sigma_Y})&= Var(\frac{X}{\sigma_X})+Var(\frac{Y}{\sigma_Y})+2Cov(\frac{X}{\sigma_X},\frac{Y}{\sigma_Y})\\
&=2(1+\rho)\\
Var(\frac{X}{\sigma_X}-\frac{Y}{\sigma_Y})&= Var(\frac{X}{\sigma_X})+Var(\frac{Y}{\sigma_Y})-2Cov(\frac{X}{\sigma_X},\frac{Y}{\sigma_Y})\\
&=2(1-\rho)
\end{align*}
One finds, knowing that \(2(1+\rho) \geq 0\) and \(2(1-\rho)\geq 0\) that one has \(-1 \le \rho \le 1\)

  \bibliography{references.bib}

\end{document}
