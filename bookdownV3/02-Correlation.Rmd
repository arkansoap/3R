#  Correlation

## Correlation overview 

### From covariance to correlation 

The normalized version of the covariance, the correlation coefficient, however, shows by its magnitude the strength of the linear relation.

- Both covariance and correlation measure the relationship and the dependency between two variables.
- Correlation values are standardized while covariance values are not.
- Covariance indicates the direction of the linear relationship between variables as well as the correlation but the latter measure also the strengh of the relationship.

The normalized form of the covariance matrix is the correlation matrix.

### Correlation definition  

In statistics, correlation or dependence is any statistical relationship, whether causal or not, between two random variables or bivariate data.

In the broadest sense correlation is any statistical association, though it actually refers to the degree to which a pair of variables are linearly related.

There are several correlation coefficients, often denoted  $\rho$  or $r$, measuring the degree of correlation. The most common of these is the Pearson correlation coefficient, which is sensitive only to a linear relationship between two variables (which may be present even when one variable is a nonlinear function of the other). Other correlation coefficients – such as Spearman's rank correlation – have been developed to be more robust than Pearson's, that is, more sensitive to nonlinear relationships. 

### Galton, a pioneer in the history of correlation 

>> "I can only say that there is a vast field of topics that fall under the laws of correlation, which lies quite open to the research of any competent person who cares to investigate it." (Galton, 1890)

Galton's 1888 paper [@galton], presented to the Royal Society in London, defines correlation as follows:

>> "Two variable organs are said to be co-related when the variation of the one is accompanied on the average by more or less variation of the other, and in the same direction.... It is easy to see that co-relation must be the consequence of the variations of the two organs being partly due to common causes... If they were in no respect due to common causes, the co-relation would be nil."

Galton's definition reveals the properties of the correlation coefficient. It is a measure of the strength of a linear relationship; the closer it is to 1, the more two variables can be predicted from each other by a linear equation. It is a measure of direction: a positive correlation indicates that $X$, $Y$ increase together; a negative correlation indicates that one decreases as the other increases. Note that Galton does not claim that co-relation implies cause and effect (it would be absurd to assume that the size of one organ determines the size of another). Galton hypothesized that the correlation indicated the presence of "common causes" for the observed relationship between the variables (the size of each organ respectively).

More technically Galton continues his presentation as follows:

>> "Let y = the deviation of the subject [in units of the probably error, Q], whichever of the two variables may be taken in that capacity; and let x1, x2, x3, \&c., be the corresponding deviations of the relative, and let the mean of these be X. Then we find: (1) that y = rX for all values of y; (2) that r is the same, whichever of the two variables is taken for the subject; (3) that r is always less that 1; (4) that r measures the closeness of co-relation."

Galton particularly liked the correlation coefficient because it could be used to predict deviations $Y$ from $X$ or $X$ from $Y$. Thus, from the beginning, the correlation coefficient was closely related to the regression line. Originally, $r$ meant the regression slope, but there was a problem in that the regression line of the slope was partly a function of the units of measurement chosen. Galton perceived the correlation coefficient as a unitless regression slope and appropriated the label $r$.

\begin{figure}
    \centering
    \includegraphics[width= 250 pt]{Galton.PNG}
    \caption{The first Bivariate Scatterplot}
    \label{fig:my_label}
\end{figure}

<<<<<<< HEAD
### Thirteen ways to see correlation [@13cor]

> In 1885, Sir Francis Galton first defined the term "regression" and completed the theory of bivariate correlation. A decade later, Karl Pearson developed the index that we still use to measure correlation, Pearson's r . Our article is written in recognition of the 100th anniversary of Galton's first discussion of regression and correlation.

According Joseph Lee Rodgers and co article, Several ways to interpret the correlation:

- As standardized covariance
$$r=\frac{Cov(x,y)}{\sigma^2_x\sigma^2_y}$$
- As standardized slope of regression line
$$r=b_{Y.X}\left(\frac{\sigma^2_x}{\sigma^2_y}\right)=b_{X.Y}\left(\frac{\sigma^2_y}{\sigma^2_x}\right)$$
- As geometric mean of the two regression slopes
$$r=\pm\sqrt{b_{Y.X}\times b_{X.Y}}$$
- As the square root of  the ratio of two variances
$$r=\sqrt{\frac{\sum(Y_i -\hat{Y}_i)}{\sum(Y_i -\bar{Y}_i)}}=\sqrt{\frac{SS_{reg}}{SS_{tot}}}$$
- And so many other...

### Pearson correlation coefficient 

- Some notations that may be useful:

$S_{xx} = \sum_{i=1}^n(x_i -\bar{x})^2$

$S_{yy} = \sum_{i=1}^n(y_i -\bar{y})^2$

$S_{xy} = \sum_{i=1}^n(x_i -\bar{x})(y_i -\bar{y})$

- Pearson correlation coefficient:

$$\rho=\frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}$$

The Pearson correlation coefficient is a bounded index (i.e., $-1 \leq \rho \leq 1$) that provides a unitless measure for the strength and direction of the association between two variables.

## Alternatives correlation measurement

### Spearman's rank correlation coefficient 

Measures the association based on the ranks of the variables.

$$\hat{\theta}=\frac{\sum_{i=1}^n(R_i-\bar{R}(S_i-\bar{S}))}{\sqrt{\sum_{i=1}^n(R_i-\bar{R})^2\sum_{i=1}^n(S_i-\bar{S})^2}}$$

Where $R_i$ and $S_i$ are the rank of the $x_i$ and $y_i$ values, respectively.

Note that this is just the estimated Pearson's correlation coeffcient, but the values of the variables have been replaced by their respective ranks.

The Spearman correlation is the non-parametric equivalent of the Pearson correlation. It measures the relationship between two variables. If the variables are ordinal, discrete or do not follow a normal distribution, the Spearman correlation is used. This correlation does not use the values of the data but their rank.
In fact, nothing changes, everything is the same as calculating a Pearson correlation but on transformed variables.
The interest of establishing a correlation on the ranks of the variables is to detect if there is a monotonic relationship, which may not be linear. 

### Partial Correlation 

The partial correlation coefficient, noted here $r_{AB.C}$, allows us to know the value of the correlation between two variables A and B, if the variable C had remained constant for the series of observations considered. 

Put another way, the partial correlation coefficient $r_{AB.C}$ is the total correlation coefficient between variables A and B when we have removed their best linear explanation in terms of C. It is given by the formula :

$$r_{AB.C}=\frac{r_{AB}-r_{AC} \cdot r_{BC}}{\sqrt{1-r_{AC}^2} \cdot \sqrt{1-r_{BC}^2}}$$
Let's go a little further in understanding this coefficient:

The OLS estimator of $\beta$ is written
$$\hat{\beta}=\frac{Cov(y,x_1)}{\mathbb{V}ar(x_1)}$$
The estimator $\beta'$ is written

\begin{align*}
  \hat{\beta'} &= \frac{Cov(y,x_1)\mathbb{V}(x_2)-Cov(y,x_2)Cov(x_1,x_2)}{
  \mathbb{V}(x_1)\mathbb{V}(x_2)-Cov(x_1,x_2)^2} \\
&= \hat{\beta'}=\frac{\rho_{y1} \sigma_y \sigma_1\sigma_2^2-\rho_{y2} \sigma_y     \sigma_2\rho_{12} \sigma_1 \sigma_2}{\sigma_1^2\sigma_2^2-\rho_{12}^2 \sigma_1^2 \sigma_2^2} \\
&= \hat{\beta'}={\frac{\rho_{y1}-\rho_{y2}\rho_{12}} 
{1-\rho_{12}^2}}\quad\frac{\sigma_y}{\sigma_1}
\end{align*}

After some transformation we have: 
$$\hat{\beta'}=\underbrace{\underbrace{\frac{\rho_{y1}-\rho_{y2}\rho_{12}} 
{\sqrt{1-\rho_{12}^2}\sqrt{1-\rho_{y2}^2}}}_{\text{Partial correlation}}}_{\rho_{y1|2}}
\quad\frac{\sigma_y\sqrt{1-\rho_{y2}^2}}{\sigma_1\sqrt{1-\rho_{12}^2}}$$

<<<<<<< HEAD
In order to understand this expression, consider the following two regressions:

$$x_1=\kappa +\tau x_2+\varepsilon_{1|2}$$  
$$y=\delta +\gamma x_2+\varepsilon_{y|2}$$

We have:
\begin{align*}
Cov(e_{1|2},e_{y|2})&=Cov(x_1-\hat{\kappa}-\hat{\tau} x_2,~y-\hat{\delta} -\hat{\gamma} x_2)\\
&=Cov(x_1,y)-\hat{\gamma}Cov(x_1,x_2)-\hat{\tau}Cov(x_1,y)+\hat{\gamma}\hat{\tau}\mathbb{V}ar(x_2)\\
\mathbb{V}ar(e_{y|2})&=\mathbb{V}ar(y-\hat{\delta} - \hat{\gamma} x_2)\\
&=\mathbb{V}ar(y)-2\hat{\gamma}Cov(x_1,y)+\hat{\gamma}^2\mathbb{V}ar(x_2)\\
\mathbb{V}ar(e_{1|2})&=\mathbb{V}ar(x_1-\hat{\kappa} - \hat{\tau} x_2)\\
&=\mathbb{V}ar(x_1)-2\hat{\tau}Cov(x_1,x_2)+\hat{\tau}^2\mathbb{V}ar(x_2)
\end{align*}

The linear correlation coefficient between $e_{1|2}$ and $e_{y|2}$ corresponds to the correlation between $y$ and $x_1$ after taking into account the linear influence of $x_2$ on these two respective variables:

\begin{align*}
\rho_{yx_1|x_2}&=\frac{Cov(e_{1|2},e_{y|2})}{\sqrt{\mathbb{V}ar(e_{y|2})\mathbb{V}ar(e_{1|2})}}\\
\end{align*}

After simplification we obtain the expression of the partial correlation:

\begin{align*}
\rho_{y1|2}&=\frac{\rho_{y1}-\rho_{y2}\rho_{12}}{\sqrt{(1-\rho_{y2}^2)(1-\rho_{12}^2)}}
\end{align*}


And so, the estimator $\hat{\beta'}$ can thus be written as that of a simple linear regression where the variables are the residuals of prior regressions of $y$ respectively $x_1$ on $x_2$. 

$$\hat{\beta'}=\rho_{y1|2}\frac{\sigma_y\sqrt{1-\rho_{y2}^2}}{\sigma_1\sqrt{1-\rho_{12}^2}}$$

\begin{tikzpicture}
  \draw[<-,thick] (2,1) node[right,state,pattern=dots] (Y) {$y$} -- (0,0) node[left,state] (Y) {$x_2$} node[midway,above] {$\gamma$};
  \draw[<-,thick] (2,-1) node[right,state,pattern=dots] (X1) {$x_1$} -- (0,0) node[left,state] (Y) {$x_2$} node[midway,above] {$\tau$};
  \path[->,ultra thick] (3,-1) edge[bend right=90] node[right]{$\beta'$} (2.9,1);
\end{tikzpicture}
<<<<<<< HEAD

When the number of variable is quite high, computing the partial correlation coefficient can be quite laborious. It is advised to use some regression methods when there are more than 3 variables. The alternative is to compute regression's residuals of both chosen variables on the other variables. 
This approach leads to the same results. Let's remind that partial correlation measuring the link between the residual information of $X$ and $Y$ which is not already explained by the other variables.
The $j$-order partial correlation amounts to calculating the correlation between the regression's residuals.

$$\rho_{x_1y.x_2, \ldots, x_j} = \rho_{e_{x_1}e_y}$$

### Semi partial Correlation

Unlike to partial correlation, semi-partial correlation is asymetrical. it get closer from multiple regression. We try to quantify, for one variable, its additional ability to explain. 

For more accuracy, we take off the third variable information from one of both variables. Thanks to it, We are seeking to quantify the link between $y$ and the residuals parts of $x$ in relation to the third variable. 

$$\rho_{xy.z_1, \ldots, z_j} = \rho_{e_xe_y}$$

\begin{tikzpicture}
  \draw[-] (2,1) node[right,state] (Y) {$y$}  (0,0) node[left,state] (Y) {$Z$};
  \draw[<-,thick] (2,-1) node[right,state,pattern=dots] (X1) {$x$} -- (0,0) node[left,state] (Y) {$Z$} node[midway,above] {$\tau$};
  \path[->,ultra thick] (3,-1) edge[bend right=90] node[right]{$coef$} (2.9,1);
\end{tikzpicture}

<<<<<<< HEAD
$$r_{y(x.z)}= \frac{r_{yx} - r_{yz}r_{xz}}{\sqrt{1-r^2_{xz}}}$$
It is obvious that if $X$ and $Z$ are indepedent so $r_{y(x.z)}= r_{yx}$. Unlike, If $X$ and $Z$ are perfectly correlated, $r_{y(x.z)}$ is undefined, which means nothing remains in the $X$-residuals to explain $Z$.

### Transitivity correlation

Let $\rho_{xy}$ be the correlation between the variables $X$ and $Y$. Let $\rho_{xz}$ and $\rho_{yz}$ be the correlations of variables $X$ and $Y$ with respect to a third variable $Z$. 

Given $\rho_{xz}$ and $\rho_{yz}$, can we deduce the possible values for $\rho_{xy}$?


$$\rho_{XY \mid Z}={\frac {\rho_{XY}-\rho _{XZ}\rho_{YZ}}{{\sqrt {1-\rho_{XZ}^{2}}}{\sqrt {1-\rho_{YZ}^{2}}}}}$$

\begin{align*}
  \rho_{XY} 
  &=  \left( \rho_{XY \mid Z} - \frac{ - \rho_{XZ} \rho_{YZ}}{\sqrt{1 - \rho_{XZ}^{2}} \sqrt{1 - \rho_{YZ}^{2}}} \right)  \sqrt{1 - \rho_{XZ}^{2}} \sqrt{1 - \rho_{YZ}^{2}}  \\
  &=  \rho_{XY \mid Z} \sqrt{1 - \rho_{XZ}^{2}} \sqrt{1 - \rho_{YZ}^{2}} +  \rho_{XZ} \rho_{YZ}
\end{align*}

$\rho_{xy}$ is in the range $\rho_{XZ} \rho_{YZ} \pm \sqrt{1 - \rho_{XZ}^{2}} \sqrt{1 - \rho_{YZ}^{2}}$

\begin{align*}
  \rho_{XZ} \rho_{YZ} - \sqrt{1 - \rho_{XZ}^{2}} \sqrt{1 - \rho_{YZ}^{2}} &> 0 \\
  \rho_{XZ} \rho_{YZ} + \sqrt{1 - \rho_{XZ}^{2}} \sqrt{1 - \rho_{YZ}^{2}} &> 0 .
\end{align*}

\begin{align*}
  \rho_{XZ}^2 \rho_{YZ}^2 
  &> \left ( 1 - \rho_{XZ}^{2}\right ) \left (1 - \rho_{YZ}^{2}\right ) \\
  &= 1 - \rho_{XZ}^{2} - \rho_{YZ}^{2} + \rho_{XZ}^2 \rho_{YZ}^2 .
\end{align*}
Si $\rho_{xy}>0$ alors on a 

\begin{align*}
  \rho_{XZ}^2 \rho_{YZ}^2 
  &> \left ( 1 - \rho_{XZ}^{2}\right ) \left (1 - \rho_{YZ}^{2}\right ) \\
  &= 1 - \rho_{XZ}^{2} - \rho_{YZ}^{2} + \rho_{XZ}^2 \rho_{YZ}^2 .
\end{align*}

\begin{equation*}
\text{sign}(\rho_{XY}) 
  =
\begin{cases}
    \text{sign}(\rho_{XZ}) \text{sign}(\rho_{YZ}) & \text{ if } \rho_{XZ}^2 + \rho_{YZ}^2 > 1  \\
    \text{not known} & \text{ if } \rho_{XZ}^2 + \rho_{YZ}^2 \leq 1.
  \end{cases}
\end{equation*}

\begin{figure}[H]
\begin{tikzpicture}
  \draw [thick,fill=red!80] (0,4) rectangle (4,8);
  \draw [thick,fill=red!80] (4,0) rectangle (8,4);
  \draw [thick,fill=blue!80] (0,0) rectangle (4,4);
  \draw [thick,fill=blue!80] (4,4) rectangle (8,8);
  \draw[ultra thick,green,fill=gray!20] (4,4) circle (4cm);


  \draw[->] (-1,-1) -- (9,-1) node[below] {$\rho_{xz}$} node[below] at(0,-1) {-1} node[below] at(4,-1) {0} node[below] at(8,-1) {1};
  \draw[->] (-1,-1) -- (-1,9) node[left] {$\rho_{yz}$}
node[left] at(-1,0) {-1} node[left] at(-1,4) {0} node[left] at(-1,8) {1};

  \matrix [draw,below left] at (11,8) {
    \node [rectangle,fill=red!80,label=right:$\rho_{xy}<0$] {};\\
    \node [rectangle,fill=gray!20,label=right:$?$] {};\\  
    \node [rectangle,fill=blue!80,label=right:$\rho_{xy}>0$] {};\\
};
%current bounding box.north east
\end{tikzpicture}
\caption{Value of $\rho_{x,y}$ correlated with a third variable}
\end{figure}

## Distance correlation

The so-called association measures are an active and recent field of research that renews the well established and old field of correlation. The *energy* package developed under R as well as Gabor's article [@discor] are good references.

Let $(x_i,y_i)$, $i=1,2, \dots, N$ be a sample of pairs of observations of the variables $X$ and $Y$.

We compute successively

$$dx_{ij}=\lVert x_i-xj \rVert$$
$$dy_{ij}=\lVert y_i-yj \rVert$$

$$\overline{\overline{dx_{ij}}}=dx_{ij}-\overline{dx_{i.}}-\overline{dx_{.j}}+\overline{dx_{..}}$$
$$\overline{\overline{dy_{ij}}}=dy_{ij}-\overline{dy_{i.}}-\overline{dy_{.j}}+\overline{dy_{..}}$$
with

$\overline{dx_{i.}}=\frac{1}{N}\sum_{j=1}^{N} dx_{ij}$

and

$\overline{dx_{..}}=\frac{1}{N^2}\sum_{i=1}^{N}\sum_{j=1}^{N} dx_{ij}$



$$dCov(X,Y)=\frac{1}{N^2}\sum_{i=1}^{N}\sum_{j=1}^{N}
\overline{\overline{dx_{ij}}}~ 
\overline{\overline{dy_{ij}}}$$


$$dVar(X)=dCov^2(X,X)=\frac{1}{N^2}\sum_{i=1}^{N}\sum_{j=1}^{N} 
\overline{\overline{dx_{ij}}}~^2$$



$$d\rho=\frac{dCov(X,Y)}{d\sigma_X~d\sigma_Y}$$


### Other Correlations 

Here we introduced some correlation type [@mako]: 

- Kendall’s rank correlation: In the normal case, the Kendall correlation is preferred
to the Spearman correlation because of a smaller gross error sensitivity (GES) and a
smaller asymptotic variance (AV), making it more robust and more efficient. However,
the interpretation of Kendall’s tau is less direct compared to that of the Spearman’s rho,
in the sense that it quantifies the difference between the % of concordant and discordant
pairs among all possible pairwise events. Confidence Intervals (CI) for Kendall’s correlations are computed using the Fieller et al. (1957) correction (see Bishara & Hittner,
2017).

- Hoeffding’s D: The Hoeffding’s D statistic is a non-parametric rank based measure of association
that detects more general departures from independence (Hoeffding 1948), including non-linear
associations. Hoeffding’s D varies between -0.5 and 1 (if there are no tied ranks, otherwise it can
have lower values), with larger values indicating a stronger relationship between the variables.

- Gamma correlation: The Goodman-Kruskal gamma statistic is similar to Kendall’s Tau coefficient. It
is relatively robust to outliers and deals well with data that have many ties.

- Gaussian rank correlation: The Gaussian rank correlation estimator is a simple and wellperforming alternative for robust rank correlations (Boudt et al., 2012). It is based on the Gaussian
quantiles of the ranks.

- Winsorized correlation: Correlation of variables that have been Winsorized, i.e., transformed by
limiting extreme values to reduce the effect of possibly spurious outliers.

- Biweight midcorrelation: A measure of similarity that is median-based, instead of
the traditional mean-based, thus being less sensitive to outliers. It can be used as a
robust alternative to other similarity metrics, such as Pearson correlation (Langfelder &
Horvath, 2012).

- Percentage bend correlation: Introduced by Wilcox (1994), it is based on a downweight of a specified percentage of marginal observations deviating from the median (by
default, 20 percent).

- Shepherd’s Pi correlation: Equivalent to a Spearman’s rank correlation after outliers
removal (by means of bootstrapped Mahalanobis distance).

- Point-Biserial and biserial correlation: Correlation coefficient used when one variable
is continuous and the other is dichotomous (binary). Point-Biserial is equivalent to a
Pearson’s correlation, while Biserial should be used when the binary variable is assumed
to have an underlying continuity. For example, anxiety level can be measured on a
continuous scale, but can be classified dichotomously as high/low.

- Tetrachoric correlation: Special case of the polychoric correlation applicable when
both observed variables are dichotomous.
