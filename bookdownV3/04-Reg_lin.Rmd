# Linear regression and first reliability measure

## Simple linear regression

### Estimation by ordinary least squares

$$y_i = \beta_0 + \beta_1x_i +  \epsilon_i$$ 

Where $\epsilon_i$ is the residual or deviation of $y_i$ from the line $\beta_0 + \beta_1x_i$.

- Ordinary least square 

We need to find the values of $\beta_0$ and $\beta_1$ that minimizes the criterion: 
$$S = \sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n (y_i - (\beta_0 + \beta_1x_i))^2$$

Minimize this sum gives:

$$\hat{\beta_0}= \bar{y} - \hat{\beta_1}\bar{x}$$
$$\hat{\beta_1}= \frac{\sum_{i=1}^n(x_i -\bar{x})(y_i -\bar{y})}{\sum_{i=1}^n(x_i -\bar{x})^2}$$

- Estimated simple linear regression model:

$$y_i = \hat{\beta_0} + \hat{\beta_1x_i} +  e_i$$ 

From which we can calculate a few additionnal quantities:

- $\hat{y_i} = \hat{\beta_0} + \hat{\beta_1x_i}$ ; $\hat{y_i}$ is the **predicted value** (or predicted fit) of $y$ for the $i^{th}$ observation in the sample.

- $e_i=y_i -\hat{y_i}$ ; is the **observed error** (or residual) for the $i^{th}$ observation in the sample.

- $SSE = \sum_{i=1}^n (y_i -\hat{y_i})^2$ ; is the **sum of squared observed errors** for all observations in a sample of size $n$.

### Measuring overall variation from the sample line

- $MSE = \frac{SSE}{n-p}$ , where $p$ is the number of parameters of the regression equation. $p=2$ for regression with only one variable. 

- $s = RMSE = \sqrt{(MSE)}$

- $SSTO=\sum_{i=1}^n (y_i -\bar{y_i})^2$

- Coefficient of determination:

$R^2 = \frac{SSTO-SSE}{SSTO} = \frac{SSR}{SSTO}$ is the proportion of variation in $y$ that is explained by $x$.

In other words, the coefficient of determination is then the ratio of the variance explained by the SSE regression to the total SST variance.

The coefficient of determination is the square of the linear correlation coefficient $R^2$ between the predicted values $\hat{y}_{i}$ and the measurements $y_i$:

$$R^2=corr(\hat{y}_{i},y_i)$$

$R^2$ does not indicate whether:

- the independent variables are a cause of the changes in the dependent variable;
- omitted-variable bias exists;
- the correct regression was used;
- the most appropriate set of independent variables has been chosen;
- there is colinearity present in the data on the explanatory variables;
- the model might be improved by using transformed versions of the existing set of independent variables;
- there are enough data points to make a solid conclusion.

## Multiple linear regression

$${\displaystyle{\begin{cases}y_{1}=\beta_{0}+\beta_{1}x_{1,1}+\ldots +\beta_{p}x_{1,p}+\varepsilon_{1}\\y_{2}=\beta_{0}+\beta_{1}x_{2,1}+\ldots +\beta_{p}x_{2,p}+\varepsilon_{2}\\\vdots \\y_{n}=\beta_{0}+\beta_{1}x_{n,1}+\ldots +\beta_{p}x_{n,p}+\varepsilon _{n}\end{cases}}}$$

We aim to determine the coefficients of this regression. We need to use matrix writting style in order to express the multiple linear regression.

$${\displaystyle {\begin{pmatrix}y_{1}\\\vdots \\y_{n}\end{pmatrix}}={\begin{pmatrix}1&x_{1,1}&\cdots &x_{1,p}\\\vdots &\vdots &\ddots &\vdots \\1 &x_{n,1}&\cdots &x_{n,p}\end{pmatrix}}{\begin{pmatrix}\beta_{0}\\\beta_{1}\\\vdots \\\beta_{p}\\\end{pmatrix}}+{\begin{pmatrix}\varepsilon_{1}\\\vdots \\\varepsilon _{n}\\\end{pmatrix}}}$$

With a stacked notation, it gives: 

$$Y = \beta X + \varepsilon $$
And like simple linear regression, the ordinary least squares method search $\beta$'s vector that minimize the criterion. 

$${\displaystyle \min \sum_{i=1}^{n}{\hat{\epsilon }}_{i}^{2}=\min_{{\hat{\beta}}_{0},.,{\hat{\beta}}_{p}}\sum_{i=1}^{n}(y_{i}-{\hat{\beta}}_{0}-{\hat {\beta}}_{1}x_{i,1}-\cdots -{\hat{\beta}}_{p}x_{i,p})^{2}}$$
And so the ordinary least squares estimators is determined by:

$$\displaystyle {\hat {\beta}}=(X^{T}X)^{-1}X^{T}Y\qquad$$
